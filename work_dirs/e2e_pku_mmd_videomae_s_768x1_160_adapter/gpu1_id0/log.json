2025-07-23 16:56:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 16:56:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 16:58:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 16:58:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:00:26 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:00:26 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:02:30 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:02:30 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:03:04 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:03:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:04:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:04:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:04:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:04:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:09:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:09:26 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:11:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:11:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:13:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:13:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:14:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:14:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:21:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:21:44 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:23:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:23:45 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:24:18 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:24:19 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:40:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:40:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:43:58 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:43:59 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:44:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:44:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:45:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:45:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:46:31 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:46:31 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:47:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:47:37 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:47:48 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:47:48 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:49:02 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:49:03 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:50:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:51:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:52:17 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:52:17 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:52:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:52:58 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:53:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:53:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:54:46 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:54:46 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:55:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:55:11 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:55:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:55:51 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:56:18 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:56:18 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:57:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:57:44 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:58:32 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:58:33 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:59:06 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:59:07 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:03:13 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:03:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
solver = dict(
    train=dict(
        grad_clip=dict(max_norm=40, norm_type=2),
        lr_scheduler=dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR'),
        optimizer=dict(
            betas=(
                0.9,
                0.999,
            ), lr=0.0001, type='AdamW', weight_decay=0.05),
        warmup=dict(start_factor=0.1, total_iters=5, type='LinearLR')))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:03:59 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:03:59 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(
        batch_size=8,
        grad_clip=dict(max_norm=40, norm_type=2),
        lr_scheduler=dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR'),
        num_workers=4,
        optimizer=dict(
            betas=(
                0.9,
                0.999,
            ), lr=0.0001, type='AdamW', weight_decay=0.05),
        warmup=dict(start_factor=0.1, total_iters=5, type='LinearLR')),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:05:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:05:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ), lr=0.0001, type='AdamW', weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:05:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:05:44 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ), lr=0.0001, type='AdamW', weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:11:47 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:11:47 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ), lr=0.0001, type='AdamW', weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:11:48 Train INFO: Using single GPU training...
2025-07-23 18:13:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:13:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:13:09 Train INFO: Using single GPU training...
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:14:10 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:14:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:14:11 Train INFO: Using single GPU training...
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:14:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:14:43 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    T_max=60, eta_min=1e-06, max_epoch=60, type='CosineAnnealingLR')
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:14:44 Train INFO: Using single GPU training...
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:15:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:15:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    T_max=60, eta_min=1e-06, max_epoch=60, type='CosineAnnealingLR')
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:15:56 Train INFO: Using single GPU training...
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:16:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:16:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    eta_min=1e-06,
    max_epoch=60,
    type='LinearWarmupCosineAnnealingLR',
    warmup_epoch=5)
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:16:10 Train INFO: Using single GPU training...
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:16:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:16:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    eta_min=1e-06,
    max_epoch=60,
    type='LinearWarmupCosineAnnealingLR',
    warmup_epoch=5)
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=0)

2025-07-23 18:16:54 Train INFO: Using single GPU training...
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:16:54 Train INFO: Training Starts...

2025-07-24 09:44:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:44:00 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:47:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:47:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:49:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:49:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:49:45 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:49:46 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:49:47 Train INFO: Using single GPU training...
2025-07-24 09:49:47 Train INFO: Using Model EMA...
2025-07-24 09:49:47 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:49:47 Train INFO: Training Starts...

2025-07-24 09:49:47 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:50:30 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:50:30 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:50:32 Train INFO: Using single GPU training...
2025-07-24 09:50:32 Train INFO: Using Model EMA...
2025-07-24 09:50:32 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:50:32 Train INFO: Training Starts...

2025-07-24 09:50:32 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:51:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:51:24 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:51:26 Train INFO: Using single GPU training...
2025-07-24 09:51:26 Train INFO: Using Model EMA...
2025-07-24 09:51:26 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:51:26 Train INFO: Training Starts...

2025-07-24 09:51:26 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:52:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:52:11 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:52:13 Train INFO: Using single GPU training...
2025-07-24 09:52:13 Train INFO: Using Model EMA...
2025-07-24 09:52:13 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:52:13 Train INFO: Training Starts...

2025-07-24 09:52:13 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:53:02 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:53:03 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:53:04 Train INFO: Using single GPU training...
2025-07-24 09:53:04 Train INFO: Using Model EMA...
2025-07-24 09:53:04 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:53:04 Train INFO: Training Starts...

2025-07-24 09:53:04 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:53:54 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:53:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:53:57 Train INFO: Using single GPU training...
2025-07-24 09:53:57 Train INFO: Using Model EMA...
2025-07-24 09:53:57 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:53:57 Train INFO: Training Starts...

2025-07-24 09:53:57 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:55:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:55:43 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:55:45 Train INFO: Using single GPU training...
2025-07-24 09:55:45 Train INFO: Using Model EMA...
2025-07-24 09:55:45 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:55:45 Train INFO: Training Starts...

2025-07-24 09:55:45 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:56:39 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:56:39 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:56:41 Train INFO: Using single GPU training...
2025-07-24 09:56:41 Train INFO: Using Model EMA...
2025-07-24 09:56:41 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:56:41 Train INFO: Training Starts...

2025-07-24 09:56:41 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:57:54 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:57:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:57:57 Train INFO: Using single GPU training...
2025-07-24 09:57:57 Train INFO: Using Model EMA...
2025-07-24 09:57:57 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:57:57 Train INFO: Training Starts...

2025-07-24 09:57:57 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:58:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:58:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:58:55 Train INFO: Using single GPU training...
2025-07-24 09:58:55 Train INFO: Using Model EMA...
2025-07-24 09:58:55 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:58:55 Train INFO: Training Starts...

2025-07-24 09:58:55 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:59:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:59:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:59:55 Train INFO: Using single GPU training...
2025-07-24 09:59:55 Train INFO: Using Model EMA...
2025-07-24 09:59:55 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:59:55 Train INFO: Training Starts...

2025-07-24 09:59:55 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:00:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:00:58 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:01:00 Train INFO: Using single GPU training...
2025-07-24 10:01:00 Train INFO: Using Model EMA...
2025-07-24 10:01:00 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:01:00 Train INFO: Training Starts...

2025-07-24 10:01:00 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:04:46 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:04:47 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:04:49 Train INFO: Using single GPU training...
2025-07-24 10:04:49 Train INFO: Using Model EMA...
2025-07-24 10:04:49 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:04:49 Train INFO: Training Starts...

2025-07-24 10:04:49 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:06:04 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:06:04 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:06:06 Train INFO: Using single GPU training...
2025-07-24 10:06:06 Train INFO: Using Model EMA...
2025-07-24 10:06:06 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:06:06 Train INFO: Training Starts...

2025-07-24 10:06:06 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:07:32 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:07:33 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:07:35 Train INFO: Using single GPU training...
2025-07-24 10:07:35 Train INFO: Using Model EMA...
2025-07-24 10:07:35 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:07:35 Train INFO: Training Starts...

2025-07-24 10:07:35 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:09:22 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:09:22 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:09:24 Train INFO: Using single GPU training...
2025-07-24 10:09:24 Train INFO: Using Model EMA...
2025-07-24 10:09:24 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:09:24 Train INFO: Training Starts...

2025-07-24 10:09:24 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:10:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:10:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:10:22 Train INFO: Using single GPU training...
2025-07-24 10:10:22 Train INFO: Using Model EMA...
2025-07-24 10:10:22 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:10:22 Train INFO: Training Starts...

2025-07-24 10:10:22 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:12:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:12:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:12:12 Train INFO: Using single GPU training...
2025-07-24 10:12:12 Train INFO: Using Model EMA...
2025-07-24 10:12:12 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:12:12 Train INFO: Training Starts...

2025-07-24 10:12:12 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:17:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:17:29 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:18:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:18:50 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:18:52 Train INFO: Using single GPU training...
2025-07-24 10:18:52 Train INFO: Using Model EMA...
2025-07-24 10:18:52 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:18:52 Train INFO: Training Starts...

2025-07-24 10:18:52 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:20:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:20:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:20:03 Train INFO: Using single GPU training...
2025-07-24 10:20:03 Train INFO: Using Model EMA...
2025-07-24 10:20:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:20:03 Train INFO: Training Starts...

2025-07-24 10:20:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:20:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:20:56 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:20:58 Train INFO: Using single GPU training...
2025-07-24 10:20:58 Train INFO: Using Model EMA...
2025-07-24 10:20:58 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:20:58 Train INFO: Training Starts...

2025-07-24 10:20:58 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:23:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:23:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:23:12 Train INFO: Using single GPU training...
2025-07-24 10:23:12 Train INFO: Using Model EMA...
2025-07-24 10:23:12 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:23:12 Train INFO: Training Starts...

2025-07-24 10:23:12 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:23:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:23:45 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:23:47 Train INFO: Using single GPU training...
2025-07-24 10:23:47 Train INFO: Using Model EMA...
2025-07-24 10:23:47 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:23:47 Train INFO: Training Starts...

2025-07-24 10:23:47 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:28:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:28:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:28:10 Train INFO: Using single GPU training...
2025-07-24 10:28:10 Train INFO: Using Model EMA...
2025-07-24 10:28:10 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:28:10 Train INFO: Training Starts...

2025-07-24 10:28:10 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:29:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:29:29 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:29:31 Train INFO: Using single GPU training...
2025-07-24 10:29:31 Train INFO: Using Model EMA...
2025-07-24 10:29:31 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:29:31 Train INFO: Training Starts...

2025-07-24 10:29:31 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:31:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:31:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:31:03 Train INFO: Using single GPU training...
2025-07-24 10:31:03 Train INFO: Using Model EMA...
2025-07-24 10:31:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:31:03 Train INFO: Training Starts...

2025-07-24 10:31:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:31:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:31:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:31:59 Train INFO: Using single GPU training...
2025-07-24 10:31:59 Train INFO: Using Model EMA...
2025-07-24 10:31:59 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:31:59 Train INFO: Training Starts...

2025-07-24 10:31:59 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:33:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:33:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:33:17 Train INFO: Using single GPU training...
2025-07-24 10:33:17 Train INFO: Using Model EMA...
2025-07-24 10:33:17 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:33:17 Train INFO: Training Starts...

2025-07-24 10:33:17 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:33:51 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:33:51 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:33:53 Train INFO: Using single GPU training...
2025-07-24 10:33:53 Train INFO: Using Model EMA...
2025-07-24 10:33:53 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:33:53 Train INFO: Training Starts...

2025-07-24 10:33:53 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:35:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:35:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:35:03 Train INFO: Using single GPU training...
2025-07-24 10:35:03 Train INFO: Using Model EMA...
2025-07-24 10:35:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:35:03 Train INFO: Training Starts...

2025-07-24 10:35:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:36:08 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:36:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:36:11 Train INFO: Using single GPU training...
2025-07-24 10:36:11 Train INFO: Using Model EMA...
2025-07-24 10:36:11 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:36:11 Train INFO: Training Starts...

2025-07-24 10:36:11 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:37:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:37:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:37:59 Train INFO: Using single GPU training...
2025-07-24 10:37:59 Train INFO: Using Model EMA...
2025-07-24 10:37:59 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:37:59 Train INFO: Training Starts...

2025-07-24 10:37:59 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:04:49 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:04:50 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:04:52 Train INFO: Using single GPU training...
2025-07-24 11:04:52 Train INFO: Using Model EMA...
2025-07-24 11:04:52 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:04:52 Train INFO: Training Starts...

2025-07-24 11:04:52 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:06:13 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:06:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:06:16 Train INFO: Using single GPU training...
2025-07-24 11:06:16 Train INFO: Using Model EMA...
2025-07-24 11:06:16 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:06:16 Train INFO: Training Starts...

2025-07-24 11:06:16 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:07:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:07:36 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:07:38 Train INFO: Using single GPU training...
2025-07-24 11:07:38 Train INFO: Using Model EMA...
2025-07-24 11:07:38 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:07:38 Train INFO: Training Starts...

2025-07-24 11:07:38 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:11:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:11:50 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:11:52 Train INFO: Using single GPU training...
2025-07-24 11:11:52 Train INFO: Using Model EMA...
2025-07-24 11:11:52 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:11:52 Train INFO: Training Starts...

2025-07-24 11:11:52 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:13:34 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:13:35 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:13:37 Train INFO: Using single GPU training...
2025-07-24 11:13:37 Train INFO: Using Model EMA...
2025-07-24 11:13:37 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:13:37 Train INFO: Training Starts...

2025-07-24 11:13:37 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:06:13 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:06:13 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:06:15 Train INFO: Using single GPU training...
2025-07-24 13:06:15 Train INFO: Using Model EMA...
2025-07-24 13:06:15 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:06:15 Train INFO: Training Starts...

2025-07-24 13:06:15 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:08:39 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:08:39 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=768,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=12,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=True,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=True,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t h w -> b c t',
                    reduction='mean',
                    type='Reduce'),
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=768, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:10:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:10:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=True,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=True,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t h w -> b c t',
                    reduction='mean',
                    type='Reduce'),
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:10:09 Train INFO: Using single GPU training...
2025-07-24 13:10:09 Train INFO: Using Model EMA...
2025-07-24 13:10:09 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Training Starts...

2025-07-24 13:10:09 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:12:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:12:28 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=True,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=True,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:12:30 Train INFO: Using single GPU training...
2025-07-24 13:12:30 Train INFO: Using Model EMA...
2025-07-24 13:12:30 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Training Starts...

2025-07-24 13:12:30 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:13:47 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:13:48 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=False,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=False,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) n c -> b c (t1 n)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:13:49 Train INFO: Using single GPU training...
2025-07-24 13:13:49 Train INFO: Using Model EMA...
2025-07-24 13:13:49 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Training Starts...

2025-07-24 13:13:49 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:16:56 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:16:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=False,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=False,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) n c -> b c (t1 n)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:16:59 Train INFO: Using single GPU training...
2025-07-24 13:16:59 Train INFO: Using Model EMA...
2025-07-24 13:16:59 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Training Starts...

2025-07-24 13:16:59 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:19:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:19:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=False,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=False,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) n c -> b c (t1 n)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:19:54 Train INFO: Using single GPU training...
2025-07-24 13:19:54 Train INFO: Using Model EMA...
2025-07-24 13:19:54 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Training Starts...

2025-07-24 13:19:54 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:23:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:23:25 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:24:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:24:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:24:03 Train INFO: Using single GPU training...
2025-07-24 13:24:03 Train INFO: Using Model EMA...
2025-07-24 13:24:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:24:03 Train INFO: Training Starts...

2025-07-24 13:24:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:26:03 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:26:03 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:26:22 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:26:22 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:26:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:26:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:27:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:27:16 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:27:17 Train INFO: Using single GPU training...
2025-07-24 13:27:17 Train INFO: Using Model EMA...
2025-07-24 13:27:17 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:27:17 Train INFO: Training Starts...

2025-07-24 13:27:17 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:29:10 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:29:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ], ops='b n c t -> b c t', type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:29:12 Train INFO: Using single GPU training...
2025-07-24 13:29:12 Train INFO: Using Model EMA...
2025-07-24 13:29:12 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:29:12 Train INFO: Training Starts...

2025-07-24 13:29:12 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:31:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:31:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ], ops='b n c t -> b c t', type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:31:07 Train INFO: Using single GPU training...
2025-07-24 13:31:07 Train INFO: Using Model EMA...
2025-07-24 13:31:07 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:31:07 Train INFO: Training Starts...

2025-07-24 13:31:07 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:32:20 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:32:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    n=1,
                    ops='b n c t -> b c t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:32:22 Train INFO: Using single GPU training...
2025-07-24 13:32:22 Train INFO: Using Model EMA...
2025-07-24 13:32:22 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:32:22 Train INFO: Training Starts...

2025-07-24 13:32:22 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:35:41 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:35:42 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:35:45 Train INFO: Using single GPU training...
2025-07-24 13:35:45 Train INFO: Using Model EMA...
2025-07-24 13:35:45 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:35:45 Train INFO: Training Starts...

2025-07-24 13:35:45 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:38:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:38:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=16),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:39:03 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:39:04 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=16),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=384,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:39:47 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:39:48 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=16),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:39:50 Train INFO: Using single GPU training...
2025-07-24 13:39:50 Train INFO: Using Model EMA...
2025-07-24 13:39:50 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:39:50 Train INFO: Training Starts...

2025-07-24 13:39:50 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:41:17 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:41:18 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=32),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:42:03 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:42:04 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=32),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:42:05 Train INFO: Using single GPU training...
2025-07-24 13:42:05 Train INFO: Using Model EMA...
2025-07-24 13:42:05 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:42:05 Train INFO: Training Starts...

2025-07-24 13:42:05 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:43:22 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:43:23 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:43:25 Train INFO: Using single GPU training...
2025-07-24 13:43:25 Train INFO: Using Model EMA...
2025-07-24 13:43:25 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:43:25 Train INFO: Training Starts...

2025-07-24 13:43:25 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:44:34 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:44:35 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:44:36 Train INFO: Using single GPU training...
2025-07-24 13:44:36 Train INFO: Using Model EMA...
2025-07-24 13:44:36 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:44:37 Train INFO: Training Starts...

2025-07-24 13:44:37 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:49:04 Train INFO: [Train]: [000][00050/00471]  Loss=1.2492  cls_loss=0.6902  reg_loss=0.5590  lr_backbone=0.0e+00  lr_det=2.1e-06  mem=3756MB
2025-07-24 13:53:17 Train INFO: [Train]: [000][00100/00471]  Loss=1.4273  cls_loss=0.9025  reg_loss=0.5248  lr_backbone=0.0e+00  lr_det=4.2e-06  mem=3756MB
2025-07-24 13:54:08 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:54:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:54:10 Train INFO: Using single GPU training...
2025-07-24 13:54:10 Train INFO: Using Model EMA...
2025-07-24 13:54:10 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:54:10 Train INFO: Training Starts...

2025-07-24 13:54:10 Train INFO: [Train]: Epoch 0 started
2025-07-24 15:19:46 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 15:19:46 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0),
    val=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 15:19:48 Train INFO: Using single GPU training...
2025-07-24 15:19:48 Train INFO: Using Model EMA...
2025-07-24 15:19:48 Train INFO: Using Automatic Mixed Precision...
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 15:19:48 Train INFO: Training Starts...

2025-07-24 15:19:48 Train INFO: [Train]: Epoch 0 started
2025-07-24 16:58:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 16:58:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0),
    val=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 16:58:02 Train INFO: Using single GPU training...
2025-07-24 16:58:02 Train INFO: Using Model EMA...
2025-07-24 16:58:02 Train INFO: Using Automatic Mixed Precision...
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 16:58:03 Train INFO: Training Starts...

2025-07-24 16:58:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 17:36:40 Train INFO: [Train]: [000][00050/00117]  Loss=1.5018  cls_loss=0.9450  reg_loss=0.5568  lr_backbone=0.0e+00  lr_det=8.5e-06  mem=12367MB
2025-07-24 18:14:16 Train INFO: [Train]: [000][00100/00117]  Loss=1.4311  cls_loss=0.9842  reg_loss=0.4469  lr_backbone=0.0e+00  lr_det=1.7e-05  mem=12367MB
2025-07-24 18:26:57 Train INFO: [Train]: [000][00117/00117]  Loss=1.4140  cls_loss=0.9843  reg_loss=0.4298  lr_backbone=0.0e+00  lr_det=2.0e-05  mem=12367MB
2025-07-24 18:26:57 Train INFO: [Train]: Epoch 1 started
2025-07-24 19:05:41 Train INFO: [Train]: [001][00050/00117]  Loss=1.3225  cls_loss=0.9905  reg_loss=0.3320  lr_backbone=0.0e+00  lr_det=2.9e-05  mem=12367MB
2025-07-24 19:43:40 Train INFO: [Train]: [001][00100/00117]  Loss=1.3068  cls_loss=0.9798  reg_loss=0.3270  lr_backbone=0.0e+00  lr_det=3.7e-05  mem=12367MB
2025-07-24 19:56:23 Train INFO: [Train]: [001][00117/00117]  Loss=1.3014  cls_loss=0.9753  reg_loss=0.3261  lr_backbone=0.0e+00  lr_det=4.0e-05  mem=12367MB
2025-07-24 19:56:24 Train INFO: [Train]: Epoch 2 started
2025-07-24 20:35:07 Train INFO: [Train]: [002][00050/00117]  Loss=1.2868  cls_loss=0.9547  reg_loss=0.3321  lr_backbone=0.0e+00  lr_det=4.9e-05  mem=12367MB
2025-07-24 21:13:06 Train INFO: [Train]: [002][00100/00117]  Loss=1.1936  cls_loss=0.8645  reg_loss=0.3291  lr_backbone=0.0e+00  lr_det=5.7e-05  mem=12367MB
2025-07-24 21:25:50 Train INFO: [Train]: [002][00117/00117]  Loss=1.1695  cls_loss=0.8415  reg_loss=0.3280  lr_backbone=0.0e+00  lr_det=6.0e-05  mem=12367MB
2025-07-24 21:25:50 Train INFO: [Train]: Epoch 3 started
2025-07-24 22:04:36 Train INFO: [Train]: [003][00050/00117]  Loss=1.0364  cls_loss=0.7124  reg_loss=0.3240  lr_backbone=0.0e+00  lr_det=6.9e-05  mem=12367MB
2025-07-24 22:42:38 Train INFO: [Train]: [003][00100/00117]  Loss=1.0362  cls_loss=0.7123  reg_loss=0.3239  lr_backbone=0.0e+00  lr_det=7.7e-05  mem=12367MB
2025-07-24 22:55:23 Train INFO: [Train]: [003][00117/00117]  Loss=1.0277  cls_loss=0.7065  reg_loss=0.3212  lr_backbone=0.0e+00  lr_det=8.0e-05  mem=12367MB
2025-07-24 22:55:24 Train INFO: [Train]: Epoch 4 started
2025-07-24 23:34:12 Train INFO: [Train]: [004][00050/00117]  Loss=1.0415  cls_loss=0.7155  reg_loss=0.3260  lr_backbone=0.0e+00  lr_det=8.9e-05  mem=12367MB
2025-07-25 00:12:14 Train INFO: [Train]: [004][00100/00117]  Loss=1.0292  cls_loss=0.7061  reg_loss=0.3230  lr_backbone=0.0e+00  lr_det=9.7e-05  mem=12367MB
2025-07-25 00:24:55 Train INFO: [Train]: [004][00117/00117]  Loss=1.0294  cls_loss=0.7059  reg_loss=0.3235  lr_backbone=0.0e+00  lr_det=1.0e-04  mem=12367MB
2025-07-25 00:24:55 Train INFO: [Train]: Epoch 5 started
2025-07-25 01:03:36 Train INFO: [Train]: [005][00050/00117]  Loss=1.0278  cls_loss=0.7066  reg_loss=0.3212  lr_backbone=1.5e-12  lr_det=1.0e-04  mem=12367MB
2025-07-25 01:41:34 Train INFO: [Train]: [005][00100/00117]  Loss=1.0260  cls_loss=0.7045  reg_loss=0.3214  lr_backbone=5.9e-12  lr_det=1.0e-04  mem=12367MB
2025-07-25 01:54:16 Train INFO: [Train]: [005][00117/00117]  Loss=1.0161  cls_loss=0.6970  reg_loss=0.3191  lr_backbone=8.0e-12  lr_det=1.0e-04  mem=12367MB
2025-07-25 01:54:16 Train INFO: [Train]: Epoch 6 started
2025-07-25 02:33:00 Train INFO: [Train]: [006][00050/00117]  Loss=1.0504  cls_loss=0.7190  reg_loss=0.3314  lr_backbone=1.7e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 03:10:54 Train INFO: [Train]: [006][00100/00117]  Loss=1.0282  cls_loss=0.7029  reg_loss=0.3253  lr_backbone=2.8e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 03:23:35 Train INFO: [Train]: [006][00117/00117]  Loss=1.0264  cls_loss=0.7014  reg_loss=0.3250  lr_backbone=3.2e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 03:23:35 Train INFO: [Train]: Epoch 7 started
2025-07-25 04:02:11 Train INFO: [Train]: [007][00050/00117]  Loss=1.0084  cls_loss=0.6882  reg_loss=0.3202  lr_backbone=4.8e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 04:40:10 Train INFO: [Train]: [007][00100/00117]  Loss=1.0064  cls_loss=0.6866  reg_loss=0.3198  lr_backbone=6.6e-11  lr_det=9.9e-05  mem=12367MB
2025-07-25 04:52:51 Train INFO: [Train]: [007][00117/00117]  Loss=1.0051  cls_loss=0.6854  reg_loss=0.3196  lr_backbone=7.3e-11  lr_det=9.9e-05  mem=12367MB
2025-07-25 04:52:52 Train INFO: [Train]: Epoch 8 started
2025-07-25 05:31:37 Train INFO: [Train]: [008][00050/00117]  Loss=1.0140  cls_loss=0.6951  reg_loss=0.3189  lr_backbone=9.5e-11  lr_det=9.9e-05  mem=12367MB
2025-07-25 06:09:35 Train INFO: [Train]: [008][00100/00117]  Loss=1.0138  cls_loss=0.6925  reg_loss=0.3213  lr_backbone=1.2e-10  lr_det=9.9e-05  mem=12367MB
2025-07-25 06:22:17 Train INFO: [Train]: [008][00117/00117]  Loss=1.0129  cls_loss=0.6909  reg_loss=0.3220  lr_backbone=1.3e-10  lr_det=9.9e-05  mem=12367MB
2025-07-25 06:22:17 Train INFO: [Train]: Epoch 9 started
2025-07-25 07:00:53 Train INFO: [Train]: [009][00050/00117]  Loss=1.0029  cls_loss=0.6846  reg_loss=0.3183  lr_backbone=1.6e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 07:38:54 Train INFO: [Train]: [009][00100/00117]  Loss=1.0179  cls_loss=0.6951  reg_loss=0.3228  lr_backbone=1.9e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 07:51:35 Train INFO: [Train]: [009][00117/00117]  Loss=1.0091  cls_loss=0.6884  reg_loss=0.3207  lr_backbone=2.0e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 07:51:36 Train INFO: [Train]: Epoch 10 started
2025-07-25 08:30:20 Train INFO: [Train]: [010][00050/00117]  Loss=1.0232  cls_loss=0.7021  reg_loss=0.3211  lr_backbone=2.4e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 09:10:29 Train INFO: [Train]: [010][00100/00117]  Loss=1.0165  cls_loss=0.6940  reg_loss=0.3225  lr_backbone=2.8e-10  lr_det=9.7e-05  mem=12367MB
2025-07-25 09:27:32 Train INFO: [Train]: [010][00117/00117]  Loss=1.0092  cls_loss=0.6891  reg_loss=0.3201  lr_backbone=2.9e-10  lr_det=9.7e-05  mem=12367MB
2025-07-25 09:27:32 Train INFO: [Train]: Epoch 11 started
2025-07-25 10:08:41 Train INFO: [Train]: [011][00050/00117]  Loss=1.0107  cls_loss=0.6923  reg_loss=0.3184  lr_backbone=3.3e-10  lr_det=9.7e-05  mem=12367MB
2025-07-25 10:48:28 Train INFO: [Train]: [011][00100/00117]  Loss=1.0092  cls_loss=0.6910  reg_loss=0.3182  lr_backbone=3.8e-10  lr_det=9.6e-05  mem=12367MB
2025-07-25 11:01:13 Train INFO: [Train]: [011][00117/00117]  Loss=1.0139  cls_loss=0.6936  reg_loss=0.3204  lr_backbone=3.9e-10  lr_det=9.6e-05  mem=12367MB
2025-07-25 11:01:14 Train INFO: [Train]: Epoch 12 started
2025-07-25 11:39:55 Train INFO: [Train]: [012][00050/00117]  Loss=1.0135  cls_loss=0.6928  reg_loss=0.3206  lr_backbone=4.4e-10  lr_det=9.6e-05  mem=12367MB
2025-07-25 12:17:50 Train INFO: [Train]: [012][00100/00117]  Loss=1.0033  cls_loss=0.6849  reg_loss=0.3184  lr_backbone=4.9e-10  lr_det=9.5e-05  mem=12367MB
2025-07-25 12:30:28 Train INFO: [Train]: [012][00117/00117]  Loss=1.0042  cls_loss=0.6848  reg_loss=0.3193  lr_backbone=5.1e-10  lr_det=9.5e-05  mem=12367MB
2025-07-25 12:30:28 Train INFO: [Train]: Epoch 13 started
2025-07-25 13:09:15 Train INFO: [Train]: [013][00050/00117]  Loss=1.0052  cls_loss=0.6869  reg_loss=0.3183  lr_backbone=5.7e-10  lr_det=9.4e-05  mem=12367MB
2025-07-25 13:51:53 Train INFO: [Train]: [013][00100/00117]  Loss=1.0047  cls_loss=0.6857  reg_loss=0.3190  lr_backbone=6.3e-10  lr_det=9.4e-05  mem=12367MB
2025-07-25 14:07:46 Train INFO: [Train]: [013][00117/00117]  Loss=1.0069  cls_loss=0.6873  reg_loss=0.3196  lr_backbone=6.5e-10  lr_det=9.4e-05  mem=12367MB
2025-07-25 14:07:46 Train INFO: [Train]: Epoch 14 started
2025-07-25 14:46:38 Train INFO: [Train]: [014][00050/00117]  Loss=1.0091  cls_loss=0.6893  reg_loss=0.3197  lr_backbone=7.1e-10  lr_det=9.3e-05  mem=12367MB
2025-07-25 15:24:41 Train INFO: [Train]: [014][00100/00117]  Loss=1.0122  cls_loss=0.6917  reg_loss=0.3205  lr_backbone=7.7e-10  lr_det=9.2e-05  mem=12367MB
2025-07-25 15:38:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:38:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:39:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:39:06 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:39:08 Train INFO: Using single GPU training...
2025-07-25 15:39:08 Train INFO: Using Model EMA...
2025-07-25 15:39:08 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:39:08 Train INFO: Freeze the backbone...
2025-07-25 15:40:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:40:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:40:07 Train INFO: Using single GPU training...
2025-07-25 15:40:07 Train INFO: Using Model EMA...
2025-07-25 15:40:07 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:40:07 Train INFO: Freeze the backbone...
2025-07-25 15:42:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:42:45 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    lr=0.0001,
    paramwise_cfg=dict(
        backbone=dict(
            custom=[
                dict(lr=0.0002, name='adapter', weight_decay=0.05),
            ],
            exclude=[
                'backbone',
            ],
            lr=0,
            weight_decay=0)),
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:42:46 Train INFO: Using single GPU training...
2025-07-25 15:42:46 Train INFO: Using Model EMA...
2025-07-25 15:42:46 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:42:46 Train INFO: Freeze the backbone...
2025-07-25 15:43:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:43:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:43:16 Train INFO: Using single GPU training...
2025-07-25 15:43:16 Train INFO: Using Model EMA...
2025-07-25 15:43:16 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:43:16 Train INFO: Freeze the backbone...
2025-07-25 15:44:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:44:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:44:21 Train INFO: Using single GPU training...
2025-07-25 15:44:21 Train INFO: Using Model EMA...
2025-07-25 15:44:21 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:44:21 Train INFO: Freeze the backbone...
2025-07-25 15:44:21 Train INFO: Training Starts...

2025-07-25 15:44:21 Train INFO: [Train]: Epoch 0 started
2025-07-25 15:57:29 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:57:30 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=8),
    train=dict(batch_size=16, num_workers=8))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:57:31 Train INFO: Using single GPU training...
2025-07-25 15:57:31 Train INFO: Using Model EMA...
2025-07-25 15:57:31 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:57:31 Train INFO: Freeze the backbone...
2025-07-25 15:57:31 Train INFO: Training Starts...

2025-07-25 15:57:31 Train INFO: [Train]: Epoch 0 started
2025-07-25 16:20:12 Train INFO: [Train]: [000][00050/00058]  Loss=1.5328  cls_loss=1.0013  reg_loss=0.5315  lr_det=1.7e-05  mem=8731MB
2025-07-25 16:21:39 Train INFO: [Train]: [000][00058/00058]  Loss=1.5141  cls_loss=1.0079  reg_loss=0.5062  lr_det=2.0e-05  mem=8731MB
2025-07-25 16:43:10 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 16:43:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 16:43:12 Train INFO: Using single GPU training...
2025-07-25 16:43:12 Train INFO: Using Model EMA...
2025-07-25 16:43:12 Train INFO: Using Automatic Mixed Precision...
2025-07-25 16:43:12 Train INFO: Freeze the backbone...
2025-07-25 16:43:12 Train INFO: Training Starts...

2025-07-25 16:43:12 Train INFO: [Train]: Epoch 0 started
2025-07-25 17:04:10 Train INFO: [Train]: [000][00050/00058]  Loss=1.5424  cls_loss=1.0041  reg_loss=0.5383  lr_det=1.7e-05  mem=8731MB
2025-07-25 17:06:48 Train INFO: [Train]: [000][00058/00058]  Loss=1.5086  cls_loss=1.0001  reg_loss=0.5085  lr_det=2.0e-05  mem=8731MB
2025-07-25 17:06:49 Train INFO: [Train]: Epoch 1 started
2025-07-25 17:27:36 Train INFO: [Train]: [001][00050/00058]  Loss=1.3082  cls_loss=0.9883  reg_loss=0.3200  lr_det=3.7e-05  mem=8731MB
2025-07-25 17:30:28 Train INFO: [Train]: [001][00058/00058]  Loss=1.2851  cls_loss=0.9721  reg_loss=0.3131  lr_det=4.0e-05  mem=8731MB
2025-07-25 17:30:30 Train INFO: [Train]: Epoch 2 started
2025-07-25 17:51:57 Train INFO: [Train]: [002][00050/00058]  Loss=1.2799  cls_loss=0.9696  reg_loss=0.3103  lr_det=5.7e-05  mem=8731MB
2025-07-25 17:54:32 Train INFO: [Train]: [002][00058/00058]  Loss=1.2811  cls_loss=0.9687  reg_loss=0.3124  lr_det=6.0e-05  mem=8731MB
2025-07-25 17:54:32 Train INFO: [Train]: Epoch 3 started
2025-07-25 18:15:03 Train INFO: [Train]: [003][00050/00058]  Loss=1.1720  cls_loss=0.8587  reg_loss=0.3133  lr_det=7.7e-05  mem=8731MB
2025-07-25 18:17:34 Train INFO: [Train]: [003][00058/00058]  Loss=1.1449  cls_loss=0.8343  reg_loss=0.3106  lr_det=8.0e-05  mem=8731MB
2025-07-25 18:17:36 Train INFO: [Train]: Epoch 4 started
2025-07-25 18:38:25 Train INFO: [Train]: [004][00050/00058]  Loss=1.0178  cls_loss=0.7124  reg_loss=0.3054  lr_det=9.7e-05  mem=8731MB
2025-07-25 18:41:05 Train INFO: [Train]: [004][00058/00058]  Loss=1.0291  cls_loss=0.7193  reg_loss=0.3098  lr_det=1.0e-04  mem=8731MB
2025-07-25 18:41:05 Train INFO: [Train]: Epoch 5 started
2025-07-25 19:01:26 Train INFO: [Train]: [005][00050/00058]  Loss=1.0201  cls_loss=0.7116  reg_loss=0.3085  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:04:04 Train INFO: [Train]: [005][00058/00058]  Loss=1.0152  cls_loss=0.7078  reg_loss=0.3074  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:04:05 Train INFO: [Train]: Epoch 6 started
2025-07-25 19:24:25 Train INFO: [Train]: [006][00050/00058]  Loss=1.0059  cls_loss=0.6958  reg_loss=0.3100  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:27:02 Train INFO: [Train]: [006][00058/00058]  Loss=1.0057  cls_loss=0.6969  reg_loss=0.3088  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:27:03 Train INFO: [Train]: Epoch 7 started
2025-07-25 19:48:01 Train INFO: [Train]: [007][00050/00058]  Loss=1.0088  cls_loss=0.7016  reg_loss=0.3072  lr_det=9.9e-05  mem=8731MB
2025-07-25 19:50:41 Train INFO: [Train]: [007][00058/00058]  Loss=1.0035  cls_loss=0.6986  reg_loss=0.3049  lr_det=9.9e-05  mem=8731MB
2025-07-25 19:50:43 Train INFO: [Train]: Epoch 8 started
2025-07-25 20:11:37 Train INFO: [Train]: [008][00050/00058]  Loss=0.9983  cls_loss=0.6947  reg_loss=0.3037  lr_det=9.9e-05  mem=8731MB
2025-07-25 20:14:04 Train INFO: [Train]: [008][00058/00058]  Loss=0.9983  cls_loss=0.6942  reg_loss=0.3041  lr_det=9.9e-05  mem=8731MB
2025-07-25 20:14:05 Train INFO: [Train]: Epoch 9 started
2025-07-25 20:35:13 Train INFO: [Train]: [009][00050/00058]  Loss=1.0038  cls_loss=0.6940  reg_loss=0.3098  lr_det=9.8e-05  mem=8731MB
2025-07-25 20:37:50 Train INFO: [Train]: [009][00058/00058]  Loss=0.9953  cls_loss=0.6885  reg_loss=0.3068  lr_det=9.8e-05  mem=8731MB
2025-07-25 20:37:52 Train INFO: [Train]: Epoch 10 started
2025-07-25 20:58:50 Train INFO: [Train]: [010][00050/00058]  Loss=1.0131  cls_loss=0.7008  reg_loss=0.3123  lr_det=9.7e-05  mem=8731MB
2025-07-25 21:01:36 Train INFO: [Train]: [010][00058/00058]  Loss=1.0095  cls_loss=0.6986  reg_loss=0.3109  lr_det=9.7e-05  mem=8731MB
2025-07-25 21:01:37 Train INFO: [Train]: Epoch 11 started
2025-07-25 21:22:38 Train INFO: [Train]: [011][00050/00058]  Loss=0.9937  cls_loss=0.6881  reg_loss=0.3056  lr_det=9.6e-05  mem=8731MB
2025-07-25 21:25:12 Train INFO: [Train]: [011][00058/00058]  Loss=0.9873  cls_loss=0.6828  reg_loss=0.3045  lr_det=9.6e-05  mem=8731MB
2025-07-25 21:25:13 Train INFO: [Train]: Epoch 12 started
2025-07-25 21:45:50 Train INFO: [Train]: [012][00050/00058]  Loss=0.9961  cls_loss=0.6898  reg_loss=0.3063  lr_det=9.5e-05  mem=8731MB
2025-07-25 21:48:21 Train INFO: [Train]: [012][00058/00058]  Loss=0.9953  cls_loss=0.6893  reg_loss=0.3060  lr_det=9.5e-05  mem=8731MB
2025-07-25 21:48:21 Train INFO: [Train]: Epoch 13 started
2025-07-25 22:08:40 Train INFO: [Train]: [013][00050/00058]  Loss=0.9984  cls_loss=0.6899  reg_loss=0.3085  lr_det=9.4e-05  mem=8731MB
2025-07-25 22:11:31 Train INFO: [Train]: [013][00058/00058]  Loss=0.9971  cls_loss=0.6898  reg_loss=0.3073  lr_det=9.4e-05  mem=8731MB
2025-07-25 22:11:33 Train INFO: [Train]: Epoch 14 started
2025-07-25 22:32:16 Train INFO: [Train]: [014][00050/00058]  Loss=0.9941  cls_loss=0.6891  reg_loss=0.3050  lr_det=9.2e-05  mem=8731MB
2025-07-25 22:35:05 Train INFO: [Train]: [014][00058/00058]  Loss=0.9882  cls_loss=0.6849  reg_loss=0.3033  lr_det=9.2e-05  mem=8731MB
2025-07-25 22:35:06 Train INFO: [Train]: Epoch 15 started
2025-07-25 22:55:29 Train INFO: [Train]: [015][00050/00058]  Loss=1.0020  cls_loss=0.6946  reg_loss=0.3074  lr_det=9.1e-05  mem=8731MB
2025-07-25 22:58:10 Train INFO: [Train]: [015][00058/00058]  Loss=0.9890  cls_loss=0.6852  reg_loss=0.3038  lr_det=9.0e-05  mem=8731MB
2025-07-25 22:58:11 Train INFO: [Train]: Epoch 16 started
2025-07-25 23:19:30 Train INFO: [Train]: [016][00050/00058]  Loss=1.0061  cls_loss=0.6952  reg_loss=0.3109  lr_det=8.9e-05  mem=8731MB
2025-07-25 23:22:11 Train INFO: [Train]: [016][00058/00058]  Loss=0.9982  cls_loss=0.6898  reg_loss=0.3085  lr_det=8.9e-05  mem=8731MB
2025-07-25 23:22:12 Train INFO: [Train]: Epoch 17 started
2025-07-25 23:43:08 Train INFO: [Train]: [017][00050/00058]  Loss=0.9980  cls_loss=0.6915  reg_loss=0.3065  lr_det=8.7e-05  mem=8731MB
2025-07-25 23:45:47 Train INFO: [Train]: [017][00058/00058]  Loss=0.9851  cls_loss=0.6822  reg_loss=0.3029  lr_det=8.7e-05  mem=8731MB
2025-07-25 23:45:48 Train INFO: [Train]: Epoch 18 started
2025-07-26 00:05:45 Train INFO: [Train]: [018][00050/00058]  Loss=0.9938  cls_loss=0.6958  reg_loss=0.2980  lr_det=8.5e-05  mem=8731MB
2025-07-26 00:08:17 Train INFO: [Train]: [018][00058/00058]  Loss=0.9894  cls_loss=0.6910  reg_loss=0.2984  lr_det=8.5e-05  mem=8731MB
2025-07-26 00:08:18 Train INFO: [Train]: Epoch 19 started
2025-07-26 00:29:14 Train INFO: [Train]: [019][00050/00058]  Loss=0.9833  cls_loss=0.6811  reg_loss=0.3022  lr_det=8.3e-05  mem=8731MB
2025-07-26 00:31:52 Train INFO: [Train]: [019][00058/00058]  Loss=0.9887  cls_loss=0.6853  reg_loss=0.3035  lr_det=8.3e-05  mem=8731MB
2025-07-26 00:31:53 Train INFO: [Train]: Epoch 20 started
2025-07-26 00:51:33 Train INFO: [Train]: [020][00050/00058]  Loss=0.9906  cls_loss=0.6850  reg_loss=0.3056  lr_det=8.1e-05  mem=8731MB
2025-07-26 00:54:20 Train INFO: [Train]: [020][00058/00058]  Loss=0.9931  cls_loss=0.6869  reg_loss=0.3062  lr_det=8.1e-05  mem=8731MB
2025-07-26 00:54:21 Train INFO: [Train]: Epoch 21 started
2025-07-26 01:14:56 Train INFO: [Train]: [021][00050/00058]  Loss=0.9696  cls_loss=0.6722  reg_loss=0.2974  lr_det=7.9e-05  mem=8731MB
2025-07-26 01:17:25 Train INFO: [Train]: [021][00058/00058]  Loss=0.9724  cls_loss=0.6745  reg_loss=0.2979  lr_det=7.8e-05  mem=8731MB
2025-07-26 01:17:27 Train INFO: [Train]: Epoch 22 started
2025-07-26 01:38:10 Train INFO: [Train]: [022][00050/00058]  Loss=0.9821  cls_loss=0.6816  reg_loss=0.3005  lr_det=7.6e-05  mem=8731MB
2025-07-26 01:40:59 Train INFO: [Train]: [022][00058/00058]  Loss=0.9680  cls_loss=0.6722  reg_loss=0.2958  lr_det=7.6e-05  mem=8731MB
2025-07-26 01:41:00 Train INFO: [Train]: Epoch 23 started
2025-07-26 02:01:57 Train INFO: [Train]: [023][00050/00058]  Loss=1.0127  cls_loss=0.7009  reg_loss=0.3118  lr_det=7.4e-05  mem=8731MB
2025-07-26 02:04:45 Train INFO: [Train]: [023][00058/00058]  Loss=1.0072  cls_loss=0.6970  reg_loss=0.3101  lr_det=7.3e-05  mem=8731MB
2025-07-26 02:04:47 Train INFO: [Train]: Epoch 24 started
2025-07-26 02:25:11 Train INFO: [Train]: [024][00050/00058]  Loss=0.9837  cls_loss=0.6818  reg_loss=0.3019  lr_det=7.1e-05  mem=8731MB
2025-07-26 02:27:58 Train INFO: [Train]: [024][00058/00058]  Loss=0.9744  cls_loss=0.6754  reg_loss=0.2991  lr_det=7.1e-05  mem=8731MB
2025-07-26 02:27:58 Train INFO: [Train]: Epoch 25 started
2025-07-26 02:48:34 Train INFO: [Train]: [025][00050/00058]  Loss=0.9745  cls_loss=0.6722  reg_loss=0.3023  lr_det=6.9e-05  mem=8731MB
2025-07-26 02:51:15 Train INFO: [Train]: [025][00058/00058]  Loss=0.9695  cls_loss=0.6695  reg_loss=0.3000  lr_det=6.8e-05  mem=8731MB
2025-07-26 02:51:17 Train INFO: [Train]: Epoch 26 started
2025-07-26 03:11:38 Train INFO: [Train]: [026][00050/00058]  Loss=0.9943  cls_loss=0.6909  reg_loss=0.3033  lr_det=6.6e-05  mem=8731MB
2025-07-26 03:14:30 Train INFO: [Train]: [026][00058/00058]  Loss=0.9936  cls_loss=0.6892  reg_loss=0.3045  lr_det=6.6e-05  mem=8731MB
2025-07-26 03:14:31 Train INFO: [Train]: Epoch 27 started
2025-07-26 03:35:29 Train INFO: [Train]: [027][00050/00058]  Loss=0.9861  cls_loss=0.6811  reg_loss=0.3050  lr_det=6.3e-05  mem=8731MB
2025-07-26 03:38:01 Train INFO: [Train]: [027][00058/00058]  Loss=0.9780  cls_loss=0.6757  reg_loss=0.3024  lr_det=6.3e-05  mem=8731MB
2025-07-26 03:38:02 Train INFO: [Train]: Epoch 28 started
2025-07-26 03:58:30 Train INFO: [Train]: [028][00050/00058]  Loss=0.9960  cls_loss=0.6878  reg_loss=0.3082  lr_det=6.0e-05  mem=8731MB
2025-07-26 04:01:07 Train INFO: [Train]: [028][00058/00058]  Loss=0.9874  cls_loss=0.6817  reg_loss=0.3057  lr_det=6.0e-05  mem=8731MB
2025-07-26 04:01:07 Train INFO: [Train]: Epoch 29 started
2025-07-26 04:21:34 Train INFO: [Train]: [029][00050/00058]  Loss=0.9755  cls_loss=0.6747  reg_loss=0.3008  lr_det=5.8e-05  mem=8731MB
2025-07-26 04:24:22 Train INFO: [Train]: [029][00058/00058]  Loss=0.9707  cls_loss=0.6714  reg_loss=0.2993  lr_det=5.7e-05  mem=8731MB
2025-07-26 04:24:23 Train INFO: [Train]: Epoch 30 started
2025-07-26 04:45:39 Train INFO: [Train]: [030][00050/00058]  Loss=0.9891  cls_loss=0.6880  reg_loss=0.3010  lr_det=5.5e-05  mem=8731MB
2025-07-26 04:48:09 Train INFO: [Train]: [030][00058/00058]  Loss=0.9842  cls_loss=0.6841  reg_loss=0.3000  lr_det=5.4e-05  mem=8731MB
2025-07-26 04:48:10 Train INFO: [Train]: Epoch 31 started
2025-07-26 05:08:41 Train INFO: [Train]: [031][00050/00058]  Loss=0.9758  cls_loss=0.6737  reg_loss=0.3021  lr_det=5.2e-05  mem=8731MB
2025-07-26 05:11:16 Train INFO: [Train]: [031][00058/00058]  Loss=0.9750  cls_loss=0.6743  reg_loss=0.3007  lr_det=5.1e-05  mem=8731MB
2025-07-26 05:11:18 Train INFO: [Train]: Epoch 32 started
2025-07-26 05:32:00 Train INFO: [Train]: [032][00050/00058]  Loss=0.9766  cls_loss=0.6763  reg_loss=0.3003  lr_det=4.9e-05  mem=8731MB
2025-07-26 05:34:45 Train INFO: [Train]: [032][00058/00058]  Loss=0.9761  cls_loss=0.6761  reg_loss=0.3000  lr_det=4.9e-05  mem=8731MB
2025-07-26 05:34:46 Train INFO: [Train]: Epoch 33 started
2025-07-26 05:54:39 Train INFO: [Train]: [033][00050/00058]  Loss=0.9833  cls_loss=0.6776  reg_loss=0.3057  lr_det=4.6e-05  mem=8731MB
2025-07-26 05:57:19 Train INFO: [Train]: [033][00058/00058]  Loss=0.9800  cls_loss=0.6748  reg_loss=0.3051  lr_det=4.6e-05  mem=8731MB
2025-07-26 05:57:20 Train INFO: [Train]: Epoch 34 started
2025-07-26 06:17:44 Train INFO: [Train]: [034][00050/00058]  Loss=0.9778  cls_loss=0.6735  reg_loss=0.3043  lr_det=4.3e-05  mem=8731MB
2025-07-26 06:20:34 Train INFO: [Train]: [034][00058/00058]  Loss=0.9760  cls_loss=0.6744  reg_loss=0.3016  lr_det=4.3e-05  mem=8731MB
2025-07-26 06:20:35 Train INFO: [Train]: Epoch 35 started
2025-07-26 06:41:18 Train INFO: [Train]: [035][00050/00058]  Loss=0.9704  cls_loss=0.6722  reg_loss=0.2983  lr_det=4.1e-05  mem=8731MB
2025-07-26 06:43:59 Train INFO: [Train]: [035][00058/00058]  Loss=0.9648  cls_loss=0.6683  reg_loss=0.2966  lr_det=4.0e-05  mem=8731MB
2025-07-26 06:44:00 Train INFO: [Train]: Epoch 36 started
2025-07-26 07:04:38 Train INFO: [Train]: [036][00050/00058]  Loss=0.9991  cls_loss=0.6901  reg_loss=0.3089  lr_det=3.8e-05  mem=8731MB
2025-07-26 07:07:23 Train INFO: [Train]: [036][00058/00058]  Loss=0.9920  cls_loss=0.6836  reg_loss=0.3084  lr_det=3.7e-05  mem=8731MB
2025-07-26 07:07:24 Train INFO: [Train]: Epoch 37 started
2025-07-26 07:28:20 Train INFO: [Train]: [037][00050/00058]  Loss=0.9756  cls_loss=0.6742  reg_loss=0.3014  lr_det=3.5e-05  mem=8731MB
2025-07-26 07:31:01 Train INFO: [Train]: [037][00058/00058]  Loss=0.9727  cls_loss=0.6727  reg_loss=0.3000  lr_det=3.5e-05  mem=8731MB
2025-07-26 07:31:02 Train INFO: [Train]: Epoch 38 started
2025-07-26 07:51:52 Train INFO: [Train]: [038][00050/00058]  Loss=0.9735  cls_loss=0.6746  reg_loss=0.2988  lr_det=3.2e-05  mem=8731MB
2025-07-26 07:54:34 Train INFO: [Train]: [038][00058/00058]  Loss=0.9665  cls_loss=0.6698  reg_loss=0.2967  lr_det=3.2e-05  mem=8731MB
2025-07-26 07:54:35 Train INFO: [Train]: Epoch 39 started
2025-07-26 08:15:37 Train INFO: [Train]: [039][00050/00058]  Loss=0.9774  cls_loss=0.6789  reg_loss=0.2985  lr_det=3.0e-05  mem=8731MB
2025-07-26 08:18:15 Train INFO: [Train]: [039][00058/00058]  Loss=0.9701  cls_loss=0.6733  reg_loss=0.2967  lr_det=2.9e-05  mem=8731MB
2025-07-26 08:18:17 Train INFO: [Train]: Epoch 40 started
2025-07-26 08:39:12 Train INFO: [Train]: [040][00050/00058]  Loss=0.9707  cls_loss=0.6723  reg_loss=0.2983  lr_det=2.7e-05  mem=8731MB
2025-07-26 08:41:52 Train INFO: [Train]: [040][00058/00058]  Loss=0.9644  cls_loss=0.6676  reg_loss=0.2968  lr_det=2.7e-05  mem=8731MB
2025-07-26 08:41:52 Train INFO: [Train]: Epoch 41 started
2025-07-26 09:02:29 Train INFO: [Train]: [041][00050/00058]  Loss=0.9913  cls_loss=0.6888  reg_loss=0.3025  lr_det=2.5e-05  mem=8731MB
2025-07-26 09:05:15 Train INFO: [Train]: [041][00058/00058]  Loss=0.9913  cls_loss=0.6875  reg_loss=0.3038  lr_det=2.4e-05  mem=8731MB
2025-07-26 09:05:16 Train INFO: [Train]: Epoch 42 started
2025-07-26 09:26:13 Train INFO: [Train]: [042][00050/00058]  Loss=0.9771  cls_loss=0.6741  reg_loss=0.3030  lr_det=2.2e-05  mem=8731MB
2025-07-26 09:28:55 Train INFO: [Train]: [042][00058/00058]  Loss=0.9661  cls_loss=0.6666  reg_loss=0.2995  lr_det=2.2e-05  mem=8731MB
2025-07-26 09:28:56 Train INFO: [Train]: Epoch 43 started
2025-07-26 09:49:23 Train INFO: [Train]: [043][00050/00058]  Loss=0.9821  cls_loss=0.6762  reg_loss=0.3059  lr_det=2.0e-05  mem=8731MB
2025-07-26 09:52:08 Train INFO: [Train]: [043][00058/00058]  Loss=0.9806  cls_loss=0.6758  reg_loss=0.3048  lr_det=2.0e-05  mem=8731MB
2025-07-26 09:52:09 Train INFO: [Train]: Epoch 44 started
2025-07-26 10:12:30 Train INFO: [Train]: [044][00050/00058]  Loss=0.9763  cls_loss=0.6760  reg_loss=0.3004  lr_det=1.8e-05  mem=8731MB
2025-07-26 10:15:12 Train INFO: [Train]: [044][00058/00058]  Loss=0.9756  cls_loss=0.6744  reg_loss=0.3012  lr_det=1.7e-05  mem=8731MB
2025-07-26 10:15:13 Train INFO: [Train]: Epoch 45 started
2025-07-26 10:35:44 Train INFO: [Train]: [045][00050/00058]  Loss=0.9690  cls_loss=0.6712  reg_loss=0.2977  lr_det=1.5e-05  mem=8731MB
2025-07-26 10:38:18 Train INFO: [Train]: [045][00058/00058]  Loss=0.9606  cls_loss=0.6655  reg_loss=0.2951  lr_det=1.5e-05  mem=8731MB
2025-07-26 10:38:19 Train INFO: [Train]: Epoch 46 started
2025-07-26 10:58:47 Train INFO: [Train]: [046][00050/00058]  Loss=0.9847  cls_loss=0.6816  reg_loss=0.3032  lr_det=1.3e-05  mem=8731MB
2025-07-26 11:01:16 Train INFO: [Train]: [046][00058/00058]  Loss=0.9729  cls_loss=0.6742  reg_loss=0.2987  lr_det=1.3e-05  mem=8731MB
2025-07-26 11:01:17 Train INFO: [Train]: Epoch 47 started
2025-07-26 11:21:34 Train INFO: [Train]: [047][00050/00058]  Loss=0.9834  cls_loss=0.6788  reg_loss=0.3046  lr_det=1.2e-05  mem=8731MB
2025-07-26 11:24:11 Train INFO: [Train]: [047][00058/00058]  Loss=0.9785  cls_loss=0.6755  reg_loss=0.3030  lr_det=1.1e-05  mem=8731MB
2025-07-26 11:24:12 Train INFO: [Train]: Epoch 48 started
2025-07-26 11:45:05 Train INFO: [Train]: [048][00050/00058]  Loss=0.9706  cls_loss=0.6709  reg_loss=0.2997  lr_det=9.8e-06  mem=8731MB
2025-07-26 11:47:39 Train INFO: [Train]: [048][00058/00058]  Loss=0.9748  cls_loss=0.6744  reg_loss=0.3004  lr_det=9.6e-06  mem=8731MB
2025-07-26 11:47:39 Train INFO: [Train]: Epoch 49 started
2025-07-26 12:08:27 Train INFO: [Train]: [049][00050/00058]  Loss=0.9660  cls_loss=0.6684  reg_loss=0.2976  lr_det=8.2e-06  mem=8731MB
2025-07-26 12:11:06 Train INFO: [Train]: [049][00058/00058]  Loss=0.9650  cls_loss=0.6678  reg_loss=0.2972  lr_det=8.0e-06  mem=8731MB
2025-07-26 12:11:07 Train INFO: [Train]: Epoch 50 started
2025-07-26 12:32:43 Train INFO: [Train]: [050][00050/00058]  Loss=0.9750  cls_loss=0.6745  reg_loss=0.3005  lr_det=6.7e-06  mem=8731MB
2025-07-26 12:35:25 Train INFO: [Train]: [050][00058/00058]  Loss=0.9673  cls_loss=0.6695  reg_loss=0.2979  lr_det=6.5e-06  mem=8731MB
2025-07-26 12:35:25 Train INFO: [Train]: Epoch 51 started
2025-07-26 12:56:37 Train INFO: [Train]: [051][00050/00058]  Loss=1.0009  cls_loss=0.6919  reg_loss=0.3090  lr_det=5.3e-06  mem=8731MB
2025-07-26 12:59:11 Train INFO: [Train]: [051][00058/00058]  Loss=0.9966  cls_loss=0.6887  reg_loss=0.3080  lr_det=5.2e-06  mem=8731MB
2025-07-26 12:59:12 Train INFO: [Train]: Epoch 52 started
2025-07-26 13:20:09 Train INFO: [Train]: [052][00050/00058]  Loss=0.9538  cls_loss=0.6617  reg_loss=0.2921  lr_det=4.1e-06  mem=8731MB
2025-07-26 13:23:00 Train INFO: [Train]: [052][00058/00058]  Loss=0.9629  cls_loss=0.6674  reg_loss=0.2955  lr_det=4.0e-06  mem=8731MB
2025-07-26 13:23:00 Train INFO: [Train]: Epoch 53 started
2025-07-26 13:44:08 Train INFO: [Train]: [053][00050/00058]  Loss=0.9752  cls_loss=0.6697  reg_loss=0.3055  lr_det=3.1e-06  mem=8731MB
2025-07-26 13:47:07 Train INFO: [Train]: [053][00058/00058]  Loss=0.9717  cls_loss=0.6682  reg_loss=0.3035  lr_det=2.9e-06  mem=8731MB
2025-07-26 13:47:08 Train INFO: [Train]: Epoch 54 started
2025-07-26 14:08:13 Train INFO: [Train]: [054][00050/00058]  Loss=0.9685  cls_loss=0.6683  reg_loss=0.3002  lr_det=2.2e-06  mem=8731MB
2025-07-26 14:11:00 Train INFO: [Train]: [054][00058/00058]  Loss=0.9677  cls_loss=0.6676  reg_loss=0.3001  lr_det=2.0e-06  mem=8731MB
2025-07-26 14:11:00 Train INFO: [Train]: Epoch 55 started
2025-07-26 14:31:31 Train INFO: [Train]: [055][00050/00058]  Loss=0.9859  cls_loss=0.6809  reg_loss=0.3050  lr_det=1.4e-06  mem=8731MB
2025-07-26 14:34:17 Train INFO: [Train]: [055][00058/00058]  Loss=0.9744  cls_loss=0.6727  reg_loss=0.3017  lr_det=1.3e-06  mem=8731MB
2025-07-26 14:34:19 Train INFO: [Train]: Epoch 56 started
2025-07-26 14:54:19 Train INFO: [Train]: [056][00050/00058]  Loss=0.9724  cls_loss=0.6734  reg_loss=0.2991  lr_det=8.2e-07  mem=8731MB
2025-07-26 14:57:09 Train INFO: [Train]: [056][00058/00058]  Loss=0.9670  cls_loss=0.6699  reg_loss=0.2971  lr_det=7.5e-07  mem=8731MB
2025-07-26 14:57:10 Train INFO: [Train]: Epoch 57 started
2025-07-26 15:17:48 Train INFO: [Train]: [057][00050/00058]  Loss=0.9677  cls_loss=0.6712  reg_loss=0.2965  lr_det=3.9e-07  mem=8731MB
2025-07-26 15:20:29 Train INFO: [Train]: [057][00058/00058]  Loss=0.9696  cls_loss=0.6721  reg_loss=0.2975  lr_det=3.4e-07  mem=8731MB
2025-07-26 15:20:30 Train INFO: [Train]: Epoch 58 started
2025-07-26 15:41:17 Train INFO: [Train]: [058][00050/00058]  Loss=0.9780  cls_loss=0.6760  reg_loss=0.3020  lr_det=1.2e-07  mem=8731MB
2025-07-26 15:43:59 Train INFO: [Train]: [058][00058/00058]  Loss=0.9745  cls_loss=0.6734  reg_loss=0.3011  lr_det=9.4e-08  mem=8731MB
2025-07-26 15:44:00 Train INFO: [Train]: Epoch 59 started
2025-07-26 16:04:04 Train INFO: [Train]: [059][00050/00058]  Loss=0.9679  cls_loss=0.6706  reg_loss=0.2973  lr_det=1.2e-08  mem=8731MB
2025-07-26 16:06:51 Train INFO: [Train]: [059][00058/00058]  Loss=0.9637  cls_loss=0.6681  reg_loss=0.2956  lr_det=1.0e-08  mem=8731MB
2025-07-26 16:06:52 Train INFO: Training Over...

2025-07-28 09:23:03 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:23:03 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:23:05 Test INFO: Using single GPU testing...
2025-07-28 09:23:05 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/latest.pth
2025-07-28 09:24:34 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:24:34 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:24:35 Test INFO: Using single GPU testing...
2025-07-28 09:24:35 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:24:36 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:24:36 Test INFO: Using Model EMA...
2025-07-28 09:24:36 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:24:36 Test INFO: Testing Starts...

2025-07-28 09:26:45 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:26:46 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:26:47 Test INFO: Using single GPU testing...
2025-07-28 09:26:47 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:26:47 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:26:47 Test INFO: Using Model EMA...
2025-07-28 09:26:47 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:26:47 Test INFO: Testing Starts...

2025-07-28 09:29:27 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:29:28 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:29:29 Test INFO: Using single GPU testing...
2025-07-28 09:29:29 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:29:29 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:29:29 Test INFO: Using Model EMA...
2025-07-28 09:29:29 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:29:29 Test INFO: Testing Starts...

2025-07-28 09:30:29 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:30:30 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:30:31 Test INFO: Using single GPU testing...
2025-07-28 09:30:31 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:30:31 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:30:31 Test INFO: Using Model EMA...
2025-07-28 09:30:31 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:30:31 Test INFO: Testing Starts...

2025-07-28 09:33:45 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:33:46 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:33:47 Test INFO: Using single GPU testing...
2025-07-28 09:33:47 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:33:47 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:33:47 Test INFO: Using Model EMA...
2025-07-28 09:33:47 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:33:47 Test INFO: Testing Starts...

2025-07-28 09:37:08 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:37:09 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='feats', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:37:10 Test INFO: Using single GPU testing...
2025-07-28 09:37:10 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:37:10 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:37:10 Test INFO: Using Model EMA...
2025-07-28 09:37:10 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:37:10 Test INFO: Testing Starts...

2025-07-28 09:39:13 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:39:14 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:39:15 Test INFO: Using single GPU testing...
2025-07-28 09:39:15 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:39:15 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:39:15 Test INFO: Using Model EMA...
2025-07-28 09:39:15 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:39:15 Test INFO: Testing Starts...

2025-07-28 09:43:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:43:33 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:43:34 Test INFO: Using single GPU testing...
2025-07-28 09:43:34 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:43:35 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:43:35 Test INFO: Using Model EMA...
2025-07-28 09:43:35 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:43:35 Test INFO: Testing Starts...

2025-07-28 09:45:21 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:45:21 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict()
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:45:23 Test INFO: Using single GPU testing...
2025-07-28 09:45:23 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:45:23 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:45:23 Test INFO: Using Model EMA...
2025-07-28 09:45:23 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:45:23 Test INFO: Testing Starts...

2025-07-28 09:51:00 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:51:00 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:51:01 Test INFO: Using single GPU testing...
2025-07-28 09:51:01 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:51:02 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:51:02 Test INFO: Using Model EMA...
2025-07-28 09:51:02 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:51:02 Test INFO: Testing Starts...

2025-07-28 10:05:08 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:05:09 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:05:10 Test INFO: Using single GPU testing...
2025-07-28 10:05:10 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:05:10 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:05:10 Test INFO: Using Model EMA...
2025-07-28 10:05:10 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:05:10 Test INFO: Testing Starts...

2025-07-28 10:09:04 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:09:04 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:09:05 Test INFO: Using single GPU testing...
2025-07-28 10:09:05 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:09:06 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:09:06 Test INFO: Using Model EMA...
2025-07-28 10:09:06 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:09:06 Test INFO: Testing Starts...

2025-07-28 10:13:47 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:13:47 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:13:48 Test INFO: Using single GPU testing...
2025-07-28 10:13:48 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:13:49 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:13:49 Test INFO: Using Model EMA...
2025-07-28 10:13:49 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:13:49 Test INFO: Testing Starts...

2025-07-28 10:17:37 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:17:38 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:17:39 Test INFO: Using single GPU testing...
2025-07-28 10:17:39 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:17:39 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:17:39 Test INFO: Using Model EMA...
2025-07-28 10:17:39 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:17:39 Test INFO: Testing Starts...

2025-07-28 10:19:53 Test INFO: Testing Over...

2025-07-28 10:27:00 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:27:00 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename='data/pku_mmd/annotations/pku_mmd_anno.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.2,
        0.3,
        0.4,
        0.5,
    ],
    top_k=[
        1,
        5,
    ],
    type='mAP')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:27:01 Test INFO: Using single GPU testing...
2025-07-28 10:27:01 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:27:02 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:27:02 Test INFO: Using Model EMA...
2025-07-28 10:27:02 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:27:02 Test INFO: Testing Starts...

2025-07-28 10:32:30 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:32:31 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename=None,
    subset='test',
    tiou_thresholds=[
        0.1,
        0.2,
        0.3,
        0.4,
        0.5,
    ],
    top_k=[
        1,
        5,
    ],
    type='mAP_epic')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:32:32 Test INFO: Using single GPU testing...
2025-07-28 10:32:32 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:32:32 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:32:32 Test INFO: Using Model EMA...
2025-07-28 10:32:32 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:32:32 Test INFO: Testing Starts...

2025-07-28 10:37:51 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:37:52 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename=None,
    subset='test',
    tiou_thresholds=[
        0.1,
        0.2,
        0.3,
        0.4,
        0.5,
    ],
    top_k=[
        1,
        5,
    ],
    type='mAP_EPIC')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:37:53 Test INFO: Using single GPU testing...
2025-07-28 10:37:53 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:37:53 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:37:53 Test INFO: Using Model EMA...
2025-07-28 10:37:53 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:37:53 Test INFO: Testing Starts...

2025-07-28 10:43:23 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:43:24 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename=None,
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_EPIC')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:43:25 Test INFO: Using single GPU testing...
2025-07-28 10:43:25 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:43:25 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:43:25 Test INFO: Using Model EMA...
2025-07-28 10:43:25 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:43:25 Test INFO: Testing Starts...

2025-07-28 10:50:06 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:50:06 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:50:07 Test INFO: Using single GPU testing...
2025-07-28 10:50:07 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:50:08 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:50:08 Test INFO: Using Model EMA...
2025-07-28 10:50:08 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:50:08 Test INFO: Testing Starts...

2025-07-28 10:52:34 Test INFO: Evaluation starts...
2025-07-28 10:53:26 Test INFO: Loaded annotations from test subset.
2025-07-28 10:53:26 Test INFO: Number of ground truth instances: 2647
2025-07-28 10:53:26 Test INFO: Number of predictions: 40392
2025-07-28 10:53:26 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 10:53:26 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 10:53:26 Test INFO: Testing Over...

2025-07-28 10:54:35 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:54:36 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:54:37 Test INFO: Using single GPU testing...
2025-07-28 10:54:37 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:54:37 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:54:37 Test INFO: Using Model EMA...
2025-07-28 10:54:37 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:54:37 Test INFO: Testing Starts...

2025-07-28 10:57:07 Test INFO: Evaluation starts...
2025-07-28 10:58:00 Test INFO: Loaded annotations from test subset.
2025-07-28 10:58:00 Test INFO: Number of ground truth instances: 2647
2025-07-28 10:58:00 Test INFO: Number of predictions: 40392
2025-07-28 10:58:00 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 10:58:00 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 10:58:00 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 10:58:00 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 10:58:00 Test INFO: Prediction labels: [51]
2025-07-28 10:58:00 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 10:58:00 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 10:58:00 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 10:58:00 Test INFO: Testing Over...

2025-07-28 11:00:53 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 11:00:54 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 11:00:55 Test INFO: Using single GPU testing...
2025-07-28 11:00:55 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 11:00:55 Test INFO: Checkpoint is epoch 59.
2025-07-28 11:00:55 Test INFO: Using Model EMA...
2025-07-28 11:00:55 Test INFO: Using Automatic Mixed Precision...
2025-07-28 11:00:55 Test INFO: Testing Starts...

2025-07-28 11:03:24 Test INFO: Evaluation starts...
2025-07-28 11:04:17 Test INFO: Loaded annotations from test subset.
2025-07-28 11:04:17 Test INFO: Number of ground truth instances: 2647
2025-07-28 11:04:17 Test INFO: Number of predictions: 40392
2025-07-28 11:04:17 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 11:04:17 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:04:17 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:04:17 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 11:04:17 Test INFO: Prediction labels: [51]
2025-07-28 11:04:17 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 11:04:17 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 11:04:17 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 11:04:17 Test INFO: Testing Over...

2025-07-28 11:05:12 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 11:05:13 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 11:05:14 Test INFO: Using single GPU testing...
2025-07-28 11:05:14 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 11:05:14 Test INFO: Checkpoint is epoch 59.
2025-07-28 11:05:14 Test INFO: Using Model EMA...
2025-07-28 11:05:14 Test INFO: Using Automatic Mixed Precision...
2025-07-28 11:05:14 Test INFO: Testing Starts...

2025-07-28 11:08:40 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 11:08:40 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 11:08:41 Test INFO: Using single GPU testing...
2025-07-28 11:08:41 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 11:08:42 Test INFO: Checkpoint is epoch 59.
2025-07-28 11:08:42 Test INFO: Using Model EMA...
2025-07-28 11:08:42 Test INFO: Using Automatic Mixed Precision...
2025-07-28 11:08:42 Test INFO: Testing Starts...

2025-07-28 11:11:03 Test INFO: Evaluation starts...
2025-07-28 11:11:55 Test INFO: Loaded annotations from test subset.
2025-07-28 11:11:55 Test INFO: Number of ground truth instances: 2647
2025-07-28 11:11:55 Test INFO: Number of predictions: 40392
2025-07-28 11:11:55 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 11:11:55 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:11:55 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:11:55 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 11:11:55 Test INFO: Prediction labels: [51]
2025-07-28 11:11:55 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 11:11:55 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 11:11:55 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 11:11:55 Test INFO: Testing Over...

2025-07-28 13:02:14 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:02:15 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:02:17 Test INFO: Using single GPU testing...
2025-07-28 13:02:17 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/latest.pth
2025-07-28 13:02:29 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:02:29 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:02:30 Test INFO: Using single GPU testing...
2025-07-28 13:02:30 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 13:02:31 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:02:31 Test INFO: Using Model EMA...
2025-07-28 13:02:31 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:02:31 Test INFO: Testing Starts...

2025-07-28 13:05:24 Test INFO: Evaluation starts...
2025-07-28 13:06:22 Test INFO: Loaded annotations from test subset.
2025-07-28 13:06:22 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:06:22 Test INFO: Number of predictions: 40392
2025-07-28 13:06:22 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:06:22 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:06:22 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:06:22 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:06:22 Test INFO: Prediction labels: [51]
2025-07-28 13:06:22 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:06:22 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 13:06:22 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:06:23 Test INFO: Testing Over...

2025-07-28 13:07:36 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:07:37 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:07:38 Test INFO: Using single GPU testing...
2025-07-28 13:07:38 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/latest.pth
2025-07-28 13:08:16 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:08:16 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:08:17 Test INFO: Using single GPU testing...
2025-07-28 13:08:17 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:08:18 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:08:18 Test INFO: Using Model EMA...
2025-07-28 13:08:18 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:08:18 Test INFO: Testing Starts...

2025-07-28 13:10:56 Test INFO: Evaluation starts...
2025-07-28 13:11:49 Test INFO: Loaded annotations from test subset.
2025-07-28 13:11:49 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:11:49 Test INFO: Number of predictions: 40392
2025-07-28 13:11:49 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:11:49 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:11:49 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:11:49 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:11:49 Test INFO: Prediction labels: [51]
2025-07-28 13:11:49 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:11:49 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 13:11:49 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:11:50 Test INFO: Testing Over...

2025-07-28 13:18:56 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:18:57 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:18:58 Test INFO: Using single GPU testing...
2025-07-28 13:18:58 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:18:58 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:18:58 Test INFO: Using Model EMA...
2025-07-28 13:18:58 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:18:58 Test INFO: Testing Starts...

2025-07-28 13:21:29 Test INFO: Evaluation starts...
2025-07-28 13:22:22 Test INFO: Loaded annotations from test subset.
2025-07-28 13:22:22 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:22:22 Test INFO: Number of predictions: 40392
2025-07-28 13:22:22 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:22:22 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:22:22 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:22:22 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:22:22 Test INFO: Prediction labels: [51]
2025-07-28 13:22:22 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:22:22 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:22:22 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:22:23 Test INFO: Testing Over...

2025-07-28 13:23:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:23:34 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:23:35 Test INFO: Using single GPU testing...
2025-07-28 13:23:35 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:23:35 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:23:35 Test INFO: Using Model EMA...
2025-07-28 13:23:35 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:23:35 Test INFO: Testing Starts...

2025-07-28 13:26:09 Test INFO: Evaluation starts...
2025-07-28 13:27:01 Test INFO: Loaded annotations from test subset.
2025-07-28 13:27:01 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:27:01 Test INFO: Number of predictions: 40392
2025-07-28 13:27:01 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:27:01 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:27:01 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:27:01 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:27:01 Test INFO: Prediction labels: [51]
2025-07-28 13:27:01 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:27:01 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:27:01 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:27:01 Test INFO: Testing Over...

2025-07-28 13:27:54 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:27:55 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:27:56 Test INFO: Using single GPU testing...
2025-07-28 13:27:56 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:27:56 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:27:56 Test INFO: Using Model EMA...
2025-07-28 13:27:56 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:27:56 Test INFO: Testing Starts...

2025-07-28 13:30:33 Test INFO: Evaluation starts...
2025-07-28 13:31:25 Test INFO: Loaded annotations from test subset.
2025-07-28 13:31:25 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:31:25 Test INFO: Number of predictions: 40392
2025-07-28 13:31:25 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:31:25 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:31:25 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:31:25 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:31:25 Test INFO: Prediction labels: [51]
2025-07-28 13:31:25 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:31:25 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:31:25 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:31:25 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:31:25 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:31:26 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:31:26 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:31:26 Test INFO: Testing Over...

2025-07-28 13:33:47 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:33:48 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:33:49 Test INFO: Using single GPU testing...
2025-07-28 13:33:49 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:33:49 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:33:49 Test INFO: Using Model EMA...
2025-07-28 13:33:49 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:33:49 Test INFO: Testing Starts...

2025-07-28 13:36:23 Test INFO: Evaluation starts...
2025-07-28 13:37:16 Test INFO: Loaded annotations from test subset.
2025-07-28 13:37:16 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:37:16 Test INFO: Number of predictions: 40392
2025-07-28 13:37:16 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:37:16 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:37:16 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:37:16 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:37:16 Test INFO: Prediction labels: [51]
2025-07-28 13:37:16 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:37:16 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:37:16 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:37:17 Test INFO: Testing Over...

2025-07-28 13:40:49 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:40:49 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:40:50 Test INFO: Using single GPU testing...
2025-07-28 13:40:50 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:40:51 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:40:51 Test INFO: Using Model EMA...
2025-07-28 13:40:51 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:40:51 Test INFO: Testing Starts...

2025-07-28 13:43:22 Test INFO: Evaluation starts...
2025-07-28 13:44:16 Test INFO: Loaded annotations from test subset.
2025-07-28 13:44:16 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:44:16 Test INFO: Number of predictions: 40392
2025-07-28 13:44:16 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:44:16 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:44:16 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:44:16 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:44:16 Test INFO: Prediction labels: [29 50 17 38  7  4 44  8 45 27]
2025-07-28 13:44:16 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:44:16 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:44:16 Test INFO: Average-mAP:  nan (%)
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.10 is  nan%
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.30 is  nan%
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.50 is  nan%
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.70 is  nan%
2025-07-28 13:44:16 Test INFO: Testing Over...

2025-07-28 13:46:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:46:34 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:46:35 Test INFO: Using single GPU testing...
2025-07-28 13:46:35 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:46:35 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:46:35 Test INFO: Using Model EMA...
2025-07-28 13:46:35 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:46:35 Test INFO: Testing Starts...

2025-07-28 13:49:06 Test INFO: Evaluation starts...
2025-07-28 13:50:00 Test INFO: Loaded annotations from test subset.
2025-07-28 13:50:00 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:50:00 Test INFO: Number of predictions: 40392
2025-07-28 13:50:00 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:50:00 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:50:00 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:50:00 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:50:00 Test INFO: Prediction labels: [29 50 17 38  7  4 44  8 45 27]
2025-07-28 13:50:00 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:50:00 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:50:00 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:50:01 Test INFO: Testing Over...

2025-07-28 14:11:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 14:11:58 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=32, num_workers=8),
    train=dict(batch_size=32, num_workers=8))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 14:12:00 Train INFO: Using single GPU training...
2025-07-28 14:12:00 Train INFO: Using Model EMA...
2025-07-28 14:12:00 Train INFO: Using Automatic Mixed Precision...
2025-07-28 14:12:00 Train INFO: Freeze the backbone...
2025-07-28 14:12:00 Train INFO: Training Starts...

2025-07-28 14:12:00 Train INFO: [Train]: Epoch 0 started
2025-07-28 14:24:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 14:24:26 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=32, num_workers=6),
    train=dict(batch_size=32, num_workers=6))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 14:24:29 Train INFO: Using single GPU training...
2025-07-28 14:24:29 Train INFO: Using Model EMA...
2025-07-28 14:24:29 Train INFO: Using Automatic Mixed Precision...
2025-07-28 14:24:29 Train INFO: Freeze the backbone...
2025-07-28 14:24:29 Train INFO: Training Starts...

2025-07-28 14:24:29 Train INFO: [Train]: Epoch 0 started
2025-07-28 14:49:00 Train INFO: [Train]: [000][00029/00029]  Loss=1.9804  cls_loss=1.1638  reg_loss=0.8167  lr_det=9.7e-06  mem=16326MB
2025-07-28 14:49:01 Train INFO: [Train]: Epoch 1 started
2025-07-28 15:13:31 Train INFO: [Train]: [001][00029/00029]  Loss=1.3897  cls_loss=1.0406  reg_loss=0.3491  lr_det=2.0e-05  mem=16326MB
2025-07-28 15:13:33 Train INFO: [Train]: Epoch 2 started
2025-07-28 15:31:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:31:37 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                stride=256,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:32:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:32:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:32:07 Train INFO: Using single GPU training...
2025-07-28 15:32:07 Train INFO: Using Model EMA...
2025-07-28 15:32:07 Train INFO: Using Automatic Mixed Precision...
2025-07-28 15:32:07 Train INFO: Freeze the backbone...
2025-07-28 15:32:07 Train INFO: Training Starts...

2025-07-28 15:32:07 Train INFO: [Train]: Epoch 0 started
2025-07-28 15:44:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:44:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:50:08 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:50:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:50:10 Train INFO: Using single GPU training...
2025-07-28 15:50:10 Train INFO: Using Model EMA...
2025-07-28 15:50:10 Train INFO: Using Automatic Mixed Precision...
2025-07-28 15:50:10 Train INFO: Freeze the backbone...
2025-07-28 15:50:10 Train INFO: Training Starts...

2025-07-28 15:50:10 Train INFO: [Train]: Epoch 0 started
2025-07-28 16:10:29 Train INFO: [Train]: [000][00050/00688]  Loss=1.9508  cls_loss=1.0481  reg_loss=0.9027  lr_det=7.3e-07  mem=8731MB
2025-07-28 16:30:31 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 16:30:31 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                frame_interval=3,
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                frame_interval=3,
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 16:31:12 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 16:31:13 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 16:31:15 Train INFO: Using single GPU training...
2025-07-28 16:31:15 Train INFO: Using Model EMA...
2025-07-28 16:31:15 Train INFO: Using Automatic Mixed Precision...
2025-07-28 16:31:15 Train INFO: Freeze the backbone...
2025-07-28 16:31:15 Train INFO: Training Starts...

2025-07-28 16:31:15 Train INFO: [Train]: Epoch 0 started
2025-07-28 16:51:59 Train INFO: [Train]: [000][00050/01068]  Loss=1.9783  cls_loss=1.0587  reg_loss=0.9197  lr_det=4.7e-07  mem=8731MB
2025-07-28 17:12:10 Train INFO: [Train]: [000][00100/01068]  Loss=2.0043  cls_loss=1.1061  reg_loss=0.8982  lr_det=9.4e-07  mem=8731MB
2025-07-28 17:27:29 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:27:30 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=6),
    train=dict(batch_size=2, num_workers=6),
    val=dict(batch_size=2, num_workers=6))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=60)

2025-07-28 17:28:42 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:28:42 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=6),
    train=dict(batch_size=2, num_workers=6),
    val=dict(batch_size=2, num_workers=6))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=60)

2025-07-28 17:28:43 Train INFO: training subset: 942 videos
2025-07-28 17:28:43 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:28:44 Train INFO: Using single GPU training...
2025-07-28 17:28:44 Train INFO: Using Model EMA...
2025-07-28 17:28:44 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:28:44 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:28:44 Train INFO: Training Starts...

2025-07-28 17:28:44 Train INFO: [Train]: Epoch 0 started (Total iterations: 471)
2025-07-28 17:31:26 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:31:26 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=6),
    train=dict(batch_size=2, num_workers=6))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:31:27 Train INFO: training subset: 942 videos
2025-07-28 17:31:27 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:31:28 Train INFO: Using single GPU training...
2025-07-28 17:31:28 Train INFO: Using Model EMA...
2025-07-28 17:31:28 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:31:28 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:31:28 Train INFO: Training Starts...

2025-07-28 17:31:28 Train INFO: [Train]: Epoch 0 started (Total iterations: 471)
2025-07-28 17:32:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:32:25 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:32:26 Train INFO: training subset: 942 videos
2025-07-28 17:32:26 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:32:26 Train INFO: Using single GPU training...
2025-07-28 17:32:26 Train INFO: Using Model EMA...
2025-07-28 17:32:27 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:32:27 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:32:27 Train INFO: Training Starts...

2025-07-28 17:32:27 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 17:48:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:48:10 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t h w -> b c t',
                    reduction='mean',
                    type='Reduce'),
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=32,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=512, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=32,
                    type='Rearrange'),
            ]),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:48:10 Train INFO: training subset: 942 videos
2025-07-28 17:48:10 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:48:11 Train INFO: Using single GPU training...
2025-07-28 17:48:11 Train INFO: Using Model EMA...
2025-07-28 17:48:11 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:48:11 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:48:11 Train INFO: Training Starts...

2025-07-28 17:48:11 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 17:54:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:54:28 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    n=1,
                    ops='(b n) c t -> b n c t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:54:29 Train INFO: training subset: 942 videos
2025-07-28 17:54:29 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:54:30 Train INFO: Using single GPU training...
2025-07-28 17:54:30 Train INFO: Using Model EMA...
2025-07-28 17:54:30 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:54:30 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:54:30 Train INFO: Freeze the backbone...
2025-07-28 17:54:30 Train INFO: Training Starts...

2025-07-28 17:54:30 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:05:30 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 18:05:31 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ], ops='b n c t -> b c t', type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 18:05:31 Train INFO: training subset: 942 videos
2025-07-28 18:05:31 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 18:05:32 Train INFO: Using single GPU training...
2025-07-28 18:05:32 Train INFO: Using Model EMA...
2025-07-28 18:05:32 Train INFO: Using Automatic Mixed Precision...
2025-07-28 18:05:32 Train INFO: GPU Memory: 24.0 GB
2025-07-28 18:05:32 Train INFO: Freeze the backbone...
2025-07-28 18:05:32 Train INFO: Training Starts...

2025-07-28 18:05:32 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:09:31 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 18:09:32 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 18:09:32 Train INFO: training subset: 942 videos
2025-07-28 18:09:32 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 18:09:33 Train INFO: Using single GPU training...
2025-07-28 18:09:33 Train INFO: Using Model EMA...
2025-07-28 18:09:33 Train INFO: Using Automatic Mixed Precision...
2025-07-28 18:09:33 Train INFO: GPU Memory: 24.0 GB
2025-07-28 18:09:33 Train INFO: Freeze the backbone...
2025-07-28 18:09:33 Train INFO: Training Starts...

2025-07-28 18:09:33 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:12:56 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 18:12:57 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 18:12:58 Train INFO: training subset: 942 videos
2025-07-28 18:12:58 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 18:12:58 Train INFO: Using single GPU training...
2025-07-28 18:12:58 Train INFO: Using Model EMA...
2025-07-28 18:12:58 Train INFO: Using Automatic Mixed Precision...
2025-07-28 18:12:58 Train INFO: GPU Memory: 24.0 GB
2025-07-28 18:12:58 Train INFO: Freeze the backbone...
2025-07-28 18:12:58 Train INFO: Training Starts...

2025-07-28 18:12:58 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:17:57 Train INFO: [Train]: [000][00010/00058] (18.6%)  Loss=1.6274  cls_loss=0.8716  reg_loss=0.7558  lr_det=3.4e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1305s  iter_time=299.103s  fwd=2.223s/bwd=0.076s/opt=0.007s
2025-07-28 18:22:30 Train INFO: [Train]: [000][00020/00058] (35.6%)  Loss=1.7138  cls_loss=0.9560  reg_loss=0.7577  lr_det=6.8e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=273.047s
2025-07-28 18:25:34 Train INFO: [Train]: [000][00030/00058] (52.5%)  Loss=1.6507  cls_loss=0.9818  reg_loss=0.6690  lr_det=1.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=683s  iter_time=184.199s
2025-07-28 18:30:07 Train INFO: [Train]: [000][00040/00058] (69.5%)  Loss=1.5867  cls_loss=0.9983  reg_loss=0.5884  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=452s  iter_time=272.550s
2025-07-28 18:33:15 Train INFO: [Train]: [000][00050/00058] (86.4%)  Loss=1.5577  cls_loss=1.0158  reg_loss=0.5420  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=191s  iter_time=188.115s
2025-07-28 18:35:54 Train INFO: [Train]: [000][00058/00058] (100.0%)  Loss=1.5216  cls_loss=1.0101  reg_loss=0.5116  lr_det=2.0e-05  GPU=1354MB(alloc)/4212MB(reserved)/9063MB(max)  ETA=0s  iter_time=159.014s
2025-07-28 18:35:55 Train INFO: [Train]: Epoch 0 completed in 1376.8s (avg 23.336s/iter)
2025-07-28 18:35:55 Train INFO: [Train]: Final Loss=1.5216
2025-07-28 18:35:55 Train INFO: [Train]: Epoch 1 started (Total iterations: 59)
2025-07-28 18:40:58 Train INFO: [Train]: [001][00010/00058] (18.6%)  Loss=1.3968  cls_loss=1.0492  reg_loss=0.3476  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1322s  iter_time=302.910s  fwd=2.143s/bwd=0.064s/opt=0.010s
2025-07-28 18:45:15 Train INFO: [Train]: [001][00020/00058] (35.6%)  Loss=1.3410  cls_loss=1.0079  reg_loss=0.3331  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1014s  iter_time=257.690s
2025-07-28 18:48:23 Train INFO: [Train]: [001][00030/00058] (52.5%)  Loss=1.3258  cls_loss=1.0015  reg_loss=0.3242  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=676s  iter_time=187.902s
2025-07-28 18:53:06 Train INFO: [Train]: [001][00040/00058] (69.5%)  Loss=1.3106  cls_loss=0.9910  reg_loss=0.3196  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=453s  iter_time=282.229s
2025-07-28 18:56:43 Train INFO: [Train]: [001][00050/00058] (86.4%)  Loss=1.3027  cls_loss=0.9858  reg_loss=0.3169  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=196s  iter_time=217.130s
2025-07-28 18:59:45 Train INFO: [Train]: [001][00058/00058] (100.0%)  Loss=1.2932  cls_loss=0.9794  reg_loss=0.3139  lr_det=4.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=182.651s
2025-07-28 18:59:46 Train INFO: [Train]: Epoch 1 completed in 1431.7s (avg 24.265s/iter)
2025-07-28 18:59:46 Train INFO: [Train]: Final Loss=1.2932
2025-07-28 18:59:48 Train INFO: Checkpoint saved at epoch 1
2025-07-28 18:59:48 Train INFO: [Train]: Epoch 2 started (Total iterations: 59)
2025-07-28 19:05:15 Train INFO: [Train]: [002][00010/00058] (18.6%)  Loss=1.3129  cls_loss=0.9869  reg_loss=0.3260  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1426s  iter_time=326.903s  fwd=2.206s/bwd=0.075s/opt=0.011s
2025-07-28 19:10:08 Train INFO: [Train]: [002][00020/00058] (35.6%)  Loss=1.2793  cls_loss=0.9679  reg_loss=0.3114  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1121s  iter_time=292.815s
2025-07-28 19:13:32 Train INFO: [Train]: [002][00030/00058] (52.5%)  Loss=1.2876  cls_loss=0.9750  reg_loss=0.3126  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=745s  iter_time=204.785s
2025-07-28 19:18:21 Train INFO: [Train]: [002][00040/00058] (69.5%)  Loss=1.2823  cls_loss=0.9697  reg_loss=0.3126  lr_det=5.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=489s  iter_time=288.194s
2025-07-28 19:21:48 Train INFO: [Train]: [002][00050/00058] (86.4%)  Loss=1.2707  cls_loss=0.9627  reg_loss=0.3080  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=207s  iter_time=207.388s
2025-07-28 19:24:49 Train INFO: [Train]: [002][00058/00058] (100.0%)  Loss=1.2715  cls_loss=0.9616  reg_loss=0.3099  lr_det=6.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=181.019s
2025-07-28 19:24:50 Train INFO: [Train]: Epoch 2 completed in 1502.3s (avg 25.462s/iter)
2025-07-28 19:24:50 Train INFO: [Train]: Final Loss=1.2715
2025-07-28 19:24:50 Train INFO: [Train]: Epoch 3 started (Total iterations: 59)
2025-07-28 19:30:10 Train INFO: [Train]: [003][00010/00058] (18.6%)  Loss=1.2802  cls_loss=0.9583  reg_loss=0.3219  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1396s  iter_time=320.001s  fwd=2.132s/bwd=0.083s/opt=0.009s
2025-07-28 19:34:45 Train INFO: [Train]: [003][00020/00058] (35.6%)  Loss=1.2266  cls_loss=0.9130  reg_loss=0.3136  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1077s  iter_time=275.297s
2025-07-28 19:37:55 Train INFO: [Train]: [003][00030/00058] (52.5%)  Loss=1.2071  cls_loss=0.8926  reg_loss=0.3145  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=709s  iter_time=189.996s
2025-07-28 19:42:39 Train INFO: [Train]: [003][00040/00058] (69.5%)  Loss=1.1858  cls_loss=0.8720  reg_loss=0.3138  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=469s  iter_time=283.612s
2025-07-28 19:45:46 Train INFO: [Train]: [003][00050/00058] (86.4%)  Loss=1.1555  cls_loss=0.8445  reg_loss=0.3110  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=197s  iter_time=187.270s
2025-07-28 19:48:40 Train INFO: [Train]: [003][00058/00058] (100.0%)  Loss=1.1321  cls_loss=0.8241  reg_loss=0.3080  lr_det=8.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=174.118s
2025-07-28 19:48:42 Train INFO: [Train]: Epoch 3 completed in 1431.4s (avg 24.261s/iter)
2025-07-28 19:48:42 Train INFO: [Train]: Final Loss=1.1321
2025-07-28 19:48:43 Train INFO: Checkpoint saved at epoch 3
2025-07-28 19:48:43 Train INFO: [Train]: Epoch 4 started (Total iterations: 59)
2025-07-28 19:53:54 Train INFO: [Train]: [004][00010/00058] (18.6%)  Loss=1.0889  cls_loss=0.7636  reg_loss=0.3252  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1357s  iter_time=311.052s  fwd=2.063s/bwd=0.041s/opt=0.006s
2025-07-28 19:58:27 Train INFO: [Train]: [004][00020/00058] (35.6%)  Loss=1.0705  cls_loss=0.7504  reg_loss=0.3202  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1057s  iter_time=273.225s
2025-07-28 20:01:45 Train INFO: [Train]: [004][00030/00058] (52.5%)  Loss=1.0430  cls_loss=0.7313  reg_loss=0.3117  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=707s  iter_time=198.257s
2025-07-28 20:06:16 Train INFO: [Train]: [004][00040/00058] (69.5%)  Loss=1.0413  cls_loss=0.7292  reg_loss=0.3121  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=462s  iter_time=270.658s
2025-07-28 20:09:35 Train INFO: [Train]: [004][00050/00058] (86.4%)  Loss=1.0225  cls_loss=0.7167  reg_loss=0.3058  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=197s  iter_time=199.713s
2025-07-28 20:12:28 Train INFO: [Train]: [004][00058/00058] (100.0%)  Loss=1.0322  cls_loss=0.7226  reg_loss=0.3096  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=172.213s
2025-07-28 20:12:29 Train INFO: [Train]: Epoch 4 completed in 1426.3s (avg 24.175s/iter)
2025-07-28 20:12:29 Train INFO: [Train]: Final Loss=1.0322
2025-07-28 20:12:29 Train INFO: [Train]: Epoch 5 started (Total iterations: 59)
2025-07-28 20:17:42 Train INFO: [Train]: [005][00010/00058] (18.6%)  Loss=1.0442  cls_loss=0.7311  reg_loss=0.3131  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1367s  iter_time=313.209s  fwd=2.084s/bwd=0.058s/opt=0.006s
2025-07-28 20:22:22 Train INFO: [Train]: [005][00020/00058] (35.6%)  Loss=1.0195  cls_loss=0.7113  reg_loss=0.3082  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1073s  iter_time=279.884s
2025-07-28 20:25:41 Train INFO: [Train]: [005][00030/00058] (52.5%)  Loss=1.0168  cls_loss=0.7108  reg_loss=0.3060  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=715s  iter_time=198.749s
2025-07-28 20:30:19 Train INFO: [Train]: [005][00040/00058] (69.5%)  Loss=1.0156  cls_loss=0.7105  reg_loss=0.3051  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=470s  iter_time=278.657s
2025-07-28 20:33:35 Train INFO: [Train]: [005][00050/00058] (86.4%)  Loss=1.0172  cls_loss=0.7096  reg_loss=0.3076  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=199s  iter_time=196.119s
2025-07-28 20:36:32 Train INFO: [Train]: [005][00058/00058] (100.0%)  Loss=1.0072  cls_loss=0.7022  reg_loss=0.3050  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=176.539s
2025-07-28 20:36:33 Train INFO: [Train]: Epoch 5 completed in 1444.3s (avg 24.480s/iter)
2025-07-28 20:36:33 Train INFO: [Train]: Final Loss=1.0072
2025-07-28 20:36:34 Train INFO: Checkpoint saved at epoch 5
2025-07-28 20:36:34 Train INFO: [Train]: Epoch 6 started (Total iterations: 59)
2025-07-28 20:42:05 Train INFO: [Train]: [006][00010/00058] (18.6%)  Loss=1.0298  cls_loss=0.7141  reg_loss=0.3157  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1443s  iter_time=330.652s  fwd=2.083s/bwd=0.049s/opt=0.007s
2025-07-28 20:46:43 Train INFO: [Train]: [006][00020/00058] (35.6%)  Loss=1.0201  cls_loss=0.7064  reg_loss=0.3137  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1102s  iter_time=278.103s
2025-07-28 20:50:11 Train INFO: [Train]: [006][00030/00058] (52.5%)  Loss=1.0081  cls_loss=0.6983  reg_loss=0.3098  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=737s  iter_time=207.656s
2025-07-28 20:55:33 Train INFO: [Train]: [006][00040/00058] (69.5%)  Loss=1.0220  cls_loss=0.7075  reg_loss=0.3146  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=500s  iter_time=322.006s
2025-07-28 20:59:09 Train INFO: [Train]: [006][00050/00058] (86.4%)  Loss=1.0134  cls_loss=0.7022  reg_loss=0.3112  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=213s  iter_time=216.851s
2025-07-28 21:02:02 Train INFO: [Train]: [006][00058/00058] (100.0%)  Loss=1.0113  cls_loss=0.7017  reg_loss=0.3096  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=172.217s
2025-07-28 21:02:03 Train INFO: [Train]: Epoch 6 completed in 1528.8s (avg 25.912s/iter)
2025-07-28 21:02:03 Train INFO: [Train]: Final Loss=1.0113
2025-07-28 21:02:03 Train INFO: [Train]: Epoch 7 started (Total iterations: 59)
2025-07-28 21:07:50 Train INFO: [Train]: [007][00010/00058] (18.6%)  Loss=1.0124  cls_loss=0.7008  reg_loss=0.3116  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1514s  iter_time=346.903s  fwd=2.171s/bwd=0.101s/opt=0.014s
2025-07-28 21:12:48 Train INFO: [Train]: [007][00020/00058] (35.6%)  Loss=1.0088  cls_loss=0.6999  reg_loss=0.3089  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1167s  iter_time=297.960s
2025-07-28 21:16:21 Train INFO: [Train]: [007][00030/00058] (52.5%)  Loss=1.0154  cls_loss=0.7032  reg_loss=0.3122  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=775s  iter_time=213.560s
2025-07-28 21:21:19 Train INFO: [Train]: [007][00040/00058] (69.5%)  Loss=1.0133  cls_loss=0.7028  reg_loss=0.3105  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=508s  iter_time=298.064s
2025-07-28 21:24:56 Train INFO: [Train]: [007][00050/00058] (86.4%)  Loss=1.0065  cls_loss=0.6991  reg_loss=0.3074  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=215s  iter_time=216.712s
2025-07-28 21:28:11 Train INFO: [Train]: [007][00058/00058] (100.0%)  Loss=0.9987  cls_loss=0.6941  reg_loss=0.3046  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=194.490s
2025-07-28 21:28:12 Train INFO: [Train]: Epoch 7 completed in 1568.9s (avg 26.592s/iter)
2025-07-28 21:28:12 Train INFO: [Train]: Final Loss=0.9987
2025-07-28 21:28:13 Train INFO: Checkpoint saved at epoch 7
2025-07-28 21:28:13 Train INFO: [Train]: Epoch 8 started (Total iterations: 59)
2025-07-28 21:33:53 Train INFO: [Train]: [008][00010/00058] (18.6%)  Loss=1.0550  cls_loss=0.7314  reg_loss=0.3236  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1482s  iter_time=339.543s  fwd=2.140s/bwd=0.081s/opt=0.011s
2025-07-28 21:38:53 Train INFO: [Train]: [008][00020/00058] (35.6%)  Loss=1.0089  cls_loss=0.6990  reg_loss=0.3099  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1159s  iter_time=300.821s
2025-07-28 21:42:15 Train INFO: [Train]: [008][00030/00058] (52.5%)  Loss=1.0086  cls_loss=0.6992  reg_loss=0.3094  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=760s  iter_time=201.454s
2025-07-28 21:47:14 Train INFO: [Train]: [008][00040/00058] (69.5%)  Loss=1.0155  cls_loss=0.7068  reg_loss=0.3088  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=501s  iter_time=299.344s
2025-07-28 21:50:40 Train INFO: [Train]: [008][00050/00058] (86.4%)  Loss=1.0001  cls_loss=0.6959  reg_loss=0.3042  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=211s  iter_time=205.666s
2025-07-28 21:53:54 Train INFO: [Train]: [008][00058/00058] (100.0%)  Loss=1.0030  cls_loss=0.6974  reg_loss=0.3056  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=194.253s
2025-07-28 21:53:55 Train INFO: [Train]: Epoch 8 completed in 1542.3s (avg 26.140s/iter)
2025-07-28 21:53:55 Train INFO: [Train]: Final Loss=1.0030
2025-07-28 21:53:55 Train INFO: [Train]: Epoch 9 started (Total iterations: 59)
2025-07-28 21:59:02 Train INFO: [Train]: [009][00010/00058] (18.6%)  Loss=1.0418  cls_loss=0.7191  reg_loss=0.3227  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1339s  iter_time=306.785s  fwd=2.049s/bwd=0.054s/opt=0.010s
2025-07-28 22:03:22 Train INFO: [Train]: [009][00020/00058] (35.6%)  Loss=1.0042  cls_loss=0.6946  reg_loss=0.3096  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1026s  iter_time=260.077s
2025-07-28 22:06:28 Train INFO: [Train]: [009][00030/00058] (52.5%)  Loss=1.0116  cls_loss=0.6990  reg_loss=0.3127  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=186.119s
2025-07-28 22:10:52 Train INFO: [Train]: [009][00040/00058] (69.5%)  Loss=0.9980  cls_loss=0.6899  reg_loss=0.3081  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=446s  iter_time=263.900s
2025-07-28 22:14:03 Train INFO: [Train]: [009][00050/00058] (86.4%)  Loss=0.9940  cls_loss=0.6872  reg_loss=0.3067  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=190.427s
2025-07-28 22:16:42 Train INFO: [Train]: [009][00058/00058] (100.0%)  Loss=0.9832  cls_loss=0.6794  reg_loss=0.3038  lr_det=9.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=159.338s
2025-07-28 22:16:43 Train INFO: [Train]: Epoch 9 completed in 1367.4s (avg 23.176s/iter)
2025-07-28 22:16:43 Train INFO: [Train]: Final Loss=0.9832
2025-07-28 22:16:43 Train INFO: Checkpoint saved at epoch 9
2025-07-28 22:16:43 Train INFO: [Train]: Epoch 10 started (Total iterations: 59)
2025-07-28 22:21:48 Train INFO: [Train]: [010][00010/00058] (18.6%)  Loss=1.1050  cls_loss=0.7643  reg_loss=0.3406  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1329s  iter_time=304.587s  fwd=2.052s/bwd=0.059s/opt=0.012s
2025-07-28 22:26:01 Train INFO: [Train]: [010][00020/00058] (35.6%)  Loss=1.0485  cls_loss=0.7258  reg_loss=0.3227  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1009s  iter_time=252.956s
2025-07-28 22:29:18 Train INFO: [Train]: [010][00030/00058] (52.5%)  Loss=1.0291  cls_loss=0.7127  reg_loss=0.3163  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=682s  iter_time=197.350s
2025-07-28 22:33:41 Train INFO: [Train]: [010][00040/00058] (69.5%)  Loss=1.0257  cls_loss=0.7094  reg_loss=0.3162  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=262.724s
2025-07-28 22:36:42 Train INFO: [Train]: [010][00050/00058] (86.4%)  Loss=1.0239  cls_loss=0.7086  reg_loss=0.3153  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=188s  iter_time=180.633s
2025-07-28 22:39:22 Train INFO: [Train]: [010][00058/00058] (100.0%)  Loss=1.0161  cls_loss=0.7033  reg_loss=0.3128  lr_det=9.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.714s
2025-07-28 22:39:23 Train INFO: [Train]: Epoch 10 completed in 1359.7s (avg 23.046s/iter)
2025-07-28 22:39:23 Train INFO: [Train]: Final Loss=1.0161
2025-07-28 22:39:23 Train INFO: [Train]: Epoch 11 started (Total iterations: 59)
2025-07-28 22:44:07 Train INFO: [Train]: [011][00010/00058] (18.6%)  Loss=0.9867  cls_loss=0.6844  reg_loss=0.3022  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1238s  iter_time=283.746s  fwd=2.027s/bwd=0.036s/opt=0.007s
2025-07-28 22:48:19 Train INFO: [Train]: [011][00020/00058] (35.6%)  Loss=1.0043  cls_loss=0.6977  reg_loss=0.3066  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=970s  iter_time=252.449s
2025-07-28 22:51:22 Train INFO: [Train]: [011][00030/00058] (52.5%)  Loss=1.0116  cls_loss=0.7008  reg_loss=0.3107  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=650s  iter_time=183.203s
2025-07-28 22:55:45 Train INFO: [Train]: [011][00040/00058] (69.5%)  Loss=0.9948  cls_loss=0.6891  reg_loss=0.3058  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=431s  iter_time=262.447s
2025-07-28 22:58:48 Train INFO: [Train]: [011][00050/00058] (86.4%)  Loss=1.0001  cls_loss=0.6917  reg_loss=0.3084  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=183s  iter_time=183.353s
2025-07-28 23:01:18 Train INFO: [Train]: [011][00058/00058] (100.0%)  Loss=0.9918  cls_loss=0.6853  reg_loss=0.3065  lr_det=9.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=149.602s
2025-07-28 23:01:19 Train INFO: [Train]: Epoch 11 completed in 1315.5s (avg 22.297s/iter)
2025-07-28 23:01:19 Train INFO: [Train]: Final Loss=0.9918
2025-07-28 23:01:19 Train INFO: Checkpoint saved at epoch 11
2025-07-28 23:01:19 Train INFO: [Train]: Epoch 12 started (Total iterations: 59)
2025-07-28 23:06:00 Train INFO: [Train]: [012][00010/00058] (18.6%)  Loss=0.9986  cls_loss=0.6907  reg_loss=0.3078  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1226s  iter_time=280.958s  fwd=2.048s/bwd=0.035s/opt=0.009s
2025-07-28 23:10:10 Train INFO: [Train]: [012][00020/00058] (35.6%)  Loss=1.0041  cls_loss=0.6942  reg_loss=0.3099  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=960s  iter_time=249.813s
2025-07-28 23:13:11 Train INFO: [Train]: [012][00030/00058] (52.5%)  Loss=0.9865  cls_loss=0.6827  reg_loss=0.3037  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=643s  iter_time=181.602s
2025-07-28 23:17:43 Train INFO: [Train]: [012][00040/00058] (69.5%)  Loss=0.9884  cls_loss=0.6847  reg_loss=0.3037  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=432s  iter_time=271.228s
2025-07-28 23:20:44 Train INFO: [Train]: [012][00050/00058] (86.4%)  Loss=0.9912  cls_loss=0.6864  reg_loss=0.3048  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=183s  iter_time=181.097s
2025-07-28 23:23:19 Train INFO: [Train]: [012][00058/00058] (100.0%)  Loss=0.9877  cls_loss=0.6837  reg_loss=0.3040  lr_det=9.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=155.288s
2025-07-28 23:23:20 Train INFO: [Train]: Epoch 12 completed in 1320.7s (avg 22.385s/iter)
2025-07-28 23:23:20 Train INFO: [Train]: Final Loss=0.9877
2025-07-28 23:23:20 Train INFO: [Train]: Epoch 13 started (Total iterations: 59)
2025-07-28 23:28:26 Train INFO: [Train]: [013][00010/00058] (18.6%)  Loss=1.0349  cls_loss=0.7145  reg_loss=0.3204  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1336s  iter_time=306.060s  fwd=2.077s/bwd=0.053s/opt=0.012s
2025-07-28 23:32:42 Train INFO: [Train]: [013][00020/00058] (35.6%)  Loss=1.0047  cls_loss=0.6946  reg_loss=0.3101  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1016s  iter_time=255.659s
2025-07-28 23:35:57 Train INFO: [Train]: [013][00030/00058] (52.5%)  Loss=0.9922  cls_loss=0.6874  reg_loss=0.3049  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=684s  iter_time=195.877s
2025-07-28 23:40:21 Train INFO: [Train]: [013][00040/00058] (69.5%)  Loss=1.0148  cls_loss=0.7016  reg_loss=0.3132  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=448s  iter_time=263.951s
2025-07-28 23:43:26 Train INFO: [Train]: [013][00050/00058] (86.4%)  Loss=1.0084  cls_loss=0.6980  reg_loss=0.3104  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=184.845s
2025-07-28 23:46:15 Train INFO: [Train]: [013][00058/00058] (100.0%)  Loss=1.0086  cls_loss=0.6987  reg_loss=0.3099  lr_det=9.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=168.704s
2025-07-28 23:46:16 Train INFO: [Train]: Epoch 13 completed in 1375.8s (avg 23.319s/iter)
2025-07-28 23:46:16 Train INFO: [Train]: Final Loss=1.0086
2025-07-28 23:46:16 Train INFO: Checkpoint saved at epoch 13
2025-07-28 23:46:16 Train INFO: [Train]: Epoch 14 started (Total iterations: 59)
2025-07-28 23:51:17 Train INFO: [Train]: [014][00010/00058] (18.6%)  Loss=0.9702  cls_loss=0.6773  reg_loss=0.2929  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1313s  iter_time=300.903s  fwd=2.037s/bwd=0.082s/opt=0.010s
2025-07-28 23:55:42 Train INFO: [Train]: [014][00020/00058] (35.6%)  Loss=0.9590  cls_loss=0.6695  reg_loss=0.2896  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1023s  iter_time=264.330s
2025-07-28 23:58:50 Train INFO: [Train]: [014][00030/00058] (52.5%)  Loss=0.9857  cls_loss=0.6859  reg_loss=0.2998  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=681s  iter_time=188.298s
2025-07-29 00:02:56 Train INFO: [Train]: [014][00040/00058] (69.5%)  Loss=0.9838  cls_loss=0.6854  reg_loss=0.2984  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=439s  iter_time=245.730s
2025-07-29 00:06:07 Train INFO: [Train]: [014][00050/00058] (86.4%)  Loss=0.9827  cls_loss=0.6843  reg_loss=0.2985  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=190.995s
2025-07-29 00:08:49 Train INFO: [Train]: [014][00058/00058] (100.0%)  Loss=0.9819  cls_loss=0.6832  reg_loss=0.2987  lr_det=9.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=162.167s
2025-07-29 00:08:49 Train INFO: [Train]: Epoch 14 completed in 1353.2s (avg 22.935s/iter)
2025-07-29 00:08:49 Train INFO: [Train]: Final Loss=0.9819
2025-07-29 00:08:49 Train INFO: [Train]: Epoch 15 started (Total iterations: 59)
2025-07-29 00:13:44 Train INFO: [Train]: [015][00010/00058] (18.6%)  Loss=1.0217  cls_loss=0.7073  reg_loss=0.3144  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1286s  iter_time=294.668s  fwd=2.052s/bwd=0.087s/opt=0.010s
2025-07-29 00:17:55 Train INFO: [Train]: [015][00020/00058] (35.6%)  Loss=1.0292  cls_loss=0.7113  reg_loss=0.3179  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=986s  iter_time=250.481s
2025-07-29 00:21:04 Train INFO: [Train]: [015][00030/00058] (52.5%)  Loss=1.0193  cls_loss=0.7061  reg_loss=0.3132  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=663s  iter_time=188.981s
2025-07-29 00:25:22 Train INFO: [Train]: [015][00040/00058] (69.5%)  Loss=1.0230  cls_loss=0.7085  reg_loss=0.3145  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=436s  iter_time=257.983s
2025-07-29 00:28:32 Train INFO: [Train]: [015][00050/00058] (86.4%)  Loss=1.0065  cls_loss=0.6968  reg_loss=0.3097  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=186s  iter_time=190.494s
2025-07-29 00:31:08 Train INFO: [Train]: [015][00058/00058] (100.0%)  Loss=0.9933  cls_loss=0.6874  reg_loss=0.3059  lr_det=9.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=155.758s
2025-07-29 00:31:09 Train INFO: [Train]: Epoch 15 completed in 1339.1s (avg 22.697s/iter)
2025-07-29 00:31:09 Train INFO: [Train]: Final Loss=0.9933
2025-07-29 00:31:09 Train INFO: Checkpoint saved at epoch 15
2025-07-29 00:31:09 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-29 00:36:12 Train INFO: [Train]: [016][00010/00058] (18.6%)  Loss=1.0779  cls_loss=0.7418  reg_loss=0.3361  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1320s  iter_time=302.546s  fwd=2.066s/bwd=0.044s/opt=0.004s
2025-07-29 00:40:31 Train INFO: [Train]: [016][00020/00058] (35.6%)  Loss=1.0174  cls_loss=0.7011  reg_loss=0.3163  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1016s  iter_time=259.047s
2025-07-29 00:43:52 Train INFO: [Train]: [016][00030/00058] (52.5%)  Loss=1.0021  cls_loss=0.6930  reg_loss=0.3092  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=689s  iter_time=201.156s
2025-07-29 00:48:26 Train INFO: [Train]: [016][00040/00058] (69.5%)  Loss=0.9996  cls_loss=0.6906  reg_loss=0.3091  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=455s  iter_time=274.413s
2025-07-29 00:51:41 Train INFO: [Train]: [016][00050/00058] (86.4%)  Loss=1.0080  cls_loss=0.6960  reg_loss=0.3120  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=193s  iter_time=194.792s
2025-07-29 00:54:20 Train INFO: [Train]: [016][00058/00058] (100.0%)  Loss=1.0004  cls_loss=0.6915  reg_loss=0.3089  lr_det=8.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=158.885s
2025-07-29 00:54:21 Train INFO: [Train]: Epoch 16 completed in 1391.6s (avg 23.586s/iter)
2025-07-29 00:54:21 Train INFO: [Train]: Final Loss=1.0004
2025-07-29 00:54:21 Train INFO: [Train]: Epoch 17 started (Total iterations: 59)
2025-07-29 00:59:16 Train INFO: [Train]: [017][00010/00058] (18.6%)  Loss=1.0374  cls_loss=0.7173  reg_loss=0.3202  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1287s  iter_time=294.993s  fwd=2.075s/bwd=0.065s/opt=0.011s
2025-07-29 01:03:43 Train INFO: [Train]: [017][00020/00058] (35.6%)  Loss=0.9765  cls_loss=0.6779  reg_loss=0.2986  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1017s  iter_time=267.141s
2025-07-29 01:06:49 Train INFO: [Train]: [017][00030/00058] (52.5%)  Loss=0.9774  cls_loss=0.6784  reg_loss=0.2990  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=676s  iter_time=186.058s
2025-07-29 01:11:12 Train INFO: [Train]: [017][00040/00058] (69.5%)  Loss=0.9962  cls_loss=0.6913  reg_loss=0.3049  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=444s  iter_time=263.244s
2025-07-29 01:14:15 Train INFO: [Train]: [017][00050/00058] (86.4%)  Loss=0.9916  cls_loss=0.6875  reg_loss=0.3041  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=183.235s
2025-07-29 01:16:50 Train INFO: [Train]: [017][00058/00058] (100.0%)  Loss=0.9795  cls_loss=0.6786  reg_loss=0.3008  lr_det=8.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=154.633s
2025-07-29 01:16:51 Train INFO: [Train]: Epoch 17 completed in 1350.0s (avg 22.882s/iter)
2025-07-29 01:16:51 Train INFO: [Train]: Final Loss=0.9795
2025-07-29 01:16:51 Train INFO: Checkpoint saved at epoch 17
2025-07-29 01:16:51 Train INFO: [Train]: Epoch 18 started (Total iterations: 59)
2025-07-29 01:21:42 Train INFO: [Train]: [018][00010/00058] (18.6%)  Loss=1.0358  cls_loss=0.7280  reg_loss=0.3077  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1267s  iter_time=290.296s  fwd=2.064s/bwd=0.044s/opt=0.004s
2025-07-29 01:26:17 Train INFO: [Train]: [018][00020/00058] (35.6%)  Loss=1.0281  cls_loss=0.7216  reg_loss=0.3065  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1023s  iter_time=275.019s
2025-07-29 01:29:16 Train INFO: [Train]: [018][00030/00058] (52.5%)  Loss=1.0185  cls_loss=0.7130  reg_loss=0.3055  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=673s  iter_time=179.767s
2025-07-29 01:33:32 Train INFO: [Train]: [018][00040/00058] (69.5%)  Loss=1.0004  cls_loss=0.6989  reg_loss=0.3015  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=439s  iter_time=255.599s
2025-07-29 01:36:51 Train INFO: [Train]: [018][00050/00058] (86.4%)  Loss=0.9941  cls_loss=0.6955  reg_loss=0.2986  lr_det=8.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=188s  iter_time=198.469s
2025-07-29 01:39:32 Train INFO: [Train]: [018][00058/00058] (100.0%)  Loss=0.9894  cls_loss=0.6906  reg_loss=0.2988  lr_det=8.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=161.447s
2025-07-29 01:39:33 Train INFO: [Train]: Epoch 18 completed in 1361.3s (avg 23.073s/iter)
2025-07-29 01:39:33 Train INFO: [Train]: Final Loss=0.9894
2025-07-29 01:39:33 Train INFO: [Train]: Epoch 19 started (Total iterations: 59)
2025-07-29 01:44:26 Train INFO: [Train]: [019][00010/00058] (18.6%)  Loss=1.0044  cls_loss=0.6977  reg_loss=0.3066  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1282s  iter_time=293.734s  fwd=2.068s/bwd=0.056s/opt=0.004s
2025-07-29 01:49:05 Train INFO: [Train]: [019][00020/00058] (35.6%)  Loss=1.0005  cls_loss=0.6938  reg_loss=0.3067  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=278.365s
2025-07-29 01:52:24 Train INFO: [Train]: [019][00030/00058] (52.5%)  Loss=1.0035  cls_loss=0.6945  reg_loss=0.3090  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=697s  iter_time=199.453s
2025-07-29 01:56:31 Train INFO: [Train]: [019][00040/00058] (69.5%)  Loss=0.9998  cls_loss=0.6912  reg_loss=0.3085  lr_det=8.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=246.476s
2025-07-29 01:59:37 Train INFO: [Train]: [019][00050/00058] (86.4%)  Loss=0.9914  cls_loss=0.6863  reg_loss=0.3051  lr_det=8.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=186.423s
2025-07-29 02:02:18 Train INFO: [Train]: [019][00058/00058] (100.0%)  Loss=0.9911  cls_loss=0.6861  reg_loss=0.3051  lr_det=8.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.903s
2025-07-29 02:02:19 Train INFO: [Train]: Epoch 19 completed in 1366.1s (avg 23.154s/iter)
2025-07-29 02:02:19 Train INFO: [Train]: Final Loss=0.9911
2025-07-29 02:02:19 Train INFO: Checkpoint saved at epoch 19
2025-07-29 02:02:19 Train INFO: [Train]: Epoch 20 started (Total iterations: 59)
2025-07-29 02:07:22 Train INFO: [Train]: [020][00010/00058] (18.6%)  Loss=1.0233  cls_loss=0.7073  reg_loss=0.3161  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1319s  iter_time=302.180s  fwd=2.082s/bwd=0.081s/opt=0.012s
2025-07-29 02:11:36 Train INFO: [Train]: [020][00020/00058] (35.6%)  Loss=0.9905  cls_loss=0.6852  reg_loss=0.3053  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1008s  iter_time=254.622s
2025-07-29 02:14:34 Train INFO: [Train]: [020][00030/00058] (52.5%)  Loss=0.9893  cls_loss=0.6858  reg_loss=0.3035  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=663s  iter_time=177.607s
2025-07-29 02:19:06 Train INFO: [Train]: [020][00040/00058] (69.5%)  Loss=0.9947  cls_loss=0.6884  reg_loss=0.3063  lr_det=8.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=442s  iter_time=272.393s
2025-07-29 02:22:09 Train INFO: [Train]: [020][00050/00058] (86.4%)  Loss=0.9886  cls_loss=0.6843  reg_loss=0.3044  lr_det=8.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=182.373s
2025-07-29 02:24:49 Train INFO: [Train]: [020][00058/00058] (100.0%)  Loss=0.9920  cls_loss=0.6868  reg_loss=0.3053  lr_det=8.1e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.004s
2025-07-29 02:24:49 Train INFO: [Train]: Epoch 20 completed in 1349.9s (avg 22.880s/iter)
2025-07-29 02:24:49 Train INFO: [Train]: Final Loss=0.9920
2025-07-29 02:24:49 Train INFO: [Train]: Epoch 21 started (Total iterations: 59)
2025-07-29 02:29:52 Train INFO: [Train]: [021][00010/00058] (18.6%)  Loss=0.9807  cls_loss=0.6789  reg_loss=0.3018  lr_det=8.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1319s  iter_time=302.372s  fwd=2.061s/bwd=0.075s/opt=0.007s
2025-07-29 02:34:04 Train INFO: [Train]: [021][00020/00058] (35.6%)  Loss=0.9685  cls_loss=0.6699  reg_loss=0.2986  lr_det=8.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1003s  iter_time=251.956s
2025-07-29 02:37:27 Train INFO: [Train]: [021][00030/00058] (52.5%)  Loss=0.9836  cls_loss=0.6823  reg_loss=0.3013  lr_det=7.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=684s  iter_time=203.128s
2025-07-29 02:41:37 Train INFO: [Train]: [021][00040/00058] (69.5%)  Loss=0.9842  cls_loss=0.6838  reg_loss=0.3004  lr_det=7.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=442s  iter_time=249.697s
2025-07-29 02:44:54 Train INFO: [Train]: [021][00050/00058] (86.4%)  Loss=0.9759  cls_loss=0.6779  reg_loss=0.2979  lr_det=7.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=197.699s
2025-07-29 02:47:20 Train INFO: [Train]: [021][00058/00058] (100.0%)  Loss=0.9758  cls_loss=0.6781  reg_loss=0.2977  lr_det=7.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=146.265s
2025-07-29 02:47:21 Train INFO: [Train]: Epoch 21 completed in 1351.9s (avg 22.913s/iter)
2025-07-29 02:47:21 Train INFO: [Train]: Final Loss=0.9758
2025-07-29 02:47:22 Train INFO: Checkpoint saved at epoch 21
2025-07-29 02:47:22 Train INFO: [Train]: Epoch 22 started (Total iterations: 59)
2025-07-29 02:52:29 Train INFO: [Train]: [022][00010/00058] (18.6%)  Loss=1.0247  cls_loss=0.7063  reg_loss=0.3185  lr_det=7.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1341s  iter_time=307.335s  fwd=2.025s/bwd=0.034s/opt=0.005s
2025-07-29 02:56:32 Train INFO: [Train]: [022][00020/00058] (35.6%)  Loss=0.9934  cls_loss=0.6878  reg_loss=0.3056  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=996s  iter_time=242.937s
2025-07-29 02:59:38 Train INFO: [Train]: [022][00030/00058] (52.5%)  Loss=1.0015  cls_loss=0.6942  reg_loss=0.3074  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=665s  iter_time=185.852s
2025-07-29 03:04:02 Train INFO: [Train]: [022][00040/00058] (69.5%)  Loss=0.9936  cls_loss=0.6884  reg_loss=0.3052  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=439s  iter_time=263.742s
2025-07-29 03:07:16 Train INFO: [Train]: [022][00050/00058] (86.4%)  Loss=0.9940  cls_loss=0.6885  reg_loss=0.3056  lr_det=7.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=193.860s
2025-07-29 03:09:56 Train INFO: [Train]: [022][00058/00058] (100.0%)  Loss=0.9843  cls_loss=0.6816  reg_loss=0.3028  lr_det=7.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.146s
2025-07-29 03:09:56 Train INFO: [Train]: Epoch 22 completed in 1354.6s (avg 22.960s/iter)
2025-07-29 03:09:56 Train INFO: [Train]: Final Loss=0.9843
2025-07-29 03:09:56 Train INFO: [Train]: Epoch 23 started (Total iterations: 59)
2025-07-29 03:15:12 Train INFO: [Train]: [023][00010/00058] (18.6%)  Loss=0.9787  cls_loss=0.6793  reg_loss=0.2994  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1376s  iter_time=315.309s  fwd=2.080s/bwd=0.071s/opt=0.005s
2025-07-29 03:19:28 Train INFO: [Train]: [023][00020/00058] (35.6%)  Loss=0.9750  cls_loss=0.6753  reg_loss=0.2996  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=256.748s
2025-07-29 03:22:38 Train INFO: [Train]: [023][00030/00058] (52.5%)  Loss=0.9913  cls_loss=0.6869  reg_loss=0.3045  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=688s  iter_time=189.163s
2025-07-29 03:26:54 Train INFO: [Train]: [023][00040/00058] (69.5%)  Loss=0.9996  cls_loss=0.6915  reg_loss=0.3081  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=256.413s
2025-07-29 03:29:59 Train INFO: [Train]: [023][00050/00058] (86.4%)  Loss=0.9915  cls_loss=0.6849  reg_loss=0.3066  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=185.299s
2025-07-29 03:32:41 Train INFO: [Train]: [023][00058/00058] (100.0%)  Loss=0.9890  cls_loss=0.6833  reg_loss=0.3057  lr_det=7.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=161.634s
2025-07-29 03:32:42 Train INFO: [Train]: Epoch 23 completed in 1365.3s (avg 23.141s/iter)
2025-07-29 03:32:42 Train INFO: [Train]: Final Loss=0.9890
2025-07-29 03:32:42 Train INFO: Checkpoint saved at epoch 23
2025-07-29 03:32:42 Train INFO: [Train]: Epoch 24 started (Total iterations: 59)
2025-07-29 03:37:41 Train INFO: [Train]: [024][00010/00058] (18.6%)  Loss=0.9791  cls_loss=0.6820  reg_loss=0.2971  lr_det=7.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1302s  iter_time=298.439s  fwd=2.067s/bwd=0.082s/opt=0.012s
2025-07-29 03:42:09 Train INFO: [Train]: [024][00020/00058] (35.6%)  Loss=0.9741  cls_loss=0.6752  reg_loss=0.2989  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1025s  iter_time=268.202s
2025-07-29 03:45:33 Train INFO: [Train]: [024][00030/00058] (52.5%)  Loss=0.9803  cls_loss=0.6783  reg_loss=0.3021  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=696s  iter_time=204.319s
2025-07-29 03:49:47 Train INFO: [Train]: [024][00040/00058] (69.5%)  Loss=0.9913  cls_loss=0.6887  reg_loss=0.3026  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=450s  iter_time=253.599s
2025-07-29 03:53:01 Train INFO: [Train]: [024][00050/00058] (86.4%)  Loss=0.9840  cls_loss=0.6841  reg_loss=0.2999  lr_det=7.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=191s  iter_time=194.416s
2025-07-29 03:55:35 Train INFO: [Train]: [024][00058/00058] (100.0%)  Loss=0.9755  cls_loss=0.6778  reg_loss=0.2977  lr_det=7.1e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.484s
2025-07-29 03:55:36 Train INFO: [Train]: Epoch 24 completed in 1373.2s (avg 23.274s/iter)
2025-07-29 03:55:36 Train INFO: [Train]: Final Loss=0.9755
2025-07-29 03:55:36 Train INFO: [Train]: Epoch 25 started (Total iterations: 59)
2025-07-29 04:00:34 Train INFO: [Train]: [025][00010/00058] (18.6%)  Loss=0.9951  cls_loss=0.6896  reg_loss=0.3054  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1304s  iter_time=298.736s  fwd=2.107s/bwd=0.073s/opt=0.011s
2025-07-29 04:04:52 Train INFO: [Train]: [025][00020/00058] (35.6%)  Loss=0.9966  cls_loss=0.6908  reg_loss=0.3058  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1007s  iter_time=257.769s
2025-07-29 04:08:05 Train INFO: [Train]: [025][00030/00058] (52.5%)  Loss=1.0013  cls_loss=0.6933  reg_loss=0.3081  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=677s  iter_time=192.629s
2025-07-29 04:12:11 Train INFO: [Train]: [025][00040/00058] (69.5%)  Loss=0.9865  cls_loss=0.6812  reg_loss=0.3053  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=437s  iter_time=246.652s
2025-07-29 04:15:28 Train INFO: [Train]: [025][00050/00058] (86.4%)  Loss=0.9788  cls_loss=0.6758  reg_loss=0.3030  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=196.897s
2025-07-29 04:18:08 Train INFO: [Train]: [025][00058/00058] (100.0%)  Loss=0.9771  cls_loss=0.6750  reg_loss=0.3021  lr_det=6.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.194s
2025-07-29 04:18:09 Train INFO: [Train]: Epoch 25 completed in 1353.6s (avg 22.943s/iter)
2025-07-29 04:18:09 Train INFO: [Train]: Final Loss=0.9771
2025-07-29 04:18:10 Train INFO: Checkpoint saved at epoch 25
2025-07-29 04:18:10 Train INFO: [Train]: Epoch 26 started (Total iterations: 59)
2025-07-29 04:22:52 Train INFO: [Train]: [026][00010/00058] (18.6%)  Loss=1.0350  cls_loss=0.7116  reg_loss=0.3235  lr_det=6.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1233s  iter_time=282.493s  fwd=2.026s/bwd=0.035s/opt=0.005s
2025-07-29 04:27:10 Train INFO: [Train]: [026][00020/00058] (35.6%)  Loss=1.0049  cls_loss=0.6936  reg_loss=0.3112  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=978s  iter_time=257.729s
2025-07-29 04:30:17 Train INFO: [Train]: [026][00030/00058] (52.5%)  Loss=0.9996  cls_loss=0.6910  reg_loss=0.3086  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=656s  iter_time=186.546s
2025-07-29 04:34:35 Train INFO: [Train]: [026][00040/00058] (69.5%)  Loss=0.9937  cls_loss=0.6883  reg_loss=0.3054  lr_det=6.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=432s  iter_time=258.333s
2025-07-29 04:37:27 Train INFO: [Train]: [026][00050/00058] (86.4%)  Loss=0.9868  cls_loss=0.6847  reg_loss=0.3021  lr_det=6.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=182s  iter_time=172.094s
2025-07-29 04:40:00 Train INFO: [Train]: [026][00058/00058] (100.0%)  Loss=0.9867  cls_loss=0.6835  reg_loss=0.3032  lr_det=6.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.048s
2025-07-29 04:40:01 Train INFO: [Train]: Epoch 26 completed in 1311.0s (avg 22.220s/iter)
2025-07-29 04:40:01 Train INFO: [Train]: Final Loss=0.9867
2025-07-29 04:40:01 Train INFO: [Train]: Epoch 27 started (Total iterations: 59)
2025-07-29 04:44:44 Train INFO: [Train]: [027][00010/00058] (18.6%)  Loss=0.9966  cls_loss=0.6892  reg_loss=0.3075  lr_det=6.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1234s  iter_time=282.863s  fwd=2.041s/bwd=0.056s/opt=0.004s
2025-07-29 04:49:02 Train INFO: [Train]: [027][00020/00058] (35.6%)  Loss=0.9908  cls_loss=0.6877  reg_loss=0.3031  lr_det=6.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=979s  iter_time=257.972s
2025-07-29 04:52:15 Train INFO: [Train]: [027][00030/00058] (52.5%)  Loss=0.9985  cls_loss=0.6931  reg_loss=0.3053  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=663s  iter_time=193.418s
2025-07-29 04:56:35 Train INFO: [Train]: [027][00040/00058] (69.5%)  Loss=1.0023  cls_loss=0.6957  reg_loss=0.3066  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=436s  iter_time=259.827s
2025-07-29 04:59:37 Train INFO: [Train]: [027][00050/00058] (86.4%)  Loss=0.9859  cls_loss=0.6826  reg_loss=0.3032  lr_det=6.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=184s  iter_time=181.802s
2025-07-29 05:02:20 Train INFO: [Train]: [027][00058/00058] (100.0%)  Loss=0.9791  cls_loss=0.6777  reg_loss=0.3014  lr_det=6.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=162.936s
2025-07-29 05:02:20 Train INFO: [Train]: Epoch 27 completed in 1339.6s (avg 22.704s/iter)
2025-07-29 05:02:20 Train INFO: [Train]: Final Loss=0.9791
2025-07-29 05:02:21 Train INFO: Checkpoint saved at epoch 27
2025-07-29 05:02:21 Train INFO: [Train]: Epoch 28 started (Total iterations: 59)
2025-07-29 05:07:07 Train INFO: [Train]: [028][00010/00058] (18.6%)  Loss=0.9995  cls_loss=0.6912  reg_loss=0.3083  lr_det=6.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1249s  iter_time=286.131s  fwd=2.057s/bwd=0.066s/opt=0.012s
2025-07-29 05:11:29 Train INFO: [Train]: [028][00020/00058] (35.6%)  Loss=1.0123  cls_loss=0.7001  reg_loss=0.3122  lr_det=6.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=992s  iter_time=262.340s
2025-07-29 05:14:40 Train INFO: [Train]: [028][00030/00058] (52.5%)  Loss=0.9987  cls_loss=0.6892  reg_loss=0.3095  lr_det=6.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=667s  iter_time=190.341s
2025-07-29 05:18:55 Train INFO: [Train]: [028][00040/00058] (69.5%)  Loss=0.9984  cls_loss=0.6890  reg_loss=0.3094  lr_det=6.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=437s  iter_time=255.490s
2025-07-29 05:22:14 Train INFO: [Train]: [028][00050/00058] (86.4%)  Loss=0.9902  cls_loss=0.6832  reg_loss=0.3069  lr_det=6.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=199.190s
2025-07-29 05:25:02 Train INFO: [Train]: [028][00058/00058] (100.0%)  Loss=0.9841  cls_loss=0.6796  reg_loss=0.3045  lr_det=6.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=167.542s
2025-07-29 05:25:03 Train INFO: [Train]: Epoch 28 completed in 1361.8s (avg 23.081s/iter)
2025-07-29 05:25:03 Train INFO: [Train]: Final Loss=0.9841
2025-07-29 05:25:03 Train INFO: [Train]: Epoch 29 started (Total iterations: 59)
2025-07-29 05:29:53 Train INFO: [Train]: [029][00010/00058] (18.6%)  Loss=0.9851  cls_loss=0.6853  reg_loss=0.2998  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1268s  iter_time=290.697s  fwd=2.066s/bwd=0.055s/opt=0.007s
2025-07-29 05:34:26 Train INFO: [Train]: [029][00020/00058] (35.6%)  Loss=0.9926  cls_loss=0.6891  reg_loss=0.3035  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1019s  iter_time=272.631s
2025-07-29 05:37:30 Train INFO: [Train]: [029][00030/00058] (52.5%)  Loss=0.9706  cls_loss=0.6731  reg_loss=0.2975  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=675s  iter_time=184.112s
2025-07-29 05:41:50 Train INFO: [Train]: [029][00040/00058] (69.5%)  Loss=0.9717  cls_loss=0.6736  reg_loss=0.2981  lr_det=5.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=442s  iter_time=260.266s
2025-07-29 05:45:03 Train INFO: [Train]: [029][00050/00058] (86.4%)  Loss=0.9697  cls_loss=0.6720  reg_loss=0.2977  lr_det=5.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=188s  iter_time=192.906s
2025-07-29 05:47:37 Train INFO: [Train]: [029][00058/00058] (100.0%)  Loss=0.9636  cls_loss=0.6667  reg_loss=0.2970  lr_det=5.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.961s
2025-07-29 05:47:38 Train INFO: [Train]: Epoch 29 completed in 1355.3s (avg 22.972s/iter)
2025-07-29 05:47:38 Train INFO: [Train]: Final Loss=0.9636
2025-07-29 05:47:39 Train INFO: Checkpoint saved at epoch 29
2025-07-29 05:47:39 Train INFO: [Train]: Epoch 30 started (Total iterations: 59)
2025-07-29 05:52:33 Train INFO: [Train]: [030][00010/00058] (18.6%)  Loss=1.0376  cls_loss=0.7210  reg_loss=0.3166  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1284s  iter_time=294.352s  fwd=2.046s/bwd=0.032s/opt=0.004s
2025-07-29 05:56:45 Train INFO: [Train]: [030][00020/00058] (35.6%)  Loss=1.0262  cls_loss=0.7129  reg_loss=0.3133  lr_det=5.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=989s  iter_time=252.189s
2025-07-29 06:00:09 Train INFO: [Train]: [030][00030/00058] (52.5%)  Loss=1.0075  cls_loss=0.7013  reg_loss=0.3062  lr_det=5.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=678s  iter_time=203.649s
2025-07-29 06:04:47 Train INFO: [Train]: [030][00040/00058] (69.5%)  Loss=0.9992  cls_loss=0.6956  reg_loss=0.3036  lr_det=5.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=452s  iter_time=278.709s
2025-07-29 06:07:52 Train INFO: [Train]: [030][00050/00058] (86.4%)  Loss=0.9925  cls_loss=0.6905  reg_loss=0.3020  lr_det=5.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=190s  iter_time=184.900s
2025-07-29 06:10:42 Train INFO: [Train]: [030][00058/00058] (100.0%)  Loss=0.9888  cls_loss=0.6871  reg_loss=0.3017  lr_det=5.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=169.462s
2025-07-29 06:10:43 Train INFO: [Train]: Epoch 30 completed in 1384.0s (avg 23.458s/iter)
2025-07-29 06:10:43 Train INFO: [Train]: Final Loss=0.9888
2025-07-29 06:10:43 Train INFO: [Train]: Epoch 31 started (Total iterations: 59)
2025-07-29 06:15:28 Train INFO: [Train]: [031][00010/00058] (18.6%)  Loss=0.9857  cls_loss=0.6798  reg_loss=0.3059  lr_det=5.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1244s  iter_time=285.092s  fwd=2.044s/bwd=0.041s/opt=0.005s
2025-07-29 06:19:43 Train INFO: [Train]: [031][00020/00058] (35.6%)  Loss=0.9871  cls_loss=0.6827  reg_loss=0.3044  lr_det=5.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=977s  iter_time=254.881s
2025-07-29 06:22:39 Train INFO: [Train]: [031][00030/00058] (52.5%)  Loss=0.9919  cls_loss=0.6851  reg_loss=0.3068  lr_det=5.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=647s  iter_time=176.413s
2025-07-29 06:26:57 Train INFO: [Train]: [031][00040/00058] (69.5%)  Loss=0.9926  cls_loss=0.6852  reg_loss=0.3074  lr_det=5.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=428s  iter_time=258.363s
2025-07-29 06:30:04 Train INFO: [Train]: [031][00050/00058] (86.4%)  Loss=0.9827  cls_loss=0.6784  reg_loss=0.3043  lr_det=5.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=182s  iter_time=187.046s
2025-07-29 06:32:52 Train INFO: [Train]: [031][00058/00058] (100.0%)  Loss=0.9756  cls_loss=0.6741  reg_loss=0.3015  lr_det=5.1e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=167.317s
2025-07-29 06:32:52 Train INFO: [Train]: Epoch 31 completed in 1329.9s (avg 22.540s/iter)
2025-07-29 06:32:52 Train INFO: [Train]: Final Loss=0.9756
2025-07-29 06:32:53 Train INFO: Checkpoint saved at epoch 31
2025-07-29 06:32:53 Train INFO: [Train]: Epoch 32 started (Total iterations: 59)
2025-07-29 06:37:50 Train INFO: [Train]: [032][00010/00058] (18.6%)  Loss=1.0418  cls_loss=0.7245  reg_loss=0.3173  lr_det=5.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1295s  iter_time=296.820s  fwd=2.079s/bwd=0.065s/opt=0.006s
2025-07-29 06:42:27 Train INFO: [Train]: [032][00020/00058] (35.6%)  Loss=1.0022  cls_loss=0.6969  reg_loss=0.3054  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1038s  iter_time=277.037s
2025-07-29 06:45:36 Train INFO: [Train]: [032][00030/00058] (52.5%)  Loss=0.9865  cls_loss=0.6851  reg_loss=0.3014  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=689s  iter_time=188.725s
2025-07-29 06:49:57 Train INFO: [Train]: [032][00040/00058] (69.5%)  Loss=0.9942  cls_loss=0.6895  reg_loss=0.3047  lr_det=4.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=450s  iter_time=261.725s
2025-07-29 06:53:00 Train INFO: [Train]: [032][00050/00058] (86.4%)  Loss=0.9792  cls_loss=0.6780  reg_loss=0.3012  lr_det=4.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=182.790s
2025-07-29 06:55:21 Train INFO: [Train]: [032][00058/00058] (100.0%)  Loss=0.9780  cls_loss=0.6771  reg_loss=0.3009  lr_det=4.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=140.807s
2025-07-29 06:55:22 Train INFO: [Train]: Epoch 32 completed in 1348.6s (avg 22.858s/iter)
2025-07-29 06:55:22 Train INFO: [Train]: Final Loss=0.9780
2025-07-29 06:55:22 Train INFO: [Train]: Epoch 33 started (Total iterations: 59)
2025-07-29 07:00:11 Train INFO: [Train]: [033][00010/00058] (18.6%)  Loss=0.9980  cls_loss=0.6858  reg_loss=0.3122  lr_det=4.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1265s  iter_time=289.826s  fwd=2.060s/bwd=0.074s/opt=0.007s
2025-07-29 07:04:44 Train INFO: [Train]: [033][00020/00058] (35.6%)  Loss=0.9477  cls_loss=0.6517  reg_loss=0.2960  lr_det=4.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1018s  iter_time=272.609s
2025-07-29 07:08:09 Train INFO: [Train]: [033][00030/00058] (52.5%)  Loss=0.9731  cls_loss=0.6708  reg_loss=0.3023  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=693s  iter_time=205.178s
2025-07-29 07:12:21 Train INFO: [Train]: [033][00040/00058] (69.5%)  Loss=0.9724  cls_loss=0.6677  reg_loss=0.3047  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=251.672s
2025-07-29 07:15:30 Train INFO: [Train]: [033][00050/00058] (86.4%)  Loss=0.9786  cls_loss=0.6726  reg_loss=0.3059  lr_det=4.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=188.759s
2025-07-29 07:18:20 Train INFO: [Train]: [033][00058/00058] (100.0%)  Loss=0.9732  cls_loss=0.6685  reg_loss=0.3047  lr_det=4.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=170.449s
2025-07-29 07:18:21 Train INFO: [Train]: Epoch 33 completed in 1379.3s (avg 23.377s/iter)
2025-07-29 07:18:21 Train INFO: [Train]: Final Loss=0.9732
2025-07-29 07:18:21 Train INFO: Checkpoint saved at epoch 33
2025-07-29 07:18:21 Train INFO: [Train]: Epoch 34 started (Total iterations: 59)
2025-07-29 07:23:14 Train INFO: [Train]: [034][00010/00058] (18.6%)  Loss=1.0427  cls_loss=0.7174  reg_loss=0.3253  lr_det=4.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1277s  iter_time=292.652s  fwd=2.059s/bwd=0.063s/opt=0.006s
2025-07-29 07:27:23 Train INFO: [Train]: [034][00020/00058] (35.6%)  Loss=0.9903  cls_loss=0.6823  reg_loss=0.3079  lr_det=4.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=980s  iter_time=249.178s
2025-07-29 07:30:24 Train INFO: [Train]: [034][00030/00058] (52.5%)  Loss=0.9969  cls_loss=0.6868  reg_loss=0.3101  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=653s  iter_time=180.888s
2025-07-29 07:34:38 Train INFO: [Train]: [034][00040/00058] (69.5%)  Loss=0.9821  cls_loss=0.6773  reg_loss=0.3048  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=429s  iter_time=253.860s
2025-07-29 07:37:58 Train INFO: [Train]: [034][00050/00058] (86.4%)  Loss=0.9860  cls_loss=0.6793  reg_loss=0.3068  lr_det=4.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=185s  iter_time=199.692s
2025-07-29 07:40:32 Train INFO: [Train]: [034][00058/00058] (100.0%)  Loss=0.9855  cls_loss=0.6809  reg_loss=0.3045  lr_det=4.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=154.221s
2025-07-29 07:40:33 Train INFO: [Train]: Epoch 34 completed in 1331.2s (avg 22.563s/iter)
2025-07-29 07:40:33 Train INFO: [Train]: Final Loss=0.9855
2025-07-29 07:40:33 Train INFO: [Train]: Epoch 35 started (Total iterations: 59)
2025-07-29 07:45:29 Train INFO: [Train]: [035][00010/00058] (18.6%)  Loss=0.9902  cls_loss=0.6797  reg_loss=0.3105  lr_det=4.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1294s  iter_time=296.589s  fwd=2.064s/bwd=0.086s/opt=0.012s
2025-07-29 07:49:54 Train INFO: [Train]: [035][00020/00058] (35.6%)  Loss=0.9739  cls_loss=0.6705  reg_loss=0.3034  lr_det=4.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1016s  iter_time=264.866s
2025-07-29 07:53:19 Train INFO: [Train]: [035][00030/00058] (52.5%)  Loss=0.9713  cls_loss=0.6681  reg_loss=0.3032  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=692s  iter_time=204.652s
2025-07-29 07:57:32 Train INFO: [Train]: [035][00040/00058] (69.5%)  Loss=0.9732  cls_loss=0.6718  reg_loss=0.3014  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=252.865s
2025-07-29 08:00:43 Train INFO: [Train]: [035][00050/00058] (86.4%)  Loss=0.9682  cls_loss=0.6687  reg_loss=0.2995  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=190s  iter_time=191.035s
2025-07-29 08:03:31 Train INFO: [Train]: [035][00058/00058] (100.0%)  Loss=0.9613  cls_loss=0.6643  reg_loss=0.2971  lr_det=4.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=168.279s
2025-07-29 08:03:32 Train INFO: [Train]: Epoch 35 completed in 1379.0s (avg 23.373s/iter)
2025-07-29 08:03:32 Train INFO: [Train]: Final Loss=0.9613
2025-07-29 08:03:32 Train INFO: Checkpoint saved at epoch 35
2025-07-29 08:03:32 Train INFO: [Train]: Epoch 36 started (Total iterations: 59)
2025-07-29 08:08:29 Train INFO: [Train]: [036][00010/00058] (18.6%)  Loss=1.0436  cls_loss=0.7243  reg_loss=0.3193  lr_det=4.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1295s  iter_time=296.752s  fwd=2.049s/bwd=0.068s/opt=0.011s
2025-07-29 08:12:47 Train INFO: [Train]: [036][00020/00058] (35.6%)  Loss=1.0069  cls_loss=0.6999  reg_loss=0.3070  lr_det=3.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1004s  iter_time=257.881s
2025-07-29 08:15:53 Train INFO: [Train]: [036][00030/00058] (52.5%)  Loss=0.9904  cls_loss=0.6870  reg_loss=0.3034  lr_det=3.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=669s  iter_time=186.040s
2025-07-29 08:20:06 Train INFO: [Train]: [036][00040/00058] (69.5%)  Loss=0.9959  cls_loss=0.6894  reg_loss=0.3064  lr_det=3.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=436s  iter_time=252.829s
2025-07-29 08:23:12 Train INFO: [Train]: [036][00050/00058] (86.4%)  Loss=0.9965  cls_loss=0.6891  reg_loss=0.3074  lr_det=3.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=185s  iter_time=185.690s
2025-07-29 08:25:58 Train INFO: [Train]: [036][00058/00058] (100.0%)  Loss=0.9864  cls_loss=0.6814  reg_loss=0.3050  lr_det=3.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=166.156s
2025-07-29 08:25:58 Train INFO: [Train]: Epoch 36 completed in 1346.1s (avg 22.815s/iter)
2025-07-29 08:25:58 Train INFO: [Train]: Final Loss=0.9864
2025-07-29 08:25:58 Train INFO: [Train]: Epoch 37 started (Total iterations: 59)
2025-07-29 08:30:59 Train INFO: [Train]: [037][00010/00058] (18.6%)  Loss=0.9939  cls_loss=0.6875  reg_loss=0.3063  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1312s  iter_time=300.723s  fwd=2.038s/bwd=0.079s/opt=0.007s
2025-07-29 08:35:13 Train INFO: [Train]: [037][00020/00058] (35.6%)  Loss=0.9766  cls_loss=0.6783  reg_loss=0.2983  lr_det=3.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1003s  iter_time=253.368s
2025-07-29 08:38:19 Train INFO: [Train]: [037][00030/00058] (52.5%)  Loss=0.9749  cls_loss=0.6758  reg_loss=0.2991  lr_det=3.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=669s  iter_time=186.813s
2025-07-29 08:42:27 Train INFO: [Train]: [037][00040/00058] (69.5%)  Loss=0.9739  cls_loss=0.6747  reg_loss=0.2992  lr_det=3.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=434s  iter_time=247.629s
2025-07-29 08:45:34 Train INFO: [Train]: [037][00050/00058] (86.4%)  Loss=0.9796  cls_loss=0.6775  reg_loss=0.3021  lr_det=3.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=184s  iter_time=186.917s
2025-07-29 08:48:16 Train INFO: [Train]: [037][00058/00058] (100.0%)  Loss=0.9739  cls_loss=0.6744  reg_loss=0.2995  lr_det=3.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=162.141s
2025-07-29 08:48:17 Train INFO: [Train]: Epoch 37 completed in 1338.3s (avg 22.683s/iter)
2025-07-29 08:48:17 Train INFO: [Train]: Final Loss=0.9739
2025-07-29 08:48:17 Train INFO: Checkpoint saved at epoch 37
2025-07-29 08:48:17 Train INFO: [Train]: Epoch 38 started (Total iterations: 59)
2025-07-29 08:53:22 Train INFO: [Train]: [038][00010/00058] (18.6%)  Loss=0.9809  cls_loss=0.6842  reg_loss=0.2967  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1331s  iter_time=305.026s  fwd=2.051s/bwd=0.080s/opt=0.011s
2025-07-29 08:57:46 Train INFO: [Train]: [038][00020/00058] (35.6%)  Loss=0.9707  cls_loss=0.6740  reg_loss=0.2967  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1029s  iter_time=263.648s
2025-07-29 09:00:50 Train INFO: [Train]: [038][00030/00058] (52.5%)  Loss=0.9740  cls_loss=0.6786  reg_loss=0.2955  lr_det=3.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=183.777s
2025-07-29 09:05:08 Train INFO: [Train]: [038][00040/00058] (69.5%)  Loss=0.9768  cls_loss=0.6786  reg_loss=0.2982  lr_det=3.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=444s  iter_time=257.928s
2025-07-29 09:08:13 Train INFO: [Train]: [038][00050/00058] (86.4%)  Loss=0.9751  cls_loss=0.6763  reg_loss=0.2988  lr_det=3.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=184.869s
2025-07-29 09:10:50 Train INFO: [Train]: [038][00058/00058] (100.0%)  Loss=0.9679  cls_loss=0.6712  reg_loss=0.2966  lr_det=3.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=157.601s
2025-07-29 09:10:51 Train INFO: [Train]: Epoch 38 completed in 1353.6s (avg 22.942s/iter)
2025-07-29 09:10:51 Train INFO: [Train]: Final Loss=0.9679
2025-07-29 09:10:51 Train INFO: [Train]: Epoch 39 started (Total iterations: 59)
2025-07-29 09:15:50 Train INFO: [Train]: [039][00010/00058] (18.6%)  Loss=0.9908  cls_loss=0.6871  reg_loss=0.3037  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1305s  iter_time=299.177s  fwd=2.030s/bwd=0.033s/opt=0.003s
2025-07-29 09:20:14 Train INFO: [Train]: [039][00020/00058] (35.6%)  Loss=0.9661  cls_loss=0.6708  reg_loss=0.2953  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1019s  iter_time=263.833s
2025-07-29 09:23:24 Train INFO: [Train]: [039][00030/00058] (52.5%)  Loss=0.9851  cls_loss=0.6833  reg_loss=0.3017  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=190.292s
2025-07-29 09:27:58 Train INFO: [Train]: [039][00040/00058] (69.5%)  Loss=0.9773  cls_loss=0.6775  reg_loss=0.2997  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=451s  iter_time=274.232s
2025-07-29 09:31:01 Train INFO: [Train]: [039][00050/00058] (86.4%)  Loss=0.9740  cls_loss=0.6756  reg_loss=0.2984  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=190s  iter_time=182.305s
2025-07-29 09:33:34 Train INFO: [Train]: [039][00058/00058] (100.0%)  Loss=0.9758  cls_loss=0.6754  reg_loss=0.3004  lr_det=2.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.527s
2025-07-29 09:33:35 Train INFO: [Train]: Epoch 39 completed in 1364.1s (avg 23.121s/iter)
2025-07-29 09:33:35 Train INFO: [Train]: Final Loss=0.9758
2025-07-29 09:33:36 Train INFO: Checkpoint saved at epoch 39
2025-07-29 09:33:36 Train INFO: [Train]: Epoch 40 started (Total iterations: 59)
2025-07-29 09:38:49 Train INFO: [Train]: [040][00010/00058] (18.6%)  Loss=0.9788  cls_loss=0.6724  reg_loss=0.3064  lr_det=2.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1368s  iter_time=313.404s  fwd=2.066s/bwd=0.048s/opt=0.010s
2025-07-29 09:43:22 Train INFO: [Train]: [040][00020/00058] (35.6%)  Loss=0.9688  cls_loss=0.6726  reg_loss=0.2962  lr_det=2.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1062s  iter_time=273.341s
2025-07-29 09:46:37 Train INFO: [Train]: [040][00030/00058] (52.5%)  Loss=0.9748  cls_loss=0.6763  reg_loss=0.2985  lr_det=2.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=706s  iter_time=194.638s
2025-07-29 09:51:09 Train INFO: [Train]: [040][00040/00058] (69.5%)  Loss=0.9786  cls_loss=0.6782  reg_loss=0.3004  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=462s  iter_time=271.479s
2025-07-29 09:54:19 Train INFO: [Train]: [040][00050/00058] (86.4%)  Loss=0.9706  cls_loss=0.6717  reg_loss=0.2989  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=190.024s
2025-07-29 09:57:15 Train INFO: [Train]: [040][00058/00058] (100.0%)  Loss=0.9659  cls_loss=0.6683  reg_loss=0.2976  lr_det=2.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=176.154s
2025-07-29 09:57:16 Train INFO: [Train]: Epoch 40 completed in 1420.1s (avg 24.070s/iter)
2025-07-29 09:57:16 Train INFO: [Train]: Final Loss=0.9659
2025-07-29 09:57:16 Train INFO: [Train]: Epoch 41 started (Total iterations: 59)
2025-07-29 10:02:22 Train INFO: [Train]: [041][00010/00058] (18.6%)  Loss=1.0285  cls_loss=0.7161  reg_loss=0.3123  lr_det=2.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1337s  iter_time=306.465s  fwd=2.152s/bwd=0.082s/opt=0.009s
2025-07-29 10:06:48 Train INFO: [Train]: [041][00020/00058] (35.6%)  Loss=1.0033  cls_loss=0.6984  reg_loss=0.3049  lr_det=2.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=265.537s
2025-07-29 10:10:27 Train INFO: [Train]: [041][00030/00058] (52.5%)  Loss=1.0122  cls_loss=0.7040  reg_loss=0.3082  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=715s  iter_time=219.536s
2025-07-29 10:14:29 Train INFO: [Train]: [041][00040/00058] (69.5%)  Loss=0.9955  cls_loss=0.6909  reg_loss=0.3046  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=454s  iter_time=242.106s
2025-07-29 10:18:19 Train INFO: [Train]: [041][00050/00058] (86.4%)  Loss=0.9941  cls_loss=0.6903  reg_loss=0.3038  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=198s  iter_time=230.033s
2025-07-29 10:21:12 Train INFO: [Train]: [041][00058/00058] (100.0%)  Loss=0.9937  cls_loss=0.6887  reg_loss=0.3050  lr_det=2.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=172.737s
2025-07-29 10:21:13 Train INFO: [Train]: Epoch 41 completed in 1437.5s (avg 24.364s/iter)
2025-07-29 10:21:13 Train INFO: [Train]: Final Loss=0.9937
2025-07-29 10:21:14 Train INFO: Checkpoint saved at epoch 41
2025-07-29 10:21:14 Train INFO: [Train]: Epoch 42 started (Total iterations: 59)
2025-07-29 10:26:16 Train INFO: [Train]: [042][00010/00058] (18.6%)  Loss=1.0154  cls_loss=0.7037  reg_loss=0.3117  lr_det=2.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1319s  iter_time=302.173s  fwd=2.078s/bwd=0.075s/opt=0.008s
2025-07-29 10:30:47 Train INFO: [Train]: [042][00020/00058] (35.6%)  Loss=0.9919  cls_loss=0.6868  reg_loss=0.3051  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1037s  iter_time=270.762s
2025-07-29 10:33:55 Train INFO: [Train]: [042][00030/00058] (52.5%)  Loss=0.9782  cls_loss=0.6784  reg_loss=0.2998  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=687s  iter_time=188.099s
2025-07-29 10:38:33 Train INFO: [Train]: [042][00040/00058] (69.5%)  Loss=0.9826  cls_loss=0.6801  reg_loss=0.3024  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=456s  iter_time=277.967s
2025-07-29 10:41:48 Train INFO: [Train]: [042][00050/00058] (86.4%)  Loss=0.9799  cls_loss=0.6774  reg_loss=0.3025  lr_det=2.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=194s  iter_time=195.240s
2025-07-29 10:44:42 Train INFO: [Train]: [042][00058/00058] (100.0%)  Loss=0.9635  cls_loss=0.6652  reg_loss=0.2982  lr_det=2.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=173.104s
2025-07-29 10:44:43 Train INFO: [Train]: Epoch 42 completed in 1408.5s (avg 23.873s/iter)
2025-07-29 10:44:43 Train INFO: [Train]: Final Loss=0.9635
2025-07-29 10:44:43 Train INFO: [Train]: Epoch 43 started (Total iterations: 59)
2025-07-29 10:49:57 Train INFO: [Train]: [043][00010/00058] (18.6%)  Loss=1.0505  cls_loss=0.7213  reg_loss=0.3291  lr_det=2.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1369s  iter_time=313.837s  fwd=2.157s/bwd=0.084s/opt=0.011s
2025-07-29 10:54:41 Train INFO: [Train]: [043][00020/00058] (35.6%)  Loss=1.0073  cls_loss=0.6955  reg_loss=0.3118  lr_det=2.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1083s  iter_time=284.819s
2025-07-29 10:57:55 Train INFO: [Train]: [043][00030/00058] (52.5%)  Loss=0.9976  cls_loss=0.6880  reg_loss=0.3096  lr_det=2.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=716s  iter_time=193.752s
2025-07-29 11:02:36 Train INFO: [Train]: [043][00040/00058] (69.5%)  Loss=0.9820  cls_loss=0.6771  reg_loss=0.3050  lr_det=2.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=471s  iter_time=280.473s
2025-07-29 11:06:14 Train INFO: [Train]: [043][00050/00058] (86.4%)  Loss=0.9893  cls_loss=0.6833  reg_loss=0.3060  lr_det=2.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=203s  iter_time=218.661s
2025-07-29 11:09:14 Train INFO: [Train]: [043][00058/00058] (100.0%)  Loss=0.9918  cls_loss=0.6847  reg_loss=0.3070  lr_det=2.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=179.987s
2025-07-29 11:09:15 Train INFO: [Train]: Epoch 43 completed in 1472.5s (avg 24.957s/iter)
2025-07-29 11:09:15 Train INFO: [Train]: Final Loss=0.9918
2025-07-29 11:09:16 Train INFO: Checkpoint saved at epoch 43
2025-07-29 11:09:16 Train INFO: [Train]: Epoch 44 started (Total iterations: 59)
2025-07-29 11:15:19 Train INFO: [Train]: [044][00010/00058] (18.6%)  Loss=1.0070  cls_loss=0.6990  reg_loss=0.3079  lr_det=1.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1583s  iter_time=362.845s  fwd=2.410s/bwd=0.075s/opt=0.010s
2025-07-29 11:20:41 Train INFO: [Train]: [044][00020/00058] (35.6%)  Loss=0.9696  cls_loss=0.6752  reg_loss=0.2944  lr_det=1.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1239s  iter_time=322.055s
2025-07-29 11:24:25 Train INFO: [Train]: [044][00030/00058] (52.5%)  Loss=0.9731  cls_loss=0.6785  reg_loss=0.2946  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=821s  iter_time=224.583s
2025-07-29 11:29:38 Train INFO: [Train]: [044][00040/00058] (69.5%)  Loss=0.9719  cls_loss=0.6762  reg_loss=0.2957  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=537s  iter_time=313.210s
2025-07-29 11:33:22 Train INFO: [Train]: [044][00050/00058] (86.4%)  Loss=0.9714  cls_loss=0.6737  reg_loss=0.2977  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=227s  iter_time=223.550s
2025-07-29 11:36:19 Train INFO: [Train]: [044][00058/00058] (100.0%)  Loss=0.9681  cls_loss=0.6710  reg_loss=0.2972  lr_det=1.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=177.378s
2025-07-29 11:36:21 Train INFO: [Train]: Epoch 44 completed in 1624.9s (avg 27.541s/iter)
2025-07-29 11:36:21 Train INFO: [Train]: Final Loss=0.9681
2025-07-29 11:36:21 Train INFO: [Train]: Epoch 45 started (Total iterations: 59)
2025-07-29 11:42:21 Train INFO: [Train]: [045][00010/00058] (18.6%)  Loss=0.9784  cls_loss=0.6782  reg_loss=0.3003  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1571s  iter_time=360.079s  fwd=2.198s/bwd=0.061s/opt=0.011s
2025-07-29 11:47:33 Train INFO: [Train]: [045][00020/00058] (35.6%)  Loss=0.9764  cls_loss=0.6772  reg_loss=0.2992  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1216s  iter_time=311.770s
2025-07-29 11:51:09 Train INFO: [Train]: [045][00030/00058] (52.5%)  Loss=0.9820  cls_loss=0.6799  reg_loss=0.3020  lr_det=1.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=803s  iter_time=216.721s
2025-07-29 11:56:27 Train INFO: [Train]: [045][00040/00058] (69.5%)  Loss=0.9715  cls_loss=0.6735  reg_loss=0.2980  lr_det=1.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=529s  iter_time=317.324s
2025-07-29 12:00:02 Train INFO: [Train]: [045][00050/00058] (86.4%)  Loss=0.9631  cls_loss=0.6675  reg_loss=0.2956  lr_det=1.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=223s  iter_time=215.268s
2025-07-29 12:03:09 Train INFO: [Train]: [045][00058/00058] (100.0%)  Loss=0.9586  cls_loss=0.6652  reg_loss=0.2934  lr_det=1.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=186.983s
2025-07-29 12:03:10 Train INFO: [Train]: Epoch 45 completed in 1609.4s (avg 27.278s/iter)
2025-07-29 12:03:10 Train INFO: [Train]: Final Loss=0.9586
2025-07-29 12:03:11 Train INFO: Checkpoint saved at epoch 45
2025-07-29 12:03:11 Train INFO: [Train]: Epoch 46 started (Total iterations: 59)
2025-07-29 12:09:12 Train INFO: [Train]: [046][00010/00058] (18.6%)  Loss=1.0075  cls_loss=0.6954  reg_loss=0.3121  lr_det=1.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1577s  iter_time=361.314s  fwd=2.197s/bwd=0.053s/opt=0.006s
2025-07-29 12:14:34 Train INFO: [Train]: [046][00020/00058] (35.6%)  Loss=0.9977  cls_loss=0.6886  reg_loss=0.3092  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1235s  iter_time=321.376s
2025-07-29 12:18:27 Train INFO: [Train]: [046][00030/00058] (52.5%)  Loss=0.9959  cls_loss=0.6888  reg_loss=0.3071  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=828s  iter_time=233.796s
2025-07-29 12:23:48 Train INFO: [Train]: [046][00040/00058] (69.5%)  Loss=0.9779  cls_loss=0.6766  reg_loss=0.3013  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=543s  iter_time=320.257s
2025-07-29 12:27:39 Train INFO: [Train]: [046][00050/00058] (86.4%)  Loss=0.9864  cls_loss=0.6824  reg_loss=0.3040  lr_det=1.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=230s  iter_time=231.721s
2025-07-29 12:30:49 Train INFO: [Train]: [046][00058/00058] (100.0%)  Loss=0.9796  cls_loss=0.6776  reg_loss=0.3021  lr_det=1.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=189.270s
2025-07-29 12:30:50 Train INFO: [Train]: Epoch 46 completed in 1658.7s (avg 28.114s/iter)
2025-07-29 12:30:50 Train INFO: [Train]: Final Loss=0.9796
2025-07-29 12:30:50 Train INFO: [Train]: Epoch 47 started (Total iterations: 59)
2025-07-29 13:06:38 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-29 13:06:39 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-29 13:06:41 Train INFO: training subset: 942 videos
2025-07-29 13:06:41 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-29 13:06:41 Train INFO: Using single GPU training...
2025-07-29 13:06:41 Train INFO: Using Model EMA...
2025-07-29 13:06:41 Train INFO: Using Automatic Mixed Precision...
2025-07-29 13:06:41 Train INFO: GPU Memory: 24.0 GB
2025-07-29 13:06:41 Train INFO: Freeze the backbone...
2025-07-29 13:06:41 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_45.pth
2025-07-29 13:06:43 Train INFO: Resume epoch is 45
2025-07-29 13:06:43 Train INFO: Training Starts...

2025-07-29 13:06:43 Train INFO: [Train]: Epoch 46 started (Total iterations: 59)
2025-07-29 13:12:05 Train INFO: [Train]: [046][00010/00058] (18.6%)  Loss=1.0396  cls_loss=0.7161  reg_loss=0.3235  lr_det=1.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1407s  iter_time=322.364s  fwd=2.198s/bwd=0.073s/opt=0.019s
2025-07-29 13:16:53 Train INFO: [Train]: [046][00020/00058] (35.6%)  Loss=1.0201  cls_loss=0.7015  reg_loss=0.3187  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1103s  iter_time=287.457s
2025-07-29 13:20:18 Train INFO: [Train]: [046][00030/00058] (52.5%)  Loss=0.9919  cls_loss=0.6852  reg_loss=0.3067  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=736s  iter_time=205.193s
2025-07-29 13:25:06 Train INFO: [Train]: [046][00040/00058] (69.5%)  Loss=0.9784  cls_loss=0.6769  reg_loss=0.3016  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=485s  iter_time=288.644s
2025-07-29 13:28:23 Train INFO: [Train]: [046][00050/00058] (86.4%)  Loss=0.9820  cls_loss=0.6791  reg_loss=0.3029  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=204s  iter_time=196.594s
2025-07-29 13:31:27 Train INFO: [Train]: [046][00058/00058] (100.0%)  Loss=0.9738  cls_loss=0.6738  reg_loss=0.3000  lr_det=1.3e-05  GPU=1355MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=183.978s
2025-07-29 13:31:28 Train INFO: [Train]: Epoch 46 completed in 1485.4s (avg 25.176s/iter)
2025-07-29 13:31:28 Train INFO: [Train]: Final Loss=0.9738
2025-07-29 13:31:28 Train INFO: [Train]: Epoch 47 started (Total iterations: 59)
2025-07-29 13:37:13 Train INFO: [Train]: [047][00010/00058] (18.6%)  Loss=1.0254  cls_loss=0.7090  reg_loss=0.3164  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1506s  iter_time=345.229s  fwd=2.318s/bwd=0.043s/opt=0.018s
2025-07-29 13:42:20 Train INFO: [Train]: [047][00020/00058] (35.6%)  Loss=0.9991  cls_loss=0.6906  reg_loss=0.3084  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1180s  iter_time=306.887s
2025-07-29 13:46:08 Train INFO: [Train]: [047][00030/00058] (52.5%)  Loss=0.9865  cls_loss=0.6840  reg_loss=0.3025  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=794s  iter_time=227.207s
2025-07-29 13:51:27 Train INFO: [Train]: [047][00040/00058] (69.5%)  Loss=0.9818  cls_loss=0.6813  reg_loss=0.3005  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=526s  iter_time=319.717s
2025-07-29 13:55:26 Train INFO: [Train]: [047][00050/00058] (86.4%)  Loss=0.9772  cls_loss=0.6785  reg_loss=0.2988  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=226s  iter_time=238.529s
2025-07-29 13:58:47 Train INFO: [Train]: [047][00058/00058] (100.0%)  Loss=0.9692  cls_loss=0.6734  reg_loss=0.2958  lr_det=1.1e-05  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=201.366s
2025-07-29 13:58:48 Train INFO: [Train]: Epoch 47 completed in 1640.0s (avg 27.796s/iter)
2025-07-29 13:58:48 Train INFO: [Train]: Final Loss=0.9692
2025-07-29 13:58:49 Train INFO: Checkpoint saved at epoch 47
2025-07-29 13:58:49 Train INFO: [Train]: Epoch 48 started (Total iterations: 59)
2025-07-29 14:04:44 Train INFO: [Train]: [048][00010/00058] (18.6%)  Loss=1.0039  cls_loss=0.6916  reg_loss=0.3123  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1549s  iter_time=354.905s  fwd=2.198s/bwd=0.086s/opt=0.020s
2025-07-29 14:10:07 Train INFO: [Train]: [048][00020/00058] (35.6%)  Loss=0.9794  cls_loss=0.6797  reg_loss=0.2997  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1228s  iter_time=323.609s
2025-07-29 14:13:54 Train INFO: [Train]: [048][00030/00058] (52.5%)  Loss=0.9837  cls_loss=0.6856  reg_loss=0.2981  lr_det=1.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=818s  iter_time=226.682s
2025-07-29 14:19:17 Train INFO: [Train]: [048][00040/00058] (69.5%)  Loss=0.9782  cls_loss=0.6797  reg_loss=0.2985  lr_det=1.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=539s  iter_time=322.958s
2025-07-29 14:23:01 Train INFO: [Train]: [048][00050/00058] (86.4%)  Loss=0.9701  cls_loss=0.6761  reg_loss=0.2940  lr_det=9.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=224.026s
2025-07-29 14:26:22 Train INFO: [Train]: [048][00058/00058] (100.0%)  Loss=0.9723  cls_loss=0.6765  reg_loss=0.2957  lr_det=9.6e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=201.012s
2025-07-29 14:26:23 Train INFO: [Train]: Epoch 48 completed in 1654.5s (avg 28.043s/iter)
2025-07-29 14:26:23 Train INFO: [Train]: Final Loss=0.9723
2025-07-29 14:26:23 Train INFO: [Train]: Epoch 49 started (Total iterations: 59)
2025-07-29 14:32:25 Train INFO: [Train]: [049][00010/00058] (18.6%)  Loss=0.9989  cls_loss=0.6858  reg_loss=0.3131  lr_det=9.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1580s  iter_time=362.054s  fwd=2.252s/bwd=0.047s/opt=0.019s
2025-07-29 14:37:49 Train INFO: [Train]: [049][00020/00058] (35.6%)  Loss=0.9825  cls_loss=0.6789  reg_loss=0.3036  lr_det=9.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1241s  iter_time=323.680s
2025-07-29 14:41:40 Train INFO: [Train]: [049][00030/00058] (52.5%)  Loss=0.9898  cls_loss=0.6846  reg_loss=0.3052  lr_det=8.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=828s  iter_time=230.425s
2025-07-29 14:46:57 Train INFO: [Train]: [049][00040/00058] (69.5%)  Loss=0.9906  cls_loss=0.6867  reg_loss=0.3040  lr_det=8.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=542s  iter_time=317.740s
2025-07-29 14:50:39 Train INFO: [Train]: [049][00050/00058] (86.4%)  Loss=0.9808  cls_loss=0.6797  reg_loss=0.3011  lr_det=8.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=221.527s
2025-07-29 14:53:58 Train INFO: [Train]: [049][00058/00058] (100.0%)  Loss=0.9721  cls_loss=0.6742  reg_loss=0.2979  lr_det=8.0e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=199.674s
2025-07-29 14:54:00 Train INFO: [Train]: Epoch 49 completed in 1656.5s (avg 28.076s/iter)
2025-07-29 14:54:00 Train INFO: [Train]: Final Loss=0.9721
2025-07-29 14:54:01 Train INFO: Checkpoint saved at epoch 49
2025-07-29 14:54:01 Train INFO: [Train]: Epoch 50 started (Total iterations: 59)
2025-07-29 14:59:55 Train INFO: [Train]: [050][00010/00058] (18.6%)  Loss=1.0277  cls_loss=0.7077  reg_loss=0.3200  lr_det=7.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1545s  iter_time=354.060s  fwd=2.246s/bwd=0.082s/opt=0.020s
2025-07-29 15:05:26 Train INFO: [Train]: [050][00020/00058] (35.6%)  Loss=1.0155  cls_loss=0.7006  reg_loss=0.3149  lr_det=7.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1240s  iter_time=331.022s
2025-07-29 15:09:09 Train INFO: [Train]: [050][00030/00058] (52.5%)  Loss=0.9854  cls_loss=0.6812  reg_loss=0.3042  lr_det=7.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=820s  iter_time=223.167s
2025-07-29 15:14:22 Train INFO: [Train]: [050][00040/00058] (69.5%)  Loss=0.9853  cls_loss=0.6827  reg_loss=0.3027  lr_det=6.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=536s  iter_time=313.017s
2025-07-29 15:18:23 Train INFO: [Train]: [050][00050/00058] (86.4%)  Loss=0.9657  cls_loss=0.6699  reg_loss=0.2958  lr_det=6.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=229s  iter_time=240.457s
2025-07-29 15:21:34 Train INFO: [Train]: [050][00058/00058] (100.0%)  Loss=0.9760  cls_loss=0.6763  reg_loss=0.2998  lr_det=6.5e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=191.741s
2025-07-29 15:21:36 Train INFO: [Train]: Epoch 50 completed in 1654.8s (avg 28.048s/iter)
2025-07-29 15:21:36 Train INFO: [Train]: Final Loss=0.9760
2025-07-29 15:21:36 Train INFO: [Train]: Epoch 51 started (Total iterations: 59)
2025-07-29 15:27:34 Train INFO: [Train]: [051][00010/00058] (18.6%)  Loss=0.9986  cls_loss=0.6930  reg_loss=0.3056  lr_det=6.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1562s  iter_time=357.988s  fwd=2.277s/bwd=0.086s/opt=0.022s
2025-07-29 15:33:09 Train INFO: [Train]: [051][00020/00058] (35.6%)  Loss=0.9762  cls_loss=0.6768  reg_loss=0.2994  lr_det=6.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1255s  iter_time=335.494s
2025-07-29 15:36:56 Train INFO: [Train]: [051][00030/00058] (52.5%)  Loss=0.9733  cls_loss=0.6758  reg_loss=0.2975  lr_det=5.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=831s  iter_time=226.981s
2025-07-29 15:42:26 Train INFO: [Train]: [051][00040/00058] (69.5%)  Loss=0.9720  cls_loss=0.6758  reg_loss=0.2962  lr_det=5.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=549s  iter_time=329.453s
2025-07-29 15:46:12 Train INFO: [Train]: [051][00050/00058] (86.4%)  Loss=0.9746  cls_loss=0.6755  reg_loss=0.2992  lr_det=5.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=232s  iter_time=226.777s
2025-07-29 15:49:42 Train INFO: [Train]: [051][00058/00058] (100.0%)  Loss=0.9638  cls_loss=0.6675  reg_loss=0.2963  lr_det=5.2e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=209.952s
2025-07-29 15:49:44 Train INFO: [Train]: Epoch 51 completed in 1687.9s (avg 28.608s/iter)
2025-07-29 15:49:44 Train INFO: [Train]: Final Loss=0.9638
2025-07-29 15:49:45 Train INFO: Checkpoint saved at epoch 51
2025-07-29 15:49:45 Train INFO: [Train]: Epoch 52 started (Total iterations: 59)
2025-07-29 15:55:47 Train INFO: [Train]: [052][00010/00058] (18.6%)  Loss=0.9946  cls_loss=0.6836  reg_loss=0.3110  lr_det=4.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1579s  iter_time=361.745s  fwd=2.246s/bwd=0.103s/opt=0.019s
2025-07-29 16:00:58 Train INFO: [Train]: [052][00020/00058] (35.6%)  Loss=0.9882  cls_loss=0.6768  reg_loss=0.3113  lr_det=4.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1217s  iter_time=311.050s
2025-07-29 16:04:46 Train INFO: [Train]: [052][00030/00058] (52.5%)  Loss=0.9770  cls_loss=0.6705  reg_loss=0.3065  lr_det=4.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=814s  iter_time=227.914s
2025-07-29 16:10:17 Train INFO: [Train]: [052][00040/00058] (69.5%)  Loss=0.9864  cls_loss=0.6764  reg_loss=0.3100  lr_det=4.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=541s  iter_time=331.318s
2025-07-29 16:14:00 Train INFO: [Train]: [052][00050/00058] (86.4%)  Loss=0.9787  cls_loss=0.6723  reg_loss=0.3064  lr_det=4.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=223.160s
2025-07-29 16:17:20 Train INFO: [Train]: [052][00058/00058] (100.0%)  Loss=0.9772  cls_loss=0.6725  reg_loss=0.3047  lr_det=4.0e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=200.275s
2025-07-29 16:17:22 Train INFO: [Train]: Epoch 52 completed in 1656.8s (avg 28.082s/iter)
2025-07-29 16:17:22 Train INFO: [Train]: Final Loss=0.9772
2025-07-29 16:17:22 Train INFO: [Train]: Epoch 53 started (Total iterations: 59)
2025-07-29 16:23:25 Train INFO: [Train]: [053][00010/00058] (18.6%)  Loss=0.9794  cls_loss=0.6721  reg_loss=0.3074  lr_det=3.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1585s  iter_time=363.236s  fwd=2.182s/bwd=0.075s/opt=0.018s
2025-07-29 16:28:44 Train INFO: [Train]: [053][00020/00058] (35.6%)  Loss=0.9761  cls_loss=0.6750  reg_loss=0.3011  lr_det=3.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1235s  iter_time=319.007s
2025-07-29 16:32:35 Train INFO: [Train]: [053][00030/00058] (52.5%)  Loss=0.9840  cls_loss=0.6801  reg_loss=0.3039  lr_det=3.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=825s  iter_time=230.641s
2025-07-29 16:37:48 Train INFO: [Train]: [053][00040/00058] (69.5%)  Loss=0.9819  cls_loss=0.6796  reg_loss=0.3023  lr_det=3.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=538s  iter_time=313.001s
2025-07-29 16:41:40 Train INFO: [Train]: [053][00050/00058] (86.4%)  Loss=0.9748  cls_loss=0.6756  reg_loss=0.2993  lr_det=3.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=229s  iter_time=232.480s
2025-07-29 16:45:01 Train INFO: [Train]: [053][00058/00058] (100.0%)  Loss=0.9672  cls_loss=0.6709  reg_loss=0.2963  lr_det=2.9e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=200.953s
2025-07-29 16:45:02 Train INFO: [Train]: Epoch 53 completed in 1660.6s (avg 28.145s/iter)
2025-07-29 16:45:02 Train INFO: [Train]: Final Loss=0.9672
2025-07-29 16:45:03 Train INFO: Checkpoint saved at epoch 53
2025-07-29 16:45:03 Train INFO: [Train]: Epoch 54 started (Total iterations: 59)
2025-07-29 16:50:51 Train INFO: [Train]: [054][00010/00058] (18.6%)  Loss=1.0236  cls_loss=0.7086  reg_loss=0.3150  lr_det=2.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1516s  iter_time=347.523s  fwd=2.235s/bwd=0.067s/opt=0.022s
2025-07-29 16:56:18 Train INFO: [Train]: [054][00020/00058] (35.6%)  Loss=0.9805  cls_loss=0.6771  reg_loss=0.3034  lr_det=2.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1221s  iter_time=327.091s
2025-07-29 16:59:57 Train INFO: [Train]: [054][00030/00058] (52.5%)  Loss=0.9791  cls_loss=0.6767  reg_loss=0.3024  lr_det=2.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=807s  iter_time=219.259s
2025-07-29 17:05:00 Train INFO: [Train]: [054][00040/00058] (69.5%)  Loss=0.9837  cls_loss=0.6820  reg_loss=0.3017  lr_det=2.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=525s  iter_time=302.779s
2025-07-29 17:08:43 Train INFO: [Train]: [054][00050/00058] (86.4%)  Loss=0.9702  cls_loss=0.6725  reg_loss=0.2976  lr_det=2.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=223s  iter_time=223.209s
2025-07-29 17:11:33 Train INFO: [Train]: [054][00058/00058] (100.0%)  Loss=0.9728  cls_loss=0.6739  reg_loss=0.2989  lr_det=2.0e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=169.738s
2025-07-29 17:11:34 Train INFO: [Train]: Epoch 54 completed in 1590.8s (avg 26.962s/iter)
2025-07-29 17:11:34 Train INFO: [Train]: Final Loss=0.9728
2025-07-29 17:11:34 Train INFO: [Train]: Epoch 55 started (Total iterations: 59)
2025-07-29 17:17:33 Train INFO: [Train]: [055][00010/00058] (18.6%)  Loss=1.0126  cls_loss=0.6985  reg_loss=0.3141  lr_det=1.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1567s  iter_time=359.134s  fwd=2.216s/bwd=0.068s/opt=0.021s
2025-07-29 17:22:50 Train INFO: [Train]: [055][00020/00058] (35.6%)  Loss=0.9752  cls_loss=0.6735  reg_loss=0.3017  lr_det=1.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1223s  iter_time=316.920s
2025-07-29 17:26:36 Train INFO: [Train]: [055][00030/00058] (52.5%)  Loss=0.9842  cls_loss=0.6795  reg_loss=0.3048  lr_det=1.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=815s  iter_time=225.811s
2025-07-29 17:31:51 Train INFO: [Train]: [055][00040/00058] (69.5%)  Loss=0.9707  cls_loss=0.6702  reg_loss=0.3005  lr_det=1.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=534s  iter_time=314.641s
2025-07-29 17:35:30 Train INFO: [Train]: [055][00050/00058] (86.4%)  Loss=0.9663  cls_loss=0.6673  reg_loss=0.2991  lr_det=1.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=225s  iter_time=219.580s
2025-07-29 17:38:34 Train INFO: [Train]: [055][00058/00058] (100.0%)  Loss=0.9562  cls_loss=0.6594  reg_loss=0.2968  lr_det=1.3e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=184.160s
2025-07-29 17:38:35 Train INFO: [Train]: Epoch 55 completed in 1621.1s (avg 27.476s/iter)
2025-07-29 17:38:35 Train INFO: [Train]: Final Loss=0.9562
2025-07-29 17:38:36 Train INFO: Checkpoint saved at epoch 55
2025-07-29 17:38:36 Train INFO: [Train]: Epoch 56 started (Total iterations: 59)
2025-07-29 17:44:44 Train INFO: [Train]: [056][00010/00058] (18.6%)  Loss=1.0765  cls_loss=0.7435  reg_loss=0.3331  lr_det=1.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1607s  iter_time=368.248s  fwd=2.260s/bwd=0.104s/opt=0.020s
2025-07-29 17:50:16 Train INFO: [Train]: [056][00020/00058] (35.6%)  Loss=1.0233  cls_loss=0.7059  reg_loss=0.3174  lr_det=1.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1266s  iter_time=331.543s
2025-07-29 17:54:02 Train INFO: [Train]: [056][00030/00058] (52.5%)  Loss=1.0051  cls_loss=0.6937  reg_loss=0.3114  lr_det=1.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=837s  iter_time=226.639s
2025-07-29 17:59:20 Train INFO: [Train]: [056][00040/00058] (69.5%)  Loss=1.0013  cls_loss=0.6905  reg_loss=0.3108  lr_det=9.1e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=546s  iter_time=317.446s
2025-07-29 18:03:05 Train INFO: [Train]: [056][00050/00058] (86.4%)  Loss=0.9975  cls_loss=0.6880  reg_loss=0.3094  lr_det=8.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=230s  iter_time=224.901s
2025-07-29 18:06:15 Train INFO: [Train]: [056][00058/00058] (100.0%)  Loss=0.9899  cls_loss=0.6834  reg_loss=0.3066  lr_det=7.5e-07  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=190.514s
2025-07-29 18:06:16 Train INFO: [Train]: Epoch 56 completed in 1660.2s (avg 28.138s/iter)
2025-07-29 18:06:16 Train INFO: [Train]: Final Loss=0.9899
2025-07-29 18:06:16 Train INFO: [Train]: Epoch 57 started (Total iterations: 59)
2025-07-29 18:12:08 Train INFO: [Train]: [057][00010/00058] (18.6%)  Loss=0.9584  cls_loss=0.6621  reg_loss=0.2963  lr_det=6.6e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1537s  iter_time=352.239s  fwd=2.186s/bwd=0.091s/opt=0.020s
2025-07-29 18:17:38 Train INFO: [Train]: [057][00020/00058] (35.6%)  Loss=0.9756  cls_loss=0.6770  reg_loss=0.2986  lr_det=5.9e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1233s  iter_time=329.395s
2025-07-29 18:21:38 Train INFO: [Train]: [057][00030/00058] (52.5%)  Loss=0.9833  cls_loss=0.6812  reg_loss=0.3021  lr_det=5.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=832s  iter_time=239.831s
2025-07-29 18:26:58 Train INFO: [Train]: [057][00040/00058] (69.5%)  Loss=0.9696  cls_loss=0.6720  reg_loss=0.2975  lr_det=4.5e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=545s  iter_time=320.916s
2025-07-29 18:30:40 Train INFO: [Train]: [057][00050/00058] (86.4%)  Loss=0.9755  cls_loss=0.6760  reg_loss=0.2995  lr_det=3.9e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=230s  iter_time=221.591s
2025-07-29 18:33:46 Train INFO: [Train]: [057][00058/00058] (100.0%)  Loss=0.9681  cls_loss=0.6697  reg_loss=0.2984  lr_det=3.4e-07  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=185.508s
2025-07-29 18:33:46 Train INFO: [Train]: Epoch 57 completed in 1650.3s (avg 27.971s/iter)
2025-07-29 18:33:46 Train INFO: [Train]: Final Loss=0.9681
2025-07-29 18:33:47 Train INFO: Checkpoint saved at epoch 57
2025-07-29 18:33:47 Train INFO: [Train]: Epoch 58 started (Total iterations: 59)
2025-07-29 18:40:11 Train INFO: [Train]: [058][00010/00058] (18.6%)  Loss=0.9709  cls_loss=0.6719  reg_loss=0.2991  lr_det=2.8e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1677s  iter_time=384.251s  fwd=2.254s/bwd=0.085s/opt=0.021s
2025-07-29 18:45:42 Train INFO: [Train]: [058][00020/00058] (35.6%)  Loss=0.9762  cls_loss=0.6754  reg_loss=0.3008  lr_det=2.3e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1293s  iter_time=330.564s
2025-07-29 18:49:34 Train INFO: [Train]: [058][00030/00058] (52.5%)  Loss=0.9614  cls_loss=0.6663  reg_loss=0.2952  lr_det=1.9e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=855s  iter_time=231.821s
2025-07-29 18:55:02 Train INFO: [Train]: [058][00040/00058] (69.5%)  Loss=0.9640  cls_loss=0.6679  reg_loss=0.2961  lr_det=1.5e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=560s  iter_time=328.016s
2025-07-29 18:58:45 Train INFO: [Train]: [058][00050/00058] (86.4%)  Loss=0.9674  cls_loss=0.6700  reg_loss=0.2974  lr_det=1.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=235s  iter_time=223.633s
2025-07-29 19:02:11 Train INFO: [Train]: [058][00058/00058] (100.0%)  Loss=0.9648  cls_loss=0.6680  reg_loss=0.2968  lr_det=9.4e-08  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=205.304s
2025-07-29 19:02:12 Train INFO: [Train]: Epoch 58 completed in 1704.7s (avg 28.893s/iter)
2025-07-29 19:02:12 Train INFO: [Train]: Final Loss=0.9648
2025-07-29 19:02:12 Train INFO: [Train]: Epoch 59 started (Total iterations: 59)
2025-07-29 19:08:19 Train INFO: [Train]: [059][00010/00058] (18.6%)  Loss=1.0100  cls_loss=0.6986  reg_loss=0.3114  lr_det=6.6e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1603s  iter_time=367.337s  fwd=2.231s/bwd=0.050s/opt=0.013s
2025-07-29 19:13:52 Train INFO: [Train]: [059][00020/00058] (35.6%)  Loss=0.9824  cls_loss=0.6802  reg_loss=0.3023  lr_det=4.6e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1268s  iter_time=333.272s
2025-07-29 19:17:47 Train INFO: [Train]: [059][00030/00058] (52.5%)  Loss=0.9699  cls_loss=0.6710  reg_loss=0.2989  lr_det=3.0e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=845s  iter_time=234.796s
2025-07-29 19:23:04 Train INFO: [Train]: [059][00040/00058] (69.5%)  Loss=0.9907  cls_loss=0.6833  reg_loss=0.3074  lr_det=1.8e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=550s  iter_time=316.642s
2025-07-29 19:27:02 Train INFO: [Train]: [059][00050/00058] (86.4%)  Loss=0.9854  cls_loss=0.6805  reg_loss=0.3049  lr_det=1.2e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=234s  iter_time=238.221s
2025-07-29 19:30:13 Train INFO: [Train]: [059][00058/00058] (100.0%)  Loss=0.9862  cls_loss=0.6818  reg_loss=0.3044  lr_det=1.0e-08  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=191.478s
2025-07-29 19:30:14 Train INFO: [Train]: Epoch 59 completed in 1682.6s (avg 28.518s/iter)
2025-07-29 19:30:14 Train INFO: [Train]: Final Loss=0.9862
2025-07-29 19:30:15 Train INFO: Checkpoint saved at epoch 59
2025-07-29 19:30:15 Train INFO: Training Over...

2025-07-30 09:20:01 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 09:20:02 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 09:20:03 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 09:20:03 Test INFO: Using single GPU testing...
2025-07-30 09:20:03 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 09:20:05 Test INFO: Checkpoint is epoch 59.
2025-07-30 09:20:05 Test INFO: Using Model EMA...
2025-07-30 09:20:05 Test INFO: Using Automatic Mixed Precision...
2025-07-30 09:20:05 Test INFO: Testing Starts...

2025-07-30 10:35:20 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 10:35:21 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 10:35:22 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 10:35:22 Test INFO: Using single GPU testing...
2025-07-30 10:35:22 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 10:35:22 Test INFO: Checkpoint is epoch 59.
2025-07-30 10:35:22 Test INFO: Using Model EMA...
2025-07-30 10:35:22 Test INFO: Using Automatic Mixed Precision...
2025-07-30 10:35:22 Test INFO: Testing Starts...

2025-07-30 12:01:01 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 12:01:01 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 12:01:02 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 12:01:03 Test INFO: Using single GPU testing...
2025-07-30 12:01:03 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 12:01:04 Test INFO: Checkpoint is epoch 59.
2025-07-30 12:01:04 Test INFO: Using Model EMA...
2025-07-30 12:01:04 Test INFO: Using Automatic Mixed Precision...
2025-07-30 12:01:04 Test INFO: Testing Starts...

2025-07-30 15:54:01 Evaluation INFO: Starting evaluation...
2025-07-30 15:54:48 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 15:54:48 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 15:54:48 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 15:54:48 Evaluation INFO: Number of predictions: 264000
2025-07-30 15:54:48 Evaluation INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7, 0.9]
2025-07-30 15:54:48 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.10 is 0.60%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.30 is 0.45%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.50 is 0.21%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.90 is 0.00%
2025-07-30 15:54:48 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 15:54:48 Evaluation INFO: Evaluation completed!
2025-07-30 15:57:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 15:57:33 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 15:57:34 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 15:57:35 Test INFO: Using single GPU testing...
2025-07-30 15:57:35 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 15:57:35 Test INFO: Checkpoint is epoch 59.
2025-07-30 15:57:35 Test INFO: Using Model EMA...
2025-07-30 15:57:35 Test INFO: Using Automatic Mixed Precision...
2025-07-30 15:57:35 Test INFO: Testing Starts...

2025-07-30 16:01:17 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 16:01:18 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 16:01:18 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 16:01:19 Test INFO: Using single GPU testing...
2025-07-30 16:01:19 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 16:01:19 Test INFO: Checkpoint is epoch 59.
2025-07-30 16:01:19 Test INFO: Using Model EMA...
2025-07-30 16:01:19 Test INFO: Using Automatic Mixed Precision...
2025-07-30 16:01:19 Test INFO: Testing Starts...

2025-07-30 16:59:02 Test INFO: Evaluation starts...
2025-07-30 16:59:57 Test INFO: Evaluating PKU-MMD dataset.
2025-07-30 16:59:57 Test INFO: Loaded annotations from test subset.
2025-07-30 16:59:57 Test INFO: Number of ground truth instances: 2704
2025-07-30 16:59:57 Test INFO: Number of predictions: 264000
2025-07-30 16:59:57 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7, 0.9]
2025-07-30 16:59:57 Test INFO: Average-mAP: 0.29 (%)
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.10 is 0.68%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.30 is 0.50%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.90 is 0.00%
2025-07-30 16:59:58 Test INFO: Testing Over...

2025-07-30 16:59:58 Test INFO: PKU-MMD evaluation completed successfully!
2025-07-30 16:59:58 Test INFO: Results saved in: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0/
2025-07-30 17:16:45 Evaluation INFO: Starting evaluation...
2025-07-30 17:17:26 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 17:17:26 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 17:17:26 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 17:17:26 Evaluation INFO: Number of predictions: 264000
2025-07-30 17:17:26 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-30 17:17:26 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.20 is 0.55%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.40 is 0.33%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.60 is 0.10%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 17:17:26 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 17:17:26 Evaluation INFO: Evaluation completed!
2025-07-30 17:27:56 Evaluation INFO: Starting evaluation...
2025-07-30 17:28:38 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 17:28:38 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 17:28:38 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 17:28:38 Evaluation INFO: Number of predictions: 264000
2025-07-30 17:28:38 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-30 17:28:38 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.20 is 0.55%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.40 is 0.33%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.60 is 0.10%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 17:28:38 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 17:28:38 Evaluation INFO: Evaluation completed!
2025-07-30 18:01:20 Evaluation INFO: Starting evaluation...
2025-07-30 18:01:48 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 18:01:48 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 18:01:48 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 18:01:48 Evaluation INFO: Number of predictions: 264000
2025-07-30 18:01:48 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-30 18:01:48 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.20 is 0.55%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.40 is 0.33%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.60 is 0.10%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 18:01:48 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 18:01:48 Evaluation INFO: Evaluation completed!
2025-07-30 18:26:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 18:26:53 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 18:26:54 Train INFO: training subset: 942 videos
2025-07-30 18:26:54 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 18:42:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 18:42:16 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 18:42:16 Train INFO: training subset: 942 videos
2025-07-30 18:42:16 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 18:42:17 Train INFO: Using single GPU training...
2025-07-30 18:42:17 Train INFO: Using Model EMA...
2025-07-30 18:42:17 Train INFO: Using Automatic Mixed Precision...
2025-07-30 18:42:17 Train INFO: GPU Memory: 24.0 GB
2025-07-30 18:42:17 Train INFO: Freeze the backbone...
2025-07-30 18:42:17 Train INFO: Training Starts...

2025-07-30 18:42:17 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-30 18:47:18 Train INFO: [Train]: [000][00010/00058] (18.6%)  Loss=1.6274  cls_loss=0.8717  reg_loss=0.7558  lr_det=3.4e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1314s  iter_time=301.139s  fwd=2.078s/bwd=0.043s/opt=0.010s
2025-07-30 18:51:43 Train INFO: [Train]: [000][00020/00058] (35.6%)  Loss=1.7138  cls_loss=0.9560  reg_loss=0.7577  lr_det=6.8e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1024s  iter_time=264.874s
2025-07-30 18:54:57 Train INFO: [Train]: [000][00030/00058] (52.5%)  Loss=1.6507  cls_loss=0.9818  reg_loss=0.6690  lr_det=1.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=686s  iter_time=193.641s
2025-07-30 18:59:41 Train INFO: [Train]: [000][00040/00058] (69.5%)  Loss=1.5867  cls_loss=0.9983  reg_loss=0.5884  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=458s  iter_time=284.173s
2025-07-30 19:03:03 Train INFO: [Train]: [000][00050/00058] (86.4%)  Loss=1.5577  cls_loss=1.0157  reg_loss=0.5420  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=202.349s
2025-07-30 19:06:01 Train INFO: [Train]: [000][00058/00058] (100.0%)  Loss=1.5217  cls_loss=1.0101  reg_loss=0.5116  lr_det=2.0e-05  GPU=1354MB(alloc)/4212MB(reserved)/9063MB(max)  ETA=0s  iter_time=178.374s
2025-07-30 19:06:02 Train INFO: [Train]: Epoch 0 completed in 1425.4s (avg 24.159s/iter)
2025-07-30 19:06:02 Train INFO: [Train]: Final Loss=1.5217
2025-07-30 19:06:02 Train INFO: [Train]: Epoch 1 started (Total iterations: 59)
2025-07-30 19:11:04 Train INFO: [Train]: [001][00010/00058] (18.6%)  Loss=1.3965  cls_loss=1.0490  reg_loss=0.3476  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1316s  iter_time=301.625s  fwd=2.081s/bwd=0.087s/opt=0.006s
2025-07-30 19:15:42 Train INFO: [Train]: [001][00020/00058] (35.6%)  Loss=1.3406  cls_loss=1.0077  reg_loss=0.3329  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1049s  iter_time=278.341s
2025-07-30 19:18:59 Train INFO: [Train]: [001][00030/00058] (52.5%)  Loss=1.3256  cls_loss=1.0015  reg_loss=0.3241  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=701s  iter_time=196.425s
2025-07-30 19:23:41 Train INFO: [Train]: [001][00040/00058] (69.5%)  Loss=1.3104  cls_loss=0.9908  reg_loss=0.3195  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=465s  iter_time=282.419s
2025-07-30 19:26:47 Train INFO: [Train]: [001][00050/00058] (86.4%)  Loss=1.3025  cls_loss=0.9857  reg_loss=0.3168  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=186.372s
2025-07-30 19:29:33 Train INFO: [Train]: [001][00058/00058] (100.0%)  Loss=1.2930  cls_loss=0.9794  reg_loss=0.3136  lr_det=4.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=165.431s
2025-07-30 19:29:34 Train INFO: [Train]: Epoch 1 completed in 1411.4s (avg 23.921s/iter)
2025-07-30 19:29:34 Train INFO: [Train]: Final Loss=1.2930
2025-07-30 19:29:34 Train INFO: Checkpoint saved at epoch 1
2025-07-30 19:29:34 Train INFO: [Train]: Epoch 2 started (Total iterations: 59)
2025-07-30 19:34:57 Train INFO: [Train]: [002][00010/00058] (18.6%)  Loss=1.3112  cls_loss=0.9866  reg_loss=0.3246  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1409s  iter_time=322.846s  fwd=2.129s/bwd=0.069s/opt=0.008s
2025-07-30 19:39:35 Train INFO: [Train]: [002][00020/00058] (35.6%)  Loss=1.2774  cls_loss=0.9668  reg_loss=0.3106  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1088s  iter_time=278.219s
2025-07-30 19:42:40 Train INFO: [Train]: [002][00030/00058] (52.5%)  Loss=1.2866  cls_loss=0.9745  reg_loss=0.3120  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=709s  iter_time=184.186s
2025-07-30 19:47:15 Train INFO: [Train]: [002][00040/00058] (69.5%)  Loss=1.2815  cls_loss=0.9692  reg_loss=0.3123  lr_det=5.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=466s  iter_time=275.762s
2025-07-30 19:50:18 Train INFO: [Train]: [002][00050/00058] (86.4%)  Loss=1.2699  cls_loss=0.9621  reg_loss=0.3078  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=182.936s
2025-07-30 19:53:05 Train INFO: [Train]: [002][00058/00058] (100.0%)  Loss=1.2709  cls_loss=0.9610  reg_loss=0.3099  lr_det=6.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=166.536s
2025-07-30 19:53:06 Train INFO: [Train]: Epoch 2 completed in 1411.3s (avg 23.920s/iter)
2025-07-30 19:53:06 Train INFO: [Train]: Final Loss=1.2709
2025-07-30 19:53:06 Train INFO: [Train]: Epoch 3 started (Total iterations: 59)
2025-07-30 19:58:16 Train INFO: [Train]: [003][00010/00058] (18.6%)  Loss=1.2838  cls_loss=0.9620  reg_loss=0.3218  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1353s  iter_time=310.021s  fwd=2.146s/bwd=0.087s/opt=0.009s
2025-07-30 20:02:33 Train INFO: [Train]: [003][00020/00058] (35.6%)  Loss=1.2328  cls_loss=0.9183  reg_loss=0.3144  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1027s  iter_time=257.359s
2025-07-30 20:05:51 Train INFO: [Train]: [003][00030/00058] (52.5%)  Loss=1.2137  cls_loss=0.8978  reg_loss=0.3159  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=692s  iter_time=198.393s
2025-07-30 20:10:29 Train INFO: [Train]: [003][00040/00058] (69.5%)  Loss=1.1924  cls_loss=0.8772  reg_loss=0.3151  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=458s  iter_time=277.558s
2025-07-30 20:13:27 Train INFO: [Train]: [003][00050/00058] (86.4%)  Loss=1.1612  cls_loss=0.8491  reg_loss=0.3121  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=192s  iter_time=177.765s
2025-07-30 20:16:02 Train INFO: [Train]: [003][00058/00058] (100.0%)  Loss=1.1369  cls_loss=0.8278  reg_loss=0.3090  lr_det=8.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=155.223s
2025-07-30 20:16:03 Train INFO: [Train]: Epoch 3 completed in 1377.1s (avg 23.341s/iter)
2025-07-30 20:16:03 Train INFO: [Train]: Final Loss=1.1369
2025-07-30 20:16:03 Train INFO: Checkpoint saved at epoch 3
2025-07-30 20:16:03 Train INFO: [Train]: Epoch 4 started (Total iterations: 59)
2025-07-30 20:21:16 Train INFO: [Train]: [004][00010/00058] (18.6%)  Loss=1.0912  cls_loss=0.7638  reg_loss=0.3273  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1366s  iter_time=312.947s  fwd=2.084s/bwd=0.084s/opt=0.013s
2025-07-30 20:25:44 Train INFO: [Train]: [004][00020/00058] (35.6%)  Loss=1.0739  cls_loss=0.7509  reg_loss=0.3229  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1051s  iter_time=268.024s
2025-07-30 20:28:48 Train INFO: [Train]: [004][00030/00058] (52.5%)  Loss=1.0445  cls_loss=0.7308  reg_loss=0.3137  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=691s  iter_time=183.816s
2025-07-30 20:33:17 Train INFO: [Train]: [004][00040/00058] (69.5%)  Loss=1.0428  cls_loss=0.7291  reg_loss=0.3137  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=454s  iter_time=268.419s
2025-07-30 20:36:35 Train INFO: [Train]: [004][00050/00058] (86.4%)  Loss=1.0228  cls_loss=0.7163  reg_loss=0.3065  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=193s  iter_time=198.578s
2025-07-30 20:39:27 Train INFO: [Train]: [004][00058/00058] (100.0%)  Loss=1.0330  cls_loss=0.7223  reg_loss=0.3107  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=171.456s
2025-07-30 20:39:27 Train INFO: [Train]: Epoch 4 completed in 1404.0s (avg 23.796s/iter)
2025-07-30 20:39:27 Train INFO: [Train]: Final Loss=1.0330
2025-07-30 20:39:27 Train INFO: [Train]: Epoch 5 started (Total iterations: 59)
2025-07-30 20:44:25 Train INFO: [Train]: [005][00010/00058] (18.6%)  Loss=1.0508  cls_loss=0.7352  reg_loss=0.3156  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1299s  iter_time=297.713s  fwd=2.079s/bwd=0.052s/opt=0.012s
2025-07-30 20:49:10 Train INFO: [Train]: [005][00020/00058] (35.6%)  Loss=1.0220  cls_loss=0.7134  reg_loss=0.3086  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1055s  iter_time=285.400s
2025-07-30 20:52:30 Train INFO: [Train]: [005][00030/00058] (52.5%)  Loss=1.0189  cls_loss=0.7129  reg_loss=0.3060  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=707s  iter_time=199.917s
2025-07-30 20:57:25 Train INFO: [Train]: [005][00040/00058] (69.5%)  Loss=1.0181  cls_loss=0.7129  reg_loss=0.3053  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=473s  iter_time=294.649s
2025-07-30 21:00:36 Train INFO: [Train]: [005][00050/00058] (86.4%)  Loss=1.0194  cls_loss=0.7117  reg_loss=0.3078  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=199s  iter_time=190.495s
2025-07-30 21:03:23 Train INFO: [Train]: [005][00058/00058] (100.0%)  Loss=1.0090  cls_loss=0.7040  reg_loss=0.3050  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=167.114s
2025-07-30 21:03:23 Train INFO: [Train]: Epoch 5 completed in 1436.1s (avg 24.340s/iter)
2025-07-30 21:03:23 Train INFO: [Train]: Final Loss=1.0090
2025-07-30 21:03:24 Train INFO: Checkpoint saved at epoch 5
2025-07-30 21:03:24 Train INFO: [Train]: Epoch 6 started (Total iterations: 59)
2025-07-30 21:08:41 Train INFO: [Train]: [006][00010/00058] (18.6%)  Loss=1.0308  cls_loss=0.7144  reg_loss=0.3164  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1385s  iter_time=317.458s  fwd=2.296s/bwd=0.068s/opt=0.009s
2025-07-30 21:13:32 Train INFO: [Train]: [006][00020/00058] (35.6%)  Loss=1.0192  cls_loss=0.7052  reg_loss=0.3140  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1100s  iter_time=290.413s
2025-07-30 21:16:53 Train INFO: [Train]: [006][00030/00058] (52.5%)  Loss=1.0070  cls_loss=0.6973  reg_loss=0.3098  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=730s  iter_time=200.746s
2025-07-30 21:21:38 Train INFO: [Train]: [006][00040/00058] (69.5%)  Loss=1.0213  cls_loss=0.7069  reg_loss=0.3145  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=480s  iter_time=285.116s
2025-07-30 21:24:54 Train INFO: [Train]: [006][00050/00058] (86.4%)  Loss=1.0124  cls_loss=0.7013  reg_loss=0.3111  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=202s  iter_time=196.328s
2025-07-30 21:27:46 Train INFO: [Train]: [006][00058/00058] (100.0%)  Loss=1.0103  cls_loss=0.7009  reg_loss=0.3094  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=171.580s
2025-07-30 21:27:47 Train INFO: [Train]: Epoch 6 completed in 1462.5s (avg 24.788s/iter)
2025-07-30 21:27:47 Train INFO: [Train]: Final Loss=1.0103
2025-07-30 21:27:47 Train INFO: [Train]: Epoch 7 started (Total iterations: 59)
2025-07-30 21:33:40 Train INFO: [Train]: [007][00010/00058] (18.6%)  Loss=1.0113  cls_loss=0.7000  reg_loss=0.3113  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1543s  iter_time=353.628s  fwd=2.172s/bwd=0.054s/opt=0.010s
2025-07-30 21:38:25 Train INFO: [Train]: [007][00020/00058] (35.6%)  Loss=1.0072  cls_loss=0.6998  reg_loss=0.3074  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1156s  iter_time=285.289s
2025-07-30 21:41:59 Train INFO: [Train]: [007][00030/00058] (52.5%)  Loss=1.0137  cls_loss=0.7029  reg_loss=0.3108  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=770s  iter_time=213.994s
2025-07-30 21:46:53 Train INFO: [Train]: [007][00040/00058] (69.5%)  Loss=1.0121  cls_loss=0.7028  reg_loss=0.3092  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=503s  iter_time=293.282s
2025-07-30 21:50:20 Train INFO: [Train]: [007][00050/00058] (86.4%)  Loss=1.0056  cls_loss=0.6992  reg_loss=0.3064  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=212s  iter_time=207.055s
2025-07-30 21:53:08 Train INFO: [Train]: [007][00058/00058] (100.0%)  Loss=0.9980  cls_loss=0.6942  reg_loss=0.3038  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=168.461s
2025-07-30 21:53:09 Train INFO: [Train]: Epoch 7 completed in 1522.5s (avg 25.805s/iter)
2025-07-30 21:53:09 Train INFO: [Train]: Final Loss=0.9980
2025-07-30 21:53:10 Train INFO: Checkpoint saved at epoch 7
2025-07-30 21:53:10 Train INFO: [Train]: Epoch 8 started (Total iterations: 59)
2025-07-30 21:59:07 Train INFO: [Train]: [008][00010/00058] (18.6%)  Loss=1.0546  cls_loss=0.7308  reg_loss=0.3238  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1562s  iter_time=357.863s  fwd=2.080s/bwd=0.067s/opt=0.006s
2025-07-30 22:04:08 Train INFO: [Train]: [008][00020/00058] (35.6%)  Loss=1.0087  cls_loss=0.6985  reg_loss=0.3101  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1191s  iter_time=300.461s
2025-07-30 22:07:45 Train INFO: [Train]: [008][00030/00058] (52.5%)  Loss=1.0087  cls_loss=0.6994  reg_loss=0.3093  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=790s  iter_time=216.744s
2025-07-30 22:13:08 Train INFO: [Train]: [008][00040/00058] (69.5%)  Loss=1.0154  cls_loss=0.7068  reg_loss=0.3086  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=526s  iter_time=323.297s
2025-07-30 22:16:42 Train INFO: [Train]: [008][00050/00058] (86.4%)  Loss=1.0001  cls_loss=0.6958  reg_loss=0.3043  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=221s  iter_time=213.595s
2025-07-30 22:19:40 Train INFO: [Train]: [008][00058/00058] (100.0%)  Loss=1.0030  cls_loss=0.6970  reg_loss=0.3060  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=178.832s
2025-07-30 22:19:41 Train INFO: [Train]: Epoch 8 completed in 1591.7s (avg 26.978s/iter)
2025-07-30 22:19:41 Train INFO: [Train]: Final Loss=1.0030
2025-07-30 22:19:41 Train INFO: [Train]: Epoch 9 started (Total iterations: 59)
2025-07-30 22:25:23 Train INFO: [Train]: [009][00010/00058] (18.6%)  Loss=1.0409  cls_loss=0.7177  reg_loss=0.3231  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1489s  iter_time=341.292s  fwd=2.067s/bwd=0.062s/opt=0.006s
2025-07-30 22:30:30 Train INFO: [Train]: [009][00020/00058] (35.6%)  Loss=1.0026  cls_loss=0.6933  reg_loss=0.3093  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1174s  iter_time=307.683s
2025-07-30 22:34:16 Train INFO: [Train]: [009][00030/00058] (52.5%)  Loss=1.0101  cls_loss=0.6978  reg_loss=0.3123  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=790s  iter_time=226.169s
2025-07-30 22:39:28 Train INFO: [Train]: [009][00040/00058] (69.5%)  Loss=0.9967  cls_loss=0.6890  reg_loss=0.3077  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=521s  iter_time=311.581s
2025-07-30 22:43:01 Train INFO: [Train]: [009][00050/00058] (86.4%)  Loss=0.9927  cls_loss=0.6864  reg_loss=0.3063  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=220s  iter_time=213.408s
2025-07-30 22:45:58 Train INFO: [Train]: [009][00058/00058] (100.0%)  Loss=0.9819  cls_loss=0.6783  reg_loss=0.3035  lr_det=9.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=176.610s
2025-07-30 22:45:59 Train INFO: [Train]: Epoch 9 completed in 1577.5s (avg 26.738s/iter)
2025-07-30 22:45:59 Train INFO: [Train]: Final Loss=0.9819
2025-07-30 22:45:59 Train INFO: Checkpoint saved at epoch 9
2025-07-30 22:45:59 Train INFO: [Train]: Epoch 10 started (Total iterations: 59)
2025-07-30 22:51:35 Train INFO: [Train]: [010][00010/00058] (18.6%)  Loss=1.1041  cls_loss=0.7629  reg_loss=0.3412  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1464s  iter_time=335.533s  fwd=2.045s/bwd=0.050s/opt=0.004s
2025-07-30 22:56:44 Train INFO: [Train]: [010][00020/00058] (35.6%)  Loss=1.0484  cls_loss=0.7256  reg_loss=0.3228  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1167s  iter_time=309.171s
2025-07-30 23:00:14 Train INFO: [Train]: [010][00030/00058] (52.5%)  Loss=1.0285  cls_loss=0.7126  reg_loss=0.3159  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=772s  iter_time=210.013s
2025-07-30 23:05:27 Train INFO: [Train]: [010][00040/00058] (69.5%)  Loss=1.0246  cls_loss=0.7089  reg_loss=0.3157  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=513s  iter_time=313.092s
2025-07-30 23:08:56 Train INFO: [Train]: [010][00050/00058] (86.4%)  Loss=1.0228  cls_loss=0.7081  reg_loss=0.3147  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=216s  iter_time=208.671s
2025-07-30 23:12:13 Train INFO: [Train]: [010][00058/00058] (100.0%)  Loss=1.0151  cls_loss=0.7029  reg_loss=0.3122  lr_det=9.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=197.413s
2025-07-30 23:12:14 Train INFO: [Train]: Epoch 10 completed in 1574.7s (avg 26.689s/iter)
2025-07-30 23:12:14 Train INFO: [Train]: Final Loss=1.0151
2025-07-30 23:12:14 Train INFO: [Train]: Epoch 11 started (Total iterations: 59)
2025-07-30 23:18:03 Train INFO: [Train]: [011][00010/00058] (18.6%)  Loss=0.9883  cls_loss=0.6844  reg_loss=0.3039  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1524s  iter_time=349.305s  fwd=2.085s/bwd=0.074s/opt=0.010s
2025-07-30 23:23:09 Train INFO: [Train]: [011][00020/00058] (35.6%)  Loss=1.0040  cls_loss=0.6973  reg_loss=0.3067  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1184s  iter_time=305.221s
2025-07-30 23:26:37 Train INFO: [Train]: [011][00030/00058] (52.5%)  Loss=1.0114  cls_loss=0.7008  reg_loss=0.3107  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=779s  iter_time=208.426s
2025-07-30 23:31:50 Train INFO: [Train]: [011][00040/00058] (69.5%)  Loss=0.9945  cls_loss=0.6890  reg_loss=0.3054  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=516s  iter_time=312.836s
2025-07-30 23:35:35 Train INFO: [Train]: [011][00050/00058] (86.4%)  Loss=0.9993  cls_loss=0.6916  reg_loss=0.3077  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=220s  iter_time=225.248s
2025-07-30 23:38:31 Train INFO: [Train]: [011][00058/00058] (100.0%)  Loss=0.9911  cls_loss=0.6852  reg_loss=0.3059  lr_det=9.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=175.734s
2025-07-30 23:38:32 Train INFO: [Train]: Epoch 11 completed in 1577.9s (avg 26.744s/iter)
2025-07-30 23:38:32 Train INFO: [Train]: Final Loss=0.9911
2025-07-30 23:38:33 Train INFO: Checkpoint saved at epoch 11
2025-07-30 23:38:33 Train INFO: [Train]: Epoch 12 started (Total iterations: 59)
2025-07-30 23:43:57 Train INFO: [Train]: [012][00010/00058] (18.6%)  Loss=0.9987  cls_loss=0.6905  reg_loss=0.3082  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1416s  iter_time=324.539s  fwd=2.063s/bwd=0.084s/opt=0.006s
2025-07-30 23:49:04 Train INFO: [Train]: [012][00020/00058] (35.6%)  Loss=1.0048  cls_loss=0.6947  reg_loss=0.3102  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1141s  iter_time=306.277s
2025-07-30 23:52:32 Train INFO: [Train]: [012][00030/00058] (52.5%)  Loss=0.9872  cls_loss=0.6831  reg_loss=0.3041  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=758s  iter_time=208.699s
2025-07-30 23:57:51 Train INFO: [Train]: [012][00040/00058] (69.5%)  Loss=0.9887  cls_loss=0.6849  reg_loss=0.3039  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=508s  iter_time=318.238s
2025-07-31 00:01:23 Train INFO: [Train]: [012][00050/00058] (86.4%)  Loss=0.9912  cls_loss=0.6863  reg_loss=0.3049  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=215s  iter_time=211.959s
2025-07-31 00:04:26 Train INFO: [Train]: [012][00058/00058] (100.0%)  Loss=0.9875  cls_loss=0.6836  reg_loss=0.3040  lr_det=9.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=183.141s
2025-07-31 00:04:27 Train INFO: [Train]: Epoch 12 completed in 1553.9s (avg 26.337s/iter)
2025-07-31 00:04:27 Train INFO: [Train]: Final Loss=0.9875
2025-07-31 00:04:27 Train INFO: [Train]: Epoch 13 started (Total iterations: 59)
2025-07-31 00:10:27 Train INFO: [Train]: [013][00010/00058] (18.6%)  Loss=1.0352  cls_loss=0.7142  reg_loss=0.3209  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1571s  iter_time=360.025s  fwd=2.164s/bwd=0.105s/opt=0.013s
2025-07-31 00:15:44 Train INFO: [Train]: [013][00020/00058] (35.6%)  Loss=1.0050  cls_loss=0.6944  reg_loss=0.3106  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1226s  iter_time=317.584s
2025-07-31 00:19:20 Train INFO: [Train]: [013][00030/00058] (52.5%)  Loss=0.9925  cls_loss=0.6870  reg_loss=0.3054  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=807s  iter_time=216.167s
2025-07-31 00:24:25 Train INFO: [Train]: [013][00040/00058] (69.5%)  Loss=1.0150  cls_loss=0.7012  reg_loss=0.3138  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=526s  iter_time=304.126s
2025-07-31 00:27:50 Train INFO: [Train]: [013][00050/00058] (86.4%)  Loss=1.0086  cls_loss=0.6977  reg_loss=0.3110  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=220s  iter_time=204.921s
2025-07-31 00:30:49 Train INFO: [Train]: [013][00058/00058] (100.0%)  Loss=1.0087  cls_loss=0.6984  reg_loss=0.3104  lr_det=9.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=179.475s
2025-07-31 00:30:50 Train INFO: [Train]: Epoch 13 completed in 1583.1s (avg 26.832s/iter)
2025-07-31 00:30:50 Train INFO: [Train]: Final Loss=1.0087
2025-07-31 00:30:50 Train INFO: Checkpoint saved at epoch 13
2025-07-31 00:30:50 Train INFO: [Train]: Epoch 14 started (Total iterations: 59)
2025-07-31 00:36:36 Train INFO: [Train]: [014][00010/00058] (18.6%)  Loss=0.9697  cls_loss=0.6771  reg_loss=0.2926  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1510s  iter_time=345.995s  fwd=2.070s/bwd=0.058s/opt=0.007s
2025-07-31 00:41:45 Train INFO: [Train]: [014][00020/00058] (35.6%)  Loss=0.9594  cls_loss=0.6696  reg_loss=0.2898  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1185s  iter_time=308.800s
2025-07-31 00:45:21 Train INFO: [Train]: [014][00030/00058] (52.5%)  Loss=0.9862  cls_loss=0.6863  reg_loss=0.2999  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=786s  iter_time=215.691s
2025-07-31 00:50:13 Train INFO: [Train]: [014][00040/00058] (69.5%)  Loss=0.9845  cls_loss=0.6858  reg_loss=0.2987  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=510s  iter_time=291.765s
2025-07-31 00:54:10 Train INFO: [Train]: [014][00050/00058] (86.4%)  Loss=0.9832  cls_loss=0.6846  reg_loss=0.2986  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=219s  iter_time=237.021s
2025-07-31 00:57:05 Train INFO: [Train]: [014][00058/00058] (100.0%)  Loss=0.9822  cls_loss=0.6835  reg_loss=0.2987  lr_det=9.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=175.444s
2025-07-31 00:57:06 Train INFO: [Train]: Epoch 14 completed in 1575.5s (avg 26.704s/iter)
2025-07-31 00:57:06 Train INFO: [Train]: Final Loss=0.9822
2025-07-31 00:57:06 Train INFO: [Train]: Epoch 15 started (Total iterations: 59)
2025-07-31 01:02:48 Train INFO: [Train]: [015][00010/00058] (18.6%)  Loss=1.0217  cls_loss=0.7070  reg_loss=0.3146  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1492s  iter_time=341.897s  fwd=2.078s/bwd=0.097s/opt=0.013s
2025-07-31 01:07:42 Train INFO: [Train]: [015][00020/00058] (35.6%)  Loss=1.0287  cls_loss=0.7106  reg_loss=0.3182  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1150s  iter_time=293.866s
2025-07-31 01:11:11 Train INFO: [Train]: [015][00030/00058] (52.5%)  Loss=1.0192  cls_loss=0.7057  reg_loss=0.3135  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=763s  iter_time=208.924s
2025-07-31 01:16:30 Train INFO: [Train]: [015][00040/00058] (69.5%)  Loss=1.0227  cls_loss=0.7080  reg_loss=0.3147  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=511s  iter_time=318.861s
2025-07-31 01:19:57 Train INFO: [Train]: [015][00050/00058] (86.4%)  Loss=1.0062  cls_loss=0.6962  reg_loss=0.3100  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=215s  iter_time=207.773s
2025-07-31 01:22:58 Train INFO: [Train]: [015][00058/00058] (100.0%)  Loss=0.9931  cls_loss=0.6869  reg_loss=0.3063  lr_det=9.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=180.666s
2025-07-31 01:22:59 Train INFO: [Train]: Epoch 15 completed in 1552.8s (avg 26.318s/iter)
2025-07-31 01:22:59 Train INFO: [Train]: Final Loss=0.9931
2025-07-31 01:22:59 Train INFO: Checkpoint saved at epoch 15
2025-07-31 01:22:59 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-31 01:28:44 Train INFO: [Train]: [016][00010/00058] (18.6%)  Loss=1.0781  cls_loss=0.7413  reg_loss=0.3368  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1503s  iter_time=344.461s  fwd=2.035s/bwd=0.056s/opt=0.004s
2025-07-31 01:33:50 Train INFO: [Train]: [016][00020/00058] (35.6%)  Loss=1.0178  cls_loss=0.7007  reg_loss=0.3171  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1177s  iter_time=305.935s
2025-07-31 01:37:26 Train INFO: [Train]: [016][00030/00058] (52.5%)  Loss=1.0023  cls_loss=0.6928  reg_loss=0.3095  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=783s  iter_time=216.500s
2025-07-31 01:42:29 Train INFO: [Train]: [016][00040/00058] (69.5%)  Loss=0.9999  cls_loss=0.6906  reg_loss=0.3093  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=514s  iter_time=302.774s
2025-07-31 01:46:10 Train INFO: [Train]: [016][00050/00058] (86.4%)  Loss=1.0085  cls_loss=0.6963  reg_loss=0.3123  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=218s  iter_time=220.561s
2025-07-31 01:49:17 Train INFO: [Train]: [016][00058/00058] (100.0%)  Loss=1.0009  cls_loss=0.6916  reg_loss=0.3093  lr_det=8.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=187.664s
2025-07-31 01:49:18 Train INFO: [Train]: Epoch 16 completed in 1578.7s (avg 26.757s/iter)
2025-07-31 01:49:18 Train INFO: [Train]: Final Loss=1.0009
2025-07-31 01:49:18 Train INFO: [Train]: Epoch 17 started (Total iterations: 59)
2025-07-31 01:55:09 Train INFO: [Train]: [017][00010/00058] (18.6%)  Loss=1.0386  cls_loss=0.7179  reg_loss=0.3207  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1530s  iter_time=350.549s  fwd=2.041s/bwd=0.054s/opt=0.008s
2025-07-31 02:00:18 Train INFO: [Train]: [017][00020/00058] (35.6%)  Loss=0.9770  cls_loss=0.6780  reg_loss=0.2990  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1194s  iter_time=309.457s
2025-07-31 02:04:13 Train INFO: [Train]: [017][00030/00058] (52.5%)  Loss=0.9774  cls_loss=0.6780  reg_loss=0.2994  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=808s  iter_time=234.760s
2025-07-31 09:18:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:18:12 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:18:13 Train INFO: training subset: 942 videos
2025-07-31 09:18:13 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:18:13 Train INFO: Using single GPU training...
2025-07-31 09:18:13 Train INFO: Using Model EMA...
2025-07-31 09:18:14 Train INFO: Using Automatic Mixed Precision...
2025-07-31 09:18:14 Train INFO: GPU Memory: 24.0 GB
2025-07-31 09:18:14 Train INFO: Freeze the backbone...
2025-07-31 09:18:14 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_15.pth
2025-07-31 09:18:15 Train INFO: Resume epoch is 15
2025-07-31 09:18:15 Train INFO: Training Starts...

2025-07-31 09:18:15 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-31 09:27:35 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:27:35 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(pipeline=[
        dict(format='avi', type='PrepareVideoInfo'),
        dict(num_threads=4, type='mmaction.DecordInit'),
        dict(
            method='sliding_window',
            num_clips=1,
            scale_factor=1,
            type='LoadFrames'),
        dict(type='mmaction.DecordDecode'),
        dict(scale=(
            -1,
            160,
        ), type='mmaction.Resize'),
        dict(crop_size=160, type='mmaction.CenterCrop'),
        dict(input_format='NCTHW', type='mmaction.FormatShape'),
        dict(keys=[
            'imgs',
        ], type='ConvertToTensor'),
        dict(inputs='imgs', keys=[
            'masks',
        ], type='Collect'),
    ]),
    train=dict(pipeline=[
        dict(format='avi', type='PrepareVideoInfo'),
        dict(num_threads=6, type='mmaction.DecordInit'),
        dict(
            crop_ratio=[
                0.9,
                1.0,
            ],
            method='random_trunc',
            num_clips=1,
            scale_factor=1,
            trunc_len=512,
            trunc_thresh=0.5,
            type='LoadFrames'),
        dict(type='mmaction.DecordDecode'),
        dict(scale=(
            -1,
            182,
        ), type='mmaction.Resize'),
        dict(type='mmaction.RandomResizedCrop'),
        dict(keep_ratio=False, scale=(
            160,
            160,
        ), type='mmaction.Resize'),
        dict(flip_ratio=0.5, type='mmaction.Flip'),
        dict(transforms='default', type='mmaction.ImgAug'),
        dict(type='mmaction.ColorJitter'),
        dict(input_format='NCTHW', type='mmaction.FormatShape'),
        dict(
            keys=[
                'imgs',
                'gt_segments',
                'gt_labels',
            ],
            type='ConvertToTensor'),
        dict(
            inputs='imgs',
            keys=[
                'masks',
                'gt_segments',
                'gt_labels',
            ],
            type='Collect'),
    ]))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51))
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:28:20 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:28:20 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        type='PkuSlidingDataset'),
    train=dict(
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51))
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:30:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:30:11 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51))
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:30:12 Train INFO: training subset: 942 videos
2025-07-31 09:30:12 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:30:49 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:30:49 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:30:50 Train INFO: training subset: 942 videos
2025-07-31 09:30:50 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:32:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:32:05 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:32:06 Train INFO: training subset: 942 videos
2025-07-31 09:32:06 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:32:06 Train INFO: Using single GPU training...
2025-07-31 09:32:06 Train INFO: Using Model EMA...
2025-07-31 09:32:06 Train INFO: Using Automatic Mixed Precision...
2025-07-31 09:32:06 Train INFO: GPU Memory: 24.0 GB
2025-07-31 09:32:06 Train INFO: Freeze the backbone...
2025-07-31 09:32:06 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_15.pth
2025-07-31 09:32:07 Train INFO: Resume epoch is 15
2025-07-31 09:32:07 Train INFO: Training Starts...

2025-07-31 09:32:07 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-31 09:37:50 Train INFO: [Train]: [016][00010/00058] (18.6%)  Loss=1.0778  cls_loss=0.7410  reg_loss=0.3368  lr_det=9.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1497s  iter_time=343.147s  fwd=2.065s/bwd=0.056s/opt=0.014s
2025-07-31 09:41:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:41:09 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=8, num_workers=2))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:41:10 Train INFO: training subset: 942 videos
2025-07-31 09:41:10 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:41:11 Train INFO: Using single GPU training...
2025-07-31 09:41:11 Train INFO: Using Model EMA...
2025-07-31 09:41:11 Train INFO: Using Automatic Mixed Precision...
2025-07-31 09:41:11 Train INFO: GPU Memory: 24.0 GB
2025-07-31 09:41:11 Train INFO: Freeze the backbone...
2025-07-31 09:41:11 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_15.pth
2025-07-31 09:41:13 Train INFO: Resume epoch is 15
2025-07-31 09:41:13 Train INFO: Training Starts...

2025-07-31 09:41:13 Train INFO: [Train]: Epoch 16 started (Total iterations: 118)
2025-07-31 09:45:06 Train INFO: [Train]: [016][00010/00117] (9.3%)  Loss=0.6885  cls_loss=0.4831  reg_loss=0.2054  lr_det=9.0e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2267s  iter_time=233.038s  fwd=1.204s/bwd=0.078s/opt=0.021s
2025-07-31 09:48:25 Train INFO: [Train]: [016][00020/00117] (17.8%)  Loss=0.7528  cls_loss=0.5217  reg_loss=0.2311  lr_det=9.0e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1998s  iter_time=199.558s
2025-07-31 09:51:47 Train INFO: [Train]: [016][00030/00117] (26.3%)  Loss=0.8102  cls_loss=0.5634  reg_loss=0.2468  lr_det=9.0e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1781s  iter_time=202.132s
2025-07-31 09:55:03 Train INFO: [Train]: [016][00040/00117] (34.7%)  Loss=0.8565  cls_loss=0.5973  reg_loss=0.2592  lr_det=8.9e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1560s  iter_time=195.912s
2025-07-31 09:58:26 Train INFO: [Train]: [016][00050/00117] (43.2%)  Loss=0.8888  cls_loss=0.6191  reg_loss=0.2698  lr_det=8.9e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1357s  iter_time=202.519s
2025-07-31 10:01:15 Train INFO: [Train]: [016][00060/00117] (51.7%)  Loss=0.9127  cls_loss=0.6334  reg_loss=0.2793  lr_det=8.9e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1124s  iter_time=169.682s
2025-07-31 10:04:04 Train INFO: [Train]: [016][00070/00117] (60.2%)  Loss=0.9248  cls_loss=0.6425  reg_loss=0.2823  lr_det=8.8e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=908s  iter_time=168.292s
2025-07-31 10:06:44 Train INFO: [Train]: [016][00080/00117] (68.6%)  Loss=0.9364  cls_loss=0.6494  reg_loss=0.2870  lr_det=8.8e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=699s  iter_time=160.126s
2025-07-31 10:09:29 Train INFO: [Train]: [016][00090/00117] (77.1%)  Loss=0.9431  cls_loss=0.6542  reg_loss=0.2890  lr_det=8.8e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=503s  iter_time=165.469s
2025-07-31 10:12:18 Train INFO: [Train]: [016][00100/00117] (85.6%)  Loss=0.9410  cls_loss=0.6523  reg_loss=0.2887  lr_det=8.7e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=314s  iter_time=168.504s
2025-07-31 10:15:01 Train INFO: [Train]: [016][00110/00117] (94.1%)  Loss=0.9487  cls_loss=0.6561  reg_loss=0.2926  lr_det=8.7e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=128s  iter_time=163.098s
2025-07-31 10:16:33 Train INFO: [Train]: [016][00117/00117] (100.0%)  Loss=0.9511  cls_loss=0.6581  reg_loss=0.2930  lr_det=8.7e-05  GPU=1054MB(alloc)/2286MB(reserved)/4968MB(max)  ETA=0s  iter_time=91.872s
2025-07-31 10:16:34 Train INFO: [Train]: Epoch 16 completed in 2121.2s (avg 17.976s/iter)
2025-07-31 10:16:34 Train INFO: [Train]: Final Loss=0.9511
2025-07-31 10:16:34 Train INFO: [Train]: Epoch 17 started (Total iterations: 118)
2025-07-31 10:20:07 Train INFO: [Train]: [017][00010/00117] (9.3%)  Loss=1.0008  cls_loss=0.6920  reg_loss=0.3088  lr_det=8.7e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2073s  iter_time=213.067s  fwd=1.103s/bwd=0.069s/opt=0.011s
2025-07-31 10:22:53 Train INFO: [Train]: [017][00020/00117] (17.8%)  Loss=1.0102  cls_loss=0.6956  reg_loss=0.3146  lr_det=8.6e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1753s  iter_time=166.479s
2025-07-31 10:25:41 Train INFO: [Train]: [017][00030/00117] (26.3%)  Loss=1.0007  cls_loss=0.6927  reg_loss=0.3080  lr_det=8.6e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1536s  iter_time=167.625s
2025-07-31 10:28:23 Train INFO: [Train]: [017][00040/00117] (34.7%)  Loss=0.9989  cls_loss=0.6938  reg_loss=0.3052  lr_det=8.6e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1333s  iter_time=162.528s
2025-07-31 10:31:00 Train INFO: [Train]: [017][00050/00117] (43.2%)  Loss=0.9955  cls_loss=0.6922  reg_loss=0.3034  lr_det=8.5e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1138s  iter_time=156.884s
2025-07-31 10:34:05 Train INFO: [Train]: [017][00060/00117] (51.7%)  Loss=1.0043  cls_loss=0.6983  reg_loss=0.3060  lr_det=8.5e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=982s  iter_time=184.818s
2025-07-31 10:36:51 Train INFO: [Train]: [017][00070/00117] (60.2%)  Loss=0.9949  cls_loss=0.6914  reg_loss=0.3035  lr_det=8.4e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=806s  iter_time=165.535s
2025-07-31 10:39:40 Train INFO: [Train]: [017][00080/00117] (68.6%)  Loss=0.9985  cls_loss=0.6939  reg_loss=0.3046  lr_det=8.4e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=633s  iter_time=169.217s
2025-07-31 10:42:31 Train INFO: [Train]: [017][00090/00117] (77.1%)  Loss=1.0033  cls_loss=0.6964  reg_loss=0.3069  lr_det=8.4e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=462s  iter_time=170.817s
2025-07-31 10:45:23 Train INFO: [Train]: [017][00100/00117] (85.6%)  Loss=0.9966  cls_loss=0.6911  reg_loss=0.3055  lr_det=8.3e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=291s  iter_time=172.695s
2025-07-31 10:48:35 Train INFO: [Train]: [017][00110/00117] (94.1%)  Loss=0.9908  cls_loss=0.6869  reg_loss=0.3039  lr_det=8.3e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=121s  iter_time=191.813s
2025-07-31 10:50:28 Train INFO: [Train]: [017][00117/00117] (100.0%)  Loss=0.9843  cls_loss=0.6828  reg_loss=0.3016  lr_det=8.3e-05  GPU=1054MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=0s  iter_time=112.520s
2025-07-31 10:50:29 Train INFO: [Train]: Epoch 17 completed in 2035.4s (avg 17.249s/iter)
2025-07-31 10:50:29 Train INFO: [Train]: Final Loss=0.9843
2025-07-31 10:50:30 Train INFO: Checkpoint saved at epoch 17
2025-07-31 10:50:30 Train INFO: [Train]: Epoch 18 started (Total iterations: 118)
2025-07-31 10:54:37 Train INFO: [Train]: [018][00010/00117] (9.3%)  Loss=1.0581  cls_loss=0.7261  reg_loss=0.3320  lr_det=8.2e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2399s  iter_time=246.588s  fwd=1.234s/bwd=0.079s/opt=0.019s
2025-07-31 10:57:51 Train INFO: [Train]: [018][00020/00117] (17.8%)  Loss=1.0485  cls_loss=0.7233  reg_loss=0.3252  lr_det=8.2e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2035s  iter_time=193.963s
2025-07-31 11:00:49 Train INFO: [Train]: [018][00030/00117] (26.3%)  Loss=1.0493  cls_loss=0.7236  reg_loss=0.3257  lr_det=8.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1737s  iter_time=178.266s
2025-07-31 11:03:42 Train INFO: [Train]: [018][00040/00117] (34.7%)  Loss=1.0248  cls_loss=0.7068  reg_loss=0.3180  lr_det=8.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1486s  iter_time=172.518s
2025-07-31 11:06:33 Train INFO: [Train]: [018][00050/00117] (43.2%)  Loss=1.0186  cls_loss=0.7038  reg_loss=0.3147  lr_det=8.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1265s  iter_time=171.379s
2025-07-31 11:09:17 Train INFO: [Train]: [018][00060/00117] (51.7%)  Loss=1.0142  cls_loss=0.7010  reg_loss=0.3132  lr_det=8.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1053s  iter_time=164.453s
2025-07-31 11:12:01 Train INFO: [Train]: [018][00070/00117] (60.2%)  Loss=1.0100  cls_loss=0.6985  reg_loss=0.3115  lr_det=8.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=855s  iter_time=163.822s
2025-07-31 11:14:51 Train INFO: [Train]: [018][00080/00117] (68.6%)  Loss=1.0083  cls_loss=0.6959  reg_loss=0.3124  lr_det=8.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=667s  iter_time=169.300s
2025-07-31 11:17:42 Train INFO: [Train]: [018][00090/00117] (77.1%)  Loss=1.0059  cls_loss=0.6937  reg_loss=0.3122  lr_det=7.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=484s  iter_time=171.570s
2025-07-31 11:20:28 Train INFO: [Train]: [018][00100/00117] (85.6%)  Loss=0.9998  cls_loss=0.6905  reg_loss=0.3094  lr_det=7.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=303s  iter_time=165.339s
2025-07-31 11:23:12 Train INFO: [Train]: [018][00110/00117] (94.1%)  Loss=1.0024  cls_loss=0.6928  reg_loss=0.3096  lr_det=7.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=124s  iter_time=164.093s
2025-07-31 11:24:51 Train INFO: [Train]: [018][00117/00117] (100.0%)  Loss=0.9931  cls_loss=0.6863  reg_loss=0.3068  lr_det=7.8e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=99.161s
2025-07-31 11:24:52 Train INFO: [Train]: Epoch 18 completed in 2061.4s (avg 17.470s/iter)
2025-07-31 11:24:52 Train INFO: [Train]: Final Loss=0.9931
2025-07-31 11:24:52 Train INFO: [Train]: Epoch 19 started (Total iterations: 118)
2025-07-31 11:28:23 Train INFO: [Train]: [019][00010/00117] (9.3%)  Loss=1.0938  cls_loss=0.7549  reg_loss=0.3389  lr_det=7.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2060s  iter_time=211.770s  fwd=1.137s/bwd=0.040s/opt=0.017s
2025-07-31 11:31:07 Train INFO: [Train]: [019][00020/00117] (17.8%)  Loss=1.0176  cls_loss=0.7055  reg_loss=0.3122  lr_det=7.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1735s  iter_time=163.779s
2025-07-31 11:33:54 Train INFO: [Train]: [019][00030/00117] (26.3%)  Loss=1.0328  cls_loss=0.7129  reg_loss=0.3198  lr_det=7.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1521s  iter_time=166.370s
2025-07-31 11:36:34 Train INFO: [Train]: [019][00040/00117] (34.7%)  Loss=1.0296  cls_loss=0.7101  reg_loss=0.3194  lr_det=7.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1318s  iter_time=159.905s
2025-07-31 11:39:17 Train INFO: [Train]: [019][00050/00117] (43.2%)  Loss=1.0115  cls_loss=0.6978  reg_loss=0.3137  lr_det=7.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1136s  iter_time=163.019s
2025-07-31 11:42:01 Train INFO: [Train]: [019][00060/00117] (51.7%)  Loss=1.0100  cls_loss=0.6977  reg_loss=0.3123  lr_det=7.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=962s  iter_time=164.664s
2025-07-31 11:44:45 Train INFO: [Train]: [019][00070/00117] (60.2%)  Loss=1.0047  cls_loss=0.6944  reg_loss=0.3103  lr_det=7.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=790s  iter_time=163.580s
2025-07-31 11:47:27 Train INFO: [Train]: [019][00080/00117] (68.6%)  Loss=1.0050  cls_loss=0.6934  reg_loss=0.3116  lr_det=7.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=619s  iter_time=162.484s
2025-07-31 11:50:08 Train INFO: [Train]: [019][00090/00117] (77.1%)  Loss=1.0071  cls_loss=0.6952  reg_loss=0.3119  lr_det=7.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=450s  iter_time=160.944s
2025-07-31 11:52:51 Train INFO: [Train]: [019][00100/00117] (85.6%)  Loss=0.9994  cls_loss=0.6899  reg_loss=0.3095  lr_det=7.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=283s  iter_time=162.500s
2025-07-31 11:55:35 Train INFO: [Train]: [019][00110/00117] (94.1%)  Loss=1.0025  cls_loss=0.6922  reg_loss=0.3103  lr_det=7.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=116s  iter_time=164.027s
2025-07-31 11:57:14 Train INFO: [Train]: [019][00117/00117] (100.0%)  Loss=0.9988  cls_loss=0.6893  reg_loss=0.3095  lr_det=7.3e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=99.302s
2025-07-31 11:57:15 Train INFO: [Train]: Epoch 19 completed in 1943.1s (avg 16.467s/iter)
2025-07-31 11:57:15 Train INFO: [Train]: Final Loss=0.9988
2025-07-31 11:57:15 Train INFO: Checkpoint saved at epoch 19
2025-07-31 11:57:15 Train INFO: [Train]: Epoch 20 started (Total iterations: 118)
2025-07-31 12:00:45 Train INFO: [Train]: [020][00010/00117] (9.3%)  Loss=1.0085  cls_loss=0.6929  reg_loss=0.3156  lr_det=7.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2043s  iter_time=210.004s  fwd=1.221s/bwd=0.064s/opt=0.017s
2025-07-31 12:03:26 Train INFO: [Train]: [020][00020/00117] (17.8%)  Loss=0.9927  cls_loss=0.6822  reg_loss=0.3105  lr_det=7.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1710s  iter_time=160.282s
2025-07-31 12:06:12 Train INFO: [Train]: [020][00030/00117] (26.3%)  Loss=0.9718  cls_loss=0.6695  reg_loss=0.3023  lr_det=7.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1506s  iter_time=166.318s
2025-07-31 12:08:57 Train INFO: [Train]: [020][00040/00117] (34.7%)  Loss=0.9861  cls_loss=0.6822  reg_loss=0.3039  lr_det=7.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1317s  iter_time=164.578s
2025-07-31 12:11:39 Train INFO: [Train]: [020][00050/00117] (43.2%)  Loss=0.9768  cls_loss=0.6761  reg_loss=0.3007  lr_det=7.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1134s  iter_time=162.130s
2025-07-31 12:14:16 Train INFO: [Train]: [020][00060/00117] (51.7%)  Loss=0.9861  cls_loss=0.6820  reg_loss=0.3041  lr_det=7.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=954s  iter_time=157.396s
2025-07-31 12:16:54 Train INFO: [Train]: [020][00070/00117] (60.2%)  Loss=0.9884  cls_loss=0.6843  reg_loss=0.3041  lr_det=7.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=780s  iter_time=157.443s
2025-07-31 12:19:36 Train INFO: [Train]: [020][00080/00117] (68.6%)  Loss=0.9829  cls_loss=0.6802  reg_loss=0.3027  lr_det=7.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=612s  iter_time=162.494s
2025-07-31 12:22:25 Train INFO: [Train]: [020][00090/00117] (77.1%)  Loss=0.9853  cls_loss=0.6811  reg_loss=0.3041  lr_det=6.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=448s  iter_time=168.680s
2025-07-31 12:25:12 Train INFO: [Train]: [020][00100/00117] (85.6%)  Loss=0.9837  cls_loss=0.6811  reg_loss=0.3026  lr_det=6.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=282s  iter_time=166.886s
2025-07-31 12:27:55 Train INFO: [Train]: [020][00110/00117] (94.1%)  Loss=0.9779  cls_loss=0.6772  reg_loss=0.3007  lr_det=6.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=116s  iter_time=163.188s
2025-07-31 12:29:26 Train INFO: [Train]: [020][00117/00117] (100.0%)  Loss=0.9819  cls_loss=0.6801  reg_loss=0.3018  lr_det=6.8e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=90.889s
2025-07-31 12:29:27 Train INFO: [Train]: Epoch 20 completed in 1931.1s (avg 16.365s/iter)
2025-07-31 12:29:27 Train INFO: [Train]: Final Loss=0.9819
2025-07-31 12:29:27 Train INFO: [Train]: Epoch 21 started (Total iterations: 118)
2025-07-31 12:32:50 Train INFO: [Train]: [021][00010/00117] (9.3%)  Loss=1.0273  cls_loss=0.7185  reg_loss=0.3088  lr_det=6.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1981s  iter_time=203.605s  fwd=1.139s/bwd=0.061s/opt=0.021s
2025-07-31 12:35:32 Train INFO: [Train]: [021][00020/00117] (17.8%)  Loss=1.0114  cls_loss=0.7062  reg_loss=0.3053  lr_det=6.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1687s  iter_time=161.579s
2025-07-31 12:38:18 Train INFO: [Train]: [021][00030/00117] (26.3%)  Loss=0.9923  cls_loss=0.6889  reg_loss=0.3034  lr_det=6.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1492s  iter_time=166.577s
2025-07-31 12:41:27 Train INFO: [Train]: [021][00040/00117] (34.7%)  Loss=0.9887  cls_loss=0.6891  reg_loss=0.2996  lr_det=6.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1354s  iter_time=189.138s
2025-07-31 12:44:49 Train INFO: [Train]: [021][00050/00117] (43.2%)  Loss=0.9866  cls_loss=0.6883  reg_loss=0.2983  lr_det=6.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1212s  iter_time=201.356s
2025-07-31 12:47:58 Train INFO: [Train]: [021][00060/00117] (51.7%)  Loss=1.0001  cls_loss=0.6961  reg_loss=0.3040  lr_det=6.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1039s  iter_time=189.200s
2025-07-31 12:50:53 Train INFO: [Train]: [021][00070/00117] (60.2%)  Loss=0.9929  cls_loss=0.6891  reg_loss=0.3038  lr_det=6.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=852s  iter_time=175.463s
2025-07-31 12:54:09 Train INFO: [Train]: [021][00080/00117] (68.6%)  Loss=0.9878  cls_loss=0.6861  reg_loss=0.3017  lr_det=6.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=677s  iter_time=195.315s
2025-07-31 12:57:29 Train INFO: [Train]: [021][00090/00117] (77.1%)  Loss=0.9961  cls_loss=0.6913  reg_loss=0.3048  lr_det=6.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=499s  iter_time=200.330s
2025-07-31 13:00:38 Train INFO: [Train]: [021][00100/00117] (85.6%)  Loss=0.9868  cls_loss=0.6845  reg_loss=0.3022  lr_det=6.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=315s  iter_time=189.357s
2025-07-31 13:03:52 Train INFO: [Train]: [021][00110/00117] (94.1%)  Loss=0.9864  cls_loss=0.6845  reg_loss=0.3019  lr_det=6.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=130s  iter_time=193.508s
2025-07-31 13:05:51 Train INFO: [Train]: [021][00117/00117] (100.0%)  Loss=0.9834  cls_loss=0.6832  reg_loss=0.3002  lr_det=6.3e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=118.559s
2025-07-31 13:05:52 Train INFO: [Train]: Epoch 21 completed in 2185.4s (avg 18.520s/iter)
2025-07-31 13:05:52 Train INFO: [Train]: Final Loss=0.9834
2025-07-31 13:05:53 Train INFO: Checkpoint saved at epoch 21
2025-07-31 13:05:53 Train INFO: [Train]: Epoch 22 started (Total iterations: 118)
2025-07-31 13:10:04 Train INFO: [Train]: [022][00010/00117] (9.3%)  Loss=1.0299  cls_loss=0.7169  reg_loss=0.3130  lr_det=6.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2441s  iter_time=250.989s  fwd=1.172s/bwd=0.078s/opt=0.020s
2025-07-31 13:13:25 Train INFO: [Train]: [022][00020/00117] (17.8%)  Loss=1.0183  cls_loss=0.7056  reg_loss=0.3127  lr_det=6.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2086s  iter_time=200.715s
2025-07-31 13:16:40 Train INFO: [Train]: [022][00030/00117] (26.3%)  Loss=1.0293  cls_loss=0.7149  reg_loss=0.3144  lr_det=6.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1815s  iter_time=195.133s
2025-07-31 13:19:53 Train INFO: [Train]: [022][00040/00117] (34.7%)  Loss=1.0051  cls_loss=0.6972  reg_loss=0.3080  lr_det=6.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1577s  iter_time=193.081s
2025-07-31 13:23:07 Train INFO: [Train]: [022][00050/00117] (43.2%)  Loss=1.0000  cls_loss=0.6941  reg_loss=0.3058  lr_det=6.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1358s  iter_time=193.783s
2025-07-31 13:25:51 Train INFO: [Train]: [022][00060/00117] (51.7%)  Loss=1.0078  cls_loss=0.6992  reg_loss=0.3086  lr_det=6.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1119s  iter_time=164.031s
2025-07-31 13:28:40 Train INFO: [Train]: [022][00070/00117] (60.2%)  Loss=1.0001  cls_loss=0.6940  reg_loss=0.3061  lr_det=5.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=905s  iter_time=168.984s
2025-07-31 13:31:25 Train INFO: [Train]: [022][00080/00117] (68.6%)  Loss=0.9921  cls_loss=0.6889  reg_loss=0.3032  lr_det=5.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=700s  iter_time=164.750s
2025-07-31 13:34:14 Train INFO: [Train]: [022][00090/00117] (77.1%)  Loss=0.9920  cls_loss=0.6891  reg_loss=0.3029  lr_det=5.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=505s  iter_time=169.378s
2025-07-31 13:36:58 Train INFO: [Train]: [022][00100/00117] (85.6%)  Loss=0.9891  cls_loss=0.6873  reg_loss=0.3018  lr_det=5.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=314s  iter_time=163.925s
2025-07-31 13:39:42 Train INFO: [Train]: [022][00110/00117] (94.1%)  Loss=0.9901  cls_loss=0.6867  reg_loss=0.3034  lr_det=5.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=128s  iter_time=163.965s
2025-07-31 13:41:25 Train INFO: [Train]: [022][00117/00117] (100.0%)  Loss=0.9870  cls_loss=0.6843  reg_loss=0.3027  lr_det=5.7e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=103.499s
2025-07-31 13:41:27 Train INFO: [Train]: Epoch 22 completed in 2133.5s (avg 18.080s/iter)
2025-07-31 13:41:27 Train INFO: [Train]: Final Loss=0.9870
2025-07-31 13:41:27 Train INFO: [Train]: Epoch 23 started (Total iterations: 118)
2025-07-31 13:45:05 Train INFO: [Train]: [023][00010/00117] (9.3%)  Loss=0.9826  cls_loss=0.6748  reg_loss=0.3078  lr_det=5.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2121s  iter_time=218.082s  fwd=1.195s/bwd=0.048s/opt=0.012s
2025-07-31 13:47:55 Train INFO: [Train]: [023][00020/00117] (17.8%)  Loss=0.9786  cls_loss=0.6714  reg_loss=0.3072  lr_det=5.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1795s  iter_time=170.586s
2025-07-31 13:50:43 Train INFO: [Train]: [023][00030/00117] (26.3%)  Loss=0.9797  cls_loss=0.6749  reg_loss=0.3048  lr_det=5.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1563s  iter_time=168.117s
2025-07-31 13:53:35 Train INFO: [Train]: [023][00040/00117] (34.7%)  Loss=0.9873  cls_loss=0.6789  reg_loss=0.3084  lr_det=5.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1368s  iter_time=171.410s
2025-07-31 13:56:29 Train INFO: [Train]: [023][00050/00117] (43.2%)  Loss=0.9781  cls_loss=0.6743  reg_loss=0.3038  lr_det=5.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1186s  iter_time=174.370s
2025-07-31 13:59:12 Train INFO: [Train]: [023][00060/00117] (51.7%)  Loss=0.9913  cls_loss=0.6840  reg_loss=0.3073  lr_det=5.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=996s  iter_time=162.860s
2025-07-31 14:01:57 Train INFO: [Train]: [023][00070/00117] (60.2%)  Loss=0.9782  cls_loss=0.6760  reg_loss=0.3022  lr_det=5.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=815s  iter_time=165.357s
2025-07-31 14:04:49 Train INFO: [Train]: [023][00080/00117] (68.6%)  Loss=0.9789  cls_loss=0.6764  reg_loss=0.3026  lr_det=5.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=641s  iter_time=172.085s
2025-07-31 14:07:31 Train INFO: [Train]: [023][00090/00117] (77.1%)  Loss=0.9814  cls_loss=0.6769  reg_loss=0.3045  lr_det=5.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=464s  iter_time=161.570s
2025-07-31 14:10:11 Train INFO: [Train]: [023][00100/00117] (85.6%)  Loss=0.9766  cls_loss=0.6745  reg_loss=0.3021  lr_det=5.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=290s  iter_time=160.138s
2025-07-31 14:13:01 Train INFO: [Train]: [023][00110/00117] (94.1%)  Loss=0.9815  cls_loss=0.6774  reg_loss=0.3041  lr_det=5.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=119s  iter_time=169.765s
2025-07-31 14:14:41 Train INFO: [Train]: [023][00117/00117] (100.0%)  Loss=0.9798  cls_loss=0.6758  reg_loss=0.3041  lr_det=5.1e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=100.520s
2025-07-31 14:14:42 Train INFO: [Train]: Epoch 23 completed in 1995.9s (avg 16.914s/iter)
2025-07-31 14:14:42 Train INFO: [Train]: Final Loss=0.9798
2025-07-31 14:14:43 Train INFO: Checkpoint saved at epoch 23
2025-07-31 14:14:43 Train INFO: [Train]: Epoch 24 started (Total iterations: 118)
2025-07-31 14:18:27 Train INFO: [Train]: [024][00010/00117] (9.3%)  Loss=1.0056  cls_loss=0.7009  reg_loss=0.3048  lr_det=5.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2180s  iter_time=224.116s  fwd=1.122s/bwd=0.074s/opt=0.014s
2025-07-31 14:21:15 Train INFO: [Train]: [024][00020/00117] (17.8%)  Loss=0.9916  cls_loss=0.6885  reg_loss=0.3031  lr_det=5.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1812s  iter_time=168.156s
2025-07-31 14:23:47 Evaluation INFO: Starting evaluation...
2025-07-31 14:24:04 Train INFO: [Train]: [024][00030/00117] (26.3%)  Loss=1.0027  cls_loss=0.6976  reg_loss=0.3050  lr_det=5.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1576s  iter_time=169.177s
2025-07-31 14:24:49 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-31 14:24:49 Evaluation INFO: Loaded annotations from test subset.
2025-07-31 14:24:49 Evaluation INFO: Number of ground truth instances: 2704
2025-07-31 14:24:49 Evaluation INFO: Number of predictions: 264000
2025-07-31 14:24:49 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-31 14:24:49 Evaluation INFO: Average-mAP: 0.01 (%)
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.20 is 0.05%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.40 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.50 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.60 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.70 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-31 14:24:49 Evaluation INFO: Evaluation completed!
2025-07-31 14:26:59 Train INFO: [Train]: [024][00040/00117] (34.7%)  Loss=1.0011  cls_loss=0.6941  reg_loss=0.3071  lr_det=4.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1382s  iter_time=174.298s
2025-07-31 14:29:45 Train INFO: [Train]: [024][00050/00117] (43.2%)  Loss=0.9923  cls_loss=0.6892  reg_loss=0.3031  lr_det=4.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1185s  iter_time=166.149s
2025-07-31 14:32:34 Train INFO: [Train]: [024][00060/00117] (51.7%)  Loss=0.9917  cls_loss=0.6890  reg_loss=0.3026  lr_det=4.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1001s  iter_time=169.330s
2025-07-31 14:35:16 Train INFO: [Train]: [024][00070/00117] (60.2%)  Loss=0.9756  cls_loss=0.6782  reg_loss=0.2974  lr_det=4.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=816s  iter_time=162.011s
2025-07-31 14:38:02 Train INFO: [Train]: [024][00080/00117] (68.6%)  Loss=0.9839  cls_loss=0.6849  reg_loss=0.2989  lr_det=4.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=639s  iter_time=165.268s
2025-07-31 14:40:48 Train INFO: [Train]: [024][00090/00117] (77.1%)  Loss=0.9882  cls_loss=0.6872  reg_loss=0.3010  lr_det=4.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=464s  iter_time=165.983s
2025-07-31 14:43:39 Train INFO: [Train]: [024][00100/00117] (85.6%)  Loss=0.9853  cls_loss=0.6851  reg_loss=0.3002  lr_det=4.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=292s  iter_time=171.268s
2025-07-31 14:46:21 Train INFO: [Train]: [024][00110/00117] (94.1%)  Loss=0.9843  cls_loss=0.6839  reg_loss=0.3003  lr_det=4.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=120s  iter_time=162.173s
2025-07-31 14:48:01 Train INFO: [Train]: [024][00117/00117] (100.0%)  Loss=0.9786  cls_loss=0.6793  reg_loss=0.2992  lr_det=4.6e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=99.943s
2025-07-31 14:48:02 Train INFO: [Train]: Epoch 24 completed in 1998.8s (avg 16.939s/iter)
2025-07-31 14:48:02 Train INFO: [Train]: Final Loss=0.9786
2025-07-31 14:48:02 Train INFO: [Train]: Epoch 25 started (Total iterations: 118)
2025-07-31 14:51:35 Train INFO: [Train]: [025][00010/00117] (9.3%)  Loss=0.9992  cls_loss=0.7001  reg_loss=0.2991  lr_det=4.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2071s  iter_time=212.934s  fwd=1.108s/bwd=0.042s/opt=0.014s
2025-07-31 14:54:26 Train INFO: [Train]: [025][00020/00117] (17.8%)  Loss=1.0118  cls_loss=0.7051  reg_loss=0.3067  lr_det=4.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1774s  iter_time=171.087s
2025-07-31 15:20:51 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 15:20:51 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-07-31 15:20:52 Train INFO: training subset: 831 videos
2025-07-31 15:24:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 15:24:02 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-07-31 15:24:03 Train INFO: training subset: 831 videos
2025-07-31 15:25:04 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 15:25:04 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-07-31 15:25:05 Train INFO: training subset: 831 videos
2025-07-31 15:25:05 Train INFO: validation subset: 111 videos
2025-07-31 15:25:05 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-07-31 15:25:06 Train INFO: Using single GPU training...
2025-07-31 15:25:06 Train INFO: Using Model EMA...
2025-07-31 15:25:06 Train INFO: Using Automatic Mixed Precision...
2025-07-31 15:25:06 Train INFO: GPU Memory: 24.0 GB
2025-07-31 15:25:06 Train INFO: Freeze the backbone...
2025-07-31 15:25:06 Train INFO: Training Starts...

2025-07-31 15:25:06 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-07-31 15:33:13 Train INFO: [Train]: [000][00010/00051] (21.2%)  Loss=3.3739  cls_loss=1.8422  reg_loss=1.5317  lr_det=3.9e-06  GPU=1430MB(alloc)/11454MB(reserved)/9063MB(max)  ETA=1816s  iter_time=487.157s  fwd=2.145s/bwd=0.107s/opt=0.012s
2025-07-31 15:40:53 Train INFO: [Train]: [000][00020/00051] (40.4%)  Loss=2.6993  cls_loss=1.5757  reg_loss=1.1236  lr_det=7.7e-06  GPU=1430MB(alloc)/11454MB(reserved)/9063MB(max)  ETA=1398s  iter_time=460.094s
2025-07-31 15:46:10 Train INFO: [Train]: [000][00030/00051] (59.6%)  Loss=2.3302  cls_loss=1.4309  reg_loss=0.8994  lr_det=1.2e-05  GPU=1430MB(alloc)/11454MB(reserved)/9063MB(max)  ETA=856s  iter_time=317.077s
2025-07-31 15:54:37 Train INFO: [Train]: [000][00040/00051] (78.8%)  Loss=2.1058  cls_loss=1.3305  reg_loss=0.7753  lr_det=1.5e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=475s  iter_time=506.442s
2025-07-31 16:00:05 Train INFO: [Train]: [000][00050/00051] (98.1%)  Loss=1.9501  cls_loss=1.2512  reg_loss=0.6990  lr_det=1.9e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=41s  iter_time=328.329s
2025-07-31 16:00:10 Train INFO: [Train]: [000][00051/00051] (100.0%)  Loss=1.9338  cls_loss=1.2419  reg_loss=0.6919  lr_det=2.0e-05  GPU=1395MB(alloc)/4210MB(reserved)/9063MB(max)  ETA=0s  iter_time=4.863s
2025-07-31 16:00:11 Train INFO: [Train]: Epoch 0 completed in 2105.2s (avg 40.485s/iter)
2025-07-31 16:00:11 Train INFO: [Train]: Final Loss=1.9338
2025-07-31 16:00:11 Train INFO: [Train]: Epoch 1 started (Total iterations: 52)
2025-07-31 16:09:19 Train INFO: [Train]: [001][00010/00051] (21.2%)  Loss=1.2270  cls_loss=0.8527  reg_loss=0.3742  lr_det=2.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=2041s  iter_time=547.668s  fwd=2.183s/bwd=0.067s/opt=0.008s
2025-07-31 16:17:32 Train INFO: [Train]: [001][00020/00051] (40.4%)  Loss=1.2124  cls_loss=0.8401  reg_loss=0.3723  lr_det=2.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1537s  iter_time=493.328s
2025-07-31 16:23:50 Train INFO: [Train]: [001][00030/00051] (59.6%)  Loss=1.2096  cls_loss=0.8361  reg_loss=0.3735  lr_det=3.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=961s  iter_time=377.393s
2025-07-31 16:32:34 Train INFO: [Train]: [001][00040/00051] (78.8%)  Loss=1.2032  cls_loss=0.8300  reg_loss=0.3732  lr_det=3.6e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=521s  iter_time=524.847s
2025-07-31 16:38:50 Train INFO: [Train]: [001][00050/00051] (98.1%)  Loss=1.1999  cls_loss=0.8267  reg_loss=0.3732  lr_det=3.9e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=45s  iter_time=375.768s
2025-07-31 16:38:53 Train INFO: [Train]: [001][00051/00051] (100.0%)  Loss=1.1972  cls_loss=0.8247  reg_loss=0.3725  lr_det=4.0e-05  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=2.778s
2025-07-31 16:38:54 Train INFO: [Train]: Epoch 1 completed in 2323.3s (avg 44.679s/iter)
2025-07-31 16:38:54 Train INFO: [Train]: Final Loss=1.1972
2025-07-31 16:38:56 Train INFO: Checkpoint saved at epoch 1
2025-07-31 16:38:56 Train INFO: [Train]: Epoch 2 started (Total iterations: 52)
2025-07-31 16:48:25 Train INFO: [Train]: [002][00010/00051] (21.2%)  Loss=1.1956  cls_loss=0.8170  reg_loss=0.3786  lr_det=4.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=2122s  iter_time=569.369s  fwd=2.140s/bwd=0.087s/opt=0.009s
2025-07-31 16:57:04 Train INFO: [Train]: [002][00020/00051] (40.4%)  Loss=1.1807  cls_loss=0.8088  reg_loss=0.3719  lr_det=4.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1607s  iter_time=519.252s
2025-07-31 17:03:12 Train INFO: [Train]: [002][00030/00051] (59.6%)  Loss=1.1848  cls_loss=0.8134  reg_loss=0.3715  lr_det=5.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=986s  iter_time=367.402s
2025-07-31 17:12:01 Train INFO: [Train]: [002][00040/00051] (78.8%)  Loss=1.1849  cls_loss=0.8137  reg_loss=0.3712  lr_det=5.6e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=533s  iter_time=529.755s
2025-07-31 17:18:24 Train INFO: [Train]: [002][00050/00051] (98.1%)  Loss=1.1827  cls_loss=0.8122  reg_loss=0.3705  lr_det=5.9e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=46s  iter_time=382.576s
2025-07-31 17:18:26 Train INFO: [Train]: [002][00051/00051] (100.0%)  Loss=1.1800  cls_loss=0.8103  reg_loss=0.3697  lr_det=6.0e-05  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=2.218s
2025-07-31 17:18:27 Train INFO: [Train]: Epoch 2 completed in 2371.8s (avg 45.611s/iter)
2025-07-31 17:18:27 Train INFO: [Train]: Final Loss=1.1800
2025-07-31 17:18:27 Train INFO: [Train]: Epoch 3 started (Total iterations: 52)
2025-07-31 17:27:10 Train INFO: [Train]: [003][00010/00051] (21.2%)  Loss=1.1777  cls_loss=0.8053  reg_loss=0.3724  lr_det=6.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1947s  iter_time=522.247s  fwd=3.045s/bwd=0.081s/opt=0.034s
2025-07-31 17:35:12 Train INFO: [Train]: [003][00020/00051] (40.4%)  Loss=1.1466  cls_loss=0.7823  reg_loss=0.3643  lr_det=6.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1483s  iter_time=482.293s
2025-07-31 17:40:51 Train INFO: [Train]: [003][00030/00051] (59.6%)  Loss=1.1486  cls_loss=0.7805  reg_loss=0.3681  lr_det=7.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=910s  iter_time=338.970s
2025-07-31 17:48:46 Train INFO: [Train]: [003][00040/00051] (78.8%)  Loss=1.1493  cls_loss=0.7785  reg_loss=0.3708  lr_det=7.6e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=488s  iter_time=475.462s
2025-07-31 17:54:11 Train INFO: [Train]: [003][00050/00051] (98.1%)  Loss=1.1434  cls_loss=0.7733  reg_loss=0.3701  lr_det=8.0e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=42s  iter_time=324.348s
2025-07-31 17:54:13 Train INFO: [Train]: [003][00051/00051] (100.0%)  Loss=1.1391  cls_loss=0.7704  reg_loss=0.3687  lr_det=8.0e-05  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=2.175s
2025-07-31 17:54:14 Train INFO: [Train]: Epoch 3 completed in 2146.8s (avg 41.285s/iter)
2025-07-31 17:54:14 Train INFO: [Train]: Final Loss=1.1391
2025-07-31 17:54:15 Train INFO: Checkpoint saved at epoch 3
2025-07-31 17:54:15 Train INFO: [Train]: Epoch 4 started (Total iterations: 52)
2025-07-31 18:02:17 Train INFO: [Train]: [004][00010/00051] (21.2%)  Loss=1.1125  cls_loss=0.7463  reg_loss=0.3661  lr_det=8.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1797s  iter_time=482.200s  fwd=2.075s/bwd=0.085s/opt=0.014s
2025-07-31 18:09:43 Train INFO: [Train]: [004][00020/00051] (40.4%)  Loss=1.1053  cls_loss=0.7410  reg_loss=0.3643  lr_det=8.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1369s  iter_time=445.346s
2025-07-31 18:15:03 Train INFO: [Train]: [004][00030/00051] (59.6%)  Loss=1.1126  cls_loss=0.7444  reg_loss=0.3682  lr_det=9.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=845s  iter_time=320.029s
2025-07-31 18:22:19 Train INFO: [Train]: [004][00040/00051] (78.8%)  Loss=1.1139  cls_loss=0.7448  reg_loss=0.3691  lr_det=9.6e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=452s  iter_time=436.078s
2025-07-31 18:27:34 Train INFO: [Train]: [004][00050/00051] (98.1%)  Loss=1.1107  cls_loss=0.7426  reg_loss=0.3681  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=39s  iter_time=315.255s
2025-07-31 18:27:36 Train INFO: [Train]: [004][00051/00051] (100.0%)  Loss=1.1075  cls_loss=0.7405  reg_loss=0.3671  lr_det=1.0e-04  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=1.984s
2025-07-31 18:27:37 Train INFO: [Train]: Epoch 4 completed in 2001.6s (avg 38.493s/iter)
2025-07-31 18:27:37 Train INFO: [Train]: Final Loss=1.1075
2025-07-31 18:27:37 Train INFO: [Train]: Epoch 5 started (Total iterations: 52)
2025-07-31 18:35:58 Train INFO: [Train]: [005][00010/00051] (21.2%)  Loss=1.1026  cls_loss=0.7373  reg_loss=0.3653  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1867s  iter_time=500.930s  fwd=2.059s/bwd=0.059s/opt=0.008s
2025-07-31 18:43:22 Train INFO: [Train]: [005][00020/00051] (40.4%)  Loss=1.1014  cls_loss=0.7361  reg_loss=0.3653  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1395s  iter_time=444.369s
2025-07-31 18:48:34 Train INFO: [Train]: [005][00030/00051] (59.6%)  Loss=1.1042  cls_loss=0.7380  reg_loss=0.3662  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=852s  iter_time=312.198s
2025-07-31 18:56:03 Train INFO: [Train]: [005][00040/00051] (78.8%)  Loss=1.1089  cls_loss=0.7405  reg_loss=0.3684  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=458s  iter_time=448.663s
2025-07-31 19:01:06 Train INFO: [Train]: [005][00050/00051] (98.1%)  Loss=1.1062  cls_loss=0.7386  reg_loss=0.3677  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=39s  iter_time=302.976s
2025-07-31 19:01:08 Train INFO: [Train]: [005][00051/00051] (100.0%)  Loss=1.1049  cls_loss=0.7377  reg_loss=0.3672  lr_det=1.0e-04  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=1.995s
2025-07-31 19:01:09 Train INFO: [Train]: Epoch 5 completed in 2011.9s (avg 38.691s/iter)
2025-07-31 19:01:09 Train INFO: [Train]: Final Loss=1.1049
2025-07-31 19:01:09 Train INFO: Checkpoint saved at epoch 5
2025-07-31 19:01:09 Train INFO: [Train]: Epoch 6 started (Total iterations: 52)
2025-07-31 19:09:15 Train INFO: [Train]: [006][00010/00051] (21.2%)  Loss=1.1180  cls_loss=0.7448  reg_loss=0.3732  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1812s  iter_time=486.192s  fwd=2.050s/bwd=0.083s/opt=0.010s
2025-07-31 19:16:32 Train INFO: [Train]: [006][00020/00051] (40.4%)  Loss=1.1018  cls_loss=0.7350  reg_loss=0.3668  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1362s  iter_time=436.183s
2025-07-31 19:21:59 Train INFO: [Train]: [006][00030/00051] (59.6%)  Loss=1.1050  cls_loss=0.7370  reg_loss=0.3680  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=847s  iter_time=327.353s
2025-07-31 19:29:22 Train INFO: [Train]: [006][00040/00051] (78.8%)  Loss=1.1030  cls_loss=0.7362  reg_loss=0.3668  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=454s  iter_time=442.930s
2025-07-31 19:34:25 Train INFO: [Train]: [006][00050/00051] (98.1%)  Loss=1.1054  cls_loss=0.7375  reg_loss=0.3679  lr_det=1.0e-04  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=39s  iter_time=303.508s
2025-07-31 19:34:27 Train INFO: [Train]: [006][00051/00051] (100.0%)  Loss=1.1017  cls_loss=0.7352  reg_loss=0.3664  lr_det=1.0e-04  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=1.982s
2025-07-31 19:34:28 Train INFO: [Train]: Epoch 6 completed in 1999.0s (avg 38.442s/iter)
2025-07-31 19:34:28 Train INFO: [Train]: Final Loss=1.1017
2025-07-31 19:34:28 Train INFO: [Train]: Epoch 7 started (Total iterations: 52)
2025-08-01 09:22:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-01 09:22:06 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-08-01 09:22:08 Train INFO: training subset: 831 videos
2025-08-01 09:22:08 Train INFO: validation subset: 111 videos
2025-08-01 09:22:08 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-01 09:22:09 Train INFO: Using single GPU training...
2025-08-01 09:22:09 Train INFO: Using Model EMA...
2025-08-01 09:22:09 Train INFO: Using Automatic Mixed Precision...
2025-08-01 09:22:09 Train INFO: GPU Memory: 24.0 GB
2025-08-01 09:22:09 Train INFO: Freeze the backbone...
2025-08-01 09:22:09 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_5.pth
2025-08-01 09:22:10 Train INFO: Resume epoch is 5
2025-08-01 09:22:10 Train INFO: Training Starts...

2025-08-01 09:22:10 Train INFO: [Train]: Epoch 6 started (Total iterations: 52)
2025-08-01 09:29:44 Train INFO: [Train]: [006][00010/00051] (21.2%)  Loss=1.1040  cls_loss=0.7365  reg_loss=0.3675  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1691s  iter_time=453.759s  fwd=2.287s/bwd=0.091s/opt=0.017s
2025-08-01 09:36:54 Train INFO: [Train]: [006][00020/00051] (40.4%)  Loss=1.1009  cls_loss=0.7337  reg_loss=0.3672  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1305s  iter_time=430.346s
2025-08-01 09:42:03 Train INFO: [Train]: [006][00030/00051] (59.6%)  Loss=1.1037  cls_loss=0.7363  reg_loss=0.3674  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=808s  iter_time=308.905s
2025-08-01 09:49:22 Train INFO: [Train]: [006][00040/00051] (78.8%)  Loss=1.1051  cls_loss=0.7380  reg_loss=0.3671  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=438s  iter_time=438.975s
2025-08-01 09:54:51 Train INFO: [Train]: [006][00050/00051] (98.1%)  Loss=1.1057  cls_loss=0.7377  reg_loss=0.3680  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=38s  iter_time=328.684s
2025-08-01 09:54:56 Train INFO: [Train]: [006][00051/00051] (100.0%)  Loss=1.1028  cls_loss=0.7357  reg_loss=0.3671  lr_det=1.0e-04  GPU=1392MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=4.695s
2025-08-01 09:54:56 Train INFO: [Train]: Epoch 6 completed in 1966.2s (avg 37.811s/iter)
2025-08-01 09:54:56 Train INFO: [Train]: Final Loss=1.1028
2025-08-01 09:54:56 Train INFO: [Train]: Epoch 7 started (Total iterations: 52)
2025-08-01 10:03:42 Train INFO: [Train]: [007][00010/00051] (21.2%)  Loss=1.1003  cls_loss=0.7337  reg_loss=0.3666  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1958s  iter_time=525.348s  fwd=2.232s/bwd=0.110s/opt=0.022s
2025-08-01 10:11:31 Train INFO: [Train]: [007][00020/00051] (40.4%)  Loss=1.0963  cls_loss=0.7319  reg_loss=0.3644  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1468s  iter_time=468.857s
2025-08-01 10:16:55 Train INFO: [Train]: [007][00030/00051] (59.6%)  Loss=1.1027  cls_loss=0.7365  reg_loss=0.3662  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=893s  iter_time=324.511s
2025-08-01 10:24:48 Train INFO: [Train]: [007][00040/00051] (78.8%)  Loss=1.1012  cls_loss=0.7351  reg_loss=0.3660  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=481s  iter_time=473.025s
2025-08-01 10:30:08 Train INFO: [Train]: [007][00050/00051] (98.1%)  Loss=1.1015  cls_loss=0.7354  reg_loss=0.3661  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=41s  iter_time=319.402s
2025-08-01 10:30:10 Train INFO: [Train]: [007][00051/00051] (100.0%)  Loss=1.0990  cls_loss=0.7336  reg_loss=0.3654  lr_det=1.0e-04  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.075s
2025-08-01 10:30:10 Train INFO: [Train]: Epoch 7 completed in 2114.0s (avg 40.654s/iter)
2025-08-01 10:30:10 Train INFO: [Train]: Final Loss=1.0990
2025-08-01 10:30:11 Train INFO: Checkpoint saved at epoch 7
2025-08-01 10:30:11 Train INFO: [Train]: Epoch 8 started (Total iterations: 52)
2025-08-01 10:38:33 Train INFO: [Train]: [008][00010/00051] (21.2%)  Loss=1.1058  cls_loss=0.7379  reg_loss=0.3679  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1872s  iter_time=502.266s  fwd=2.199s/bwd=0.079s/opt=0.020s
2025-08-01 10:46:17 Train INFO: [Train]: [008][00020/00051] (40.4%)  Loss=1.0924  cls_loss=0.7284  reg_loss=0.3641  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1426s  iter_time=463.841s
2025-08-01 10:52:01 Train INFO: [Train]: [008][00030/00051] (59.6%)  Loss=1.0991  cls_loss=0.7334  reg_loss=0.3657  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=887s  iter_time=343.737s
2025-08-01 10:59:28 Train INFO: [Train]: [008][00040/00051] (78.8%)  Loss=1.0998  cls_loss=0.7337  reg_loss=0.3662  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=471s  iter_time=446.974s
2025-08-01 11:05:08 Train INFO: [Train]: [008][00050/00051] (98.1%)  Loss=1.0994  cls_loss=0.7335  reg_loss=0.3659  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=41s  iter_time=340.440s
2025-08-01 11:05:10 Train INFO: [Train]: [008][00051/00051] (100.0%)  Loss=1.0968  cls_loss=0.7317  reg_loss=0.3651  lr_det=1.0e-04  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.061s
2025-08-01 11:05:11 Train INFO: [Train]: Epoch 8 completed in 2100.1s (avg 40.387s/iter)
2025-08-01 11:05:11 Train INFO: [Train]: Final Loss=1.0968
2025-08-01 11:05:11 Train INFO: [Train]: Epoch 9 started (Total iterations: 52)
2025-08-01 11:13:42 Train INFO: [Train]: [009][00010/00051] (21.2%)  Loss=1.1087  cls_loss=0.7391  reg_loss=0.3696  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1906s  iter_time=511.338s  fwd=2.181s/bwd=0.076s/opt=0.018s
2025-08-01 11:21:28 Train INFO: [Train]: [009][00020/00051] (40.4%)  Loss=1.0852  cls_loss=0.7235  reg_loss=0.3616  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1442s  iter_time=465.608s
2025-08-01 11:26:51 Train INFO: [Train]: [009][00030/00051] (59.6%)  Loss=1.0945  cls_loss=0.7288  reg_loss=0.3657  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=880s  iter_time=322.811s
2025-08-01 11:34:37 Train INFO: [Train]: [009][00040/00051] (78.8%)  Loss=1.1014  cls_loss=0.7328  reg_loss=0.3686  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=474s  iter_time=466.121s
2025-08-01 11:40:04 Train INFO: [Train]: [009][00050/00051] (98.1%)  Loss=1.1014  cls_loss=0.7331  reg_loss=0.3683  lr_det=1.0e-04  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=41s  iter_time=326.941s
2025-08-01 11:40:06 Train INFO: [Train]: [009][00051/00051] (100.0%)  Loss=1.0975  cls_loss=0.7306  reg_loss=0.3668  lr_det=1.0e-04  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.063s
2025-08-01 11:40:07 Train INFO: [Train]: Epoch 9 completed in 2095.8s (avg 40.304s/iter)
2025-08-01 11:40:07 Train INFO: [Train]: Final Loss=1.0975
2025-08-01 11:40:08 Train INFO: Checkpoint saved at epoch 9
2025-08-01 11:40:08 Train INFO: [Train]: Epoch 10 started (Total iterations: 52)
2025-08-01 11:48:35 Train INFO: [Train]: [010][00010/00051] (21.2%)  Loss=1.0947  cls_loss=0.7303  reg_loss=0.3644  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1890s  iter_time=507.203s  fwd=2.199s/bwd=0.067s/opt=0.017s
2025-08-01 11:56:15 Train INFO: [Train]: [010][00020/00051] (40.4%)  Loss=1.0892  cls_loss=0.7273  reg_loss=0.3619  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1428s  iter_time=460.330s
2025-08-01 12:01:56 Train INFO: [Train]: [010][00030/00051] (59.6%)  Loss=1.0972  cls_loss=0.7318  reg_loss=0.3654  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=886s  iter_time=340.797s
2025-08-01 12:09:42 Train INFO: [Train]: [010][00040/00051] (78.8%)  Loss=1.0990  cls_loss=0.7329  reg_loss=0.3662  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=476s  iter_time=465.838s
2025-08-01 12:15:27 Train INFO: [Train]: [010][00050/00051] (98.1%)  Loss=1.0964  cls_loss=0.7312  reg_loss=0.3652  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=42s  iter_time=344.991s
2025-08-01 12:15:29 Train INFO: [Train]: [010][00051/00051] (100.0%)  Loss=1.0934  cls_loss=0.7292  reg_loss=0.3642  lr_det=9.9e-05  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.070s
2025-08-01 12:15:30 Train INFO: [Train]: Epoch 10 completed in 2122.1s (avg 40.809s/iter)
2025-08-01 12:15:30 Train INFO: [Train]: Final Loss=1.0934
2025-08-01 12:15:30 Train INFO: [Train]: Epoch 11 started (Total iterations: 52)
2025-08-01 12:24:06 Train INFO: [Train]: [011][00010/00051] (21.2%)  Loss=1.0929  cls_loss=0.7280  reg_loss=0.3649  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1926s  iter_time=516.785s  fwd=2.219s/bwd=0.099s/opt=0.022s
2025-08-01 12:32:02 Train INFO: [Train]: [011][00020/00051] (40.4%)  Loss=1.0923  cls_loss=0.7277  reg_loss=0.3646  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1465s  iter_time=475.565s
2025-08-01 12:37:35 Train INFO: [Train]: [011][00030/00051] (59.6%)  Loss=1.0945  cls_loss=0.7288  reg_loss=0.3657  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=898s  iter_time=333.394s
2025-08-01 12:45:38 Train INFO: [Train]: [011][00040/00051] (78.8%)  Loss=1.0995  cls_loss=0.7315  reg_loss=0.3680  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=485s  iter_time=482.226s
2025-08-01 12:51:06 Train INFO: [Train]: [011][00050/00051] (98.1%)  Loss=1.0966  cls_loss=0.7294  reg_loss=0.3671  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=42s  iter_time=328.375s
2025-08-01 12:51:08 Train INFO: [Train]: [011][00051/00051] (100.0%)  Loss=1.0952  cls_loss=0.7285  reg_loss=0.3667  lr_det=9.9e-05  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.066s
2025-08-01 12:51:09 Train INFO: [Train]: Epoch 11 completed in 2139.2s (avg 41.139s/iter)
2025-08-01 12:51:09 Train INFO: [Train]: Final Loss=1.0952
2025-08-01 12:51:10 Train INFO: Checkpoint saved at epoch 11
2025-08-01 12:51:10 Train INFO: [Train]: Epoch 12 started (Total iterations: 52)
2025-08-01 13:00:14 Train INFO: [Train]: [012][00010/00051] (21.2%)  Loss=1.1104  cls_loss=0.7382  reg_loss=0.3722  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2030s  iter_time=544.607s  fwd=2.351s/bwd=0.084s/opt=0.024s
2025-08-01 13:10:14 Train INFO: [Train]: [012][00020/00051] (40.4%)  Loss=1.0941  cls_loss=0.7281  reg_loss=0.3659  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1689s  iter_time=599.610s
2025-08-01 13:16:49 Train INFO: [Train]: [012][00030/00051] (59.6%)  Loss=1.0976  cls_loss=0.7301  reg_loss=0.3675  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1043s  iter_time=395.273s
2025-08-01 13:25:11 Train INFO: [Train]: [012][00040/00051] (78.8%)  Loss=1.0959  cls_loss=0.7295  reg_loss=0.3664  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=548s  iter_time=501.785s
2025-08-01 13:30:38 Train INFO: [Train]: [012][00050/00051] (98.1%)  Loss=1.0981  cls_loss=0.7310  reg_loss=0.3672  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=46s  iter_time=327.472s
2025-08-01 13:30:40 Train INFO: [Train]: [012][00051/00051] (100.0%)  Loss=1.0943  cls_loss=0.7286  reg_loss=0.3657  lr_det=9.9e-05  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.123s
2025-08-01 13:30:42 Train INFO: [Train]: Epoch 12 completed in 2372.2s (avg 45.619s/iter)
2025-08-01 13:30:42 Train INFO: [Train]: Final Loss=1.0943
2025-08-01 13:30:42 Train INFO: [Train]: Epoch 13 started (Total iterations: 52)
2025-08-01 13:39:30 Train INFO: [Train]: [013][00010/00051] (21.2%)  Loss=1.1056  cls_loss=0.7328  reg_loss=0.3728  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1969s  iter_time=528.350s  fwd=3.463s/bwd=0.106s/opt=0.029s
2025-08-01 13:47:34 Train INFO: [Train]: [013][00020/00051] (40.4%)  Loss=1.0921  cls_loss=0.7249  reg_loss=0.3672  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1494s  iter_time=483.437s
2025-08-01 13:53:20 Train INFO: [Train]: [013][00030/00051] (59.6%)  Loss=1.0976  cls_loss=0.7302  reg_loss=0.3673  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=920s  iter_time=346.757s
2025-08-01 14:02:16 Train INFO: [Train]: [013][00040/00051] (78.8%)  Loss=1.0999  cls_loss=0.7317  reg_loss=0.3681  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=508s  iter_time=535.678s
2025-08-01 14:08:51 Train INFO: [Train]: [013][00050/00051] (98.1%)  Loss=1.0986  cls_loss=0.7310  reg_loss=0.3676  lr_det=9.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=45s  iter_time=395.070s
2025-08-01 14:08:53 Train INFO: [Train]: [013][00051/00051] (100.0%)  Loss=1.0966  cls_loss=0.7296  reg_loss=0.3669  lr_det=9.9e-05  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.320s
2025-08-01 14:08:55 Train INFO: [Train]: Epoch 13 completed in 2293.2s (avg 44.100s/iter)
2025-08-01 14:08:55 Train INFO: [Train]: Final Loss=1.0966
2025-08-01 14:08:56 Train INFO: Checkpoint saved at epoch 13
2025-08-01 14:08:56 Train INFO: [Train]: Epoch 14 started (Total iterations: 52)
2025-08-01 14:18:30 Train INFO: [Train]: [014][00010/00051] (21.2%)  Loss=1.1031  cls_loss=0.7355  reg_loss=0.3676  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2140s  iter_time=574.244s  fwd=2.571s/bwd=0.116s/opt=0.027s
2025-08-01 14:26:37 Train INFO: [Train]: [014][00020/00051] (40.4%)  Loss=1.0871  cls_loss=0.7238  reg_loss=0.3634  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1567s  iter_time=487.298s
2025-08-01 14:32:35 Train INFO: [Train]: [014][00030/00051] (59.6%)  Loss=1.0891  cls_loss=0.7261  reg_loss=0.3630  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=961s  iter_time=357.296s
2025-08-01 14:41:07 Train INFO: [Train]: [014][00040/00051] (78.8%)  Loss=1.0961  cls_loss=0.7310  reg_loss=0.3651  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=518s  iter_time=512.004s
2025-08-01 14:47:49 Train INFO: [Train]: [014][00050/00051] (98.1%)  Loss=1.0969  cls_loss=0.7313  reg_loss=0.3656  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=46s  iter_time=402.135s
2025-08-01 14:47:51 Train INFO: [Train]: [014][00051/00051] (100.0%)  Loss=1.0943  cls_loss=0.7296  reg_loss=0.3647  lr_det=9.8e-05  GPU=1392MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.313s
2025-08-01 14:47:53 Train INFO: [Train]: Epoch 14 completed in 2336.7s (avg 44.937s/iter)
2025-08-01 14:47:53 Train INFO: [Train]: Final Loss=1.0943
2025-08-01 15:19:40 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-01 15:19:40 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-08-01 15:19:42 Train INFO: training subset: 831 videos
2025-08-01 15:19:42 Train INFO: validation subset: 111 videos
2025-08-01 15:19:42 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-01 15:19:43 Train INFO: Using single GPU training...
2025-08-01 15:19:43 Train INFO: Using Model EMA...
2025-08-01 15:19:43 Train INFO: Using Automatic Mixed Precision...
2025-08-01 15:19:43 Train INFO: GPU Memory: 24.0 GB
2025-08-01 15:19:43 Train INFO: Freeze the backbone...
2025-08-01 15:19:43 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth
2025-08-01 15:19:44 Train INFO: Resume epoch is 13
2025-08-01 15:19:44 Train INFO: Training Starts...

2025-08-01 15:19:44 Train INFO: [Train]: Epoch 14 started (Total iterations: 52)
2025-08-01 15:28:25 Train INFO: [Train]: [014][00010/00051] (21.2%)  Loss=1.0902  cls_loss=0.7252  reg_loss=0.3650  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1939s  iter_time=520.242s  fwd=2.304s/bwd=0.085s/opt=0.024s
2025-08-01 15:36:31 Train INFO: [Train]: [014][00020/00051] (40.4%)  Loss=1.0893  cls_loss=0.7241  reg_loss=0.3652  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1486s  iter_time=486.591s
2025-08-01 15:42:12 Train INFO: [Train]: [014][00030/00051] (59.6%)  Loss=1.0934  cls_loss=0.7277  reg_loss=0.3657  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=913s  iter_time=340.338s
2025-08-01 15:50:27 Train INFO: [Train]: [014][00040/00051] (78.8%)  Loss=1.0953  cls_loss=0.7295  reg_loss=0.3659  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=494s  iter_time=495.111s
2025-08-01 15:55:58 Train INFO: [Train]: [014][00050/00051] (98.1%)  Loss=1.0959  cls_loss=0.7290  reg_loss=0.3669  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=43s  iter_time=331.069s
2025-08-01 15:56:02 Train INFO: [Train]: [014][00051/00051] (100.0%)  Loss=1.0930  cls_loss=0.7270  reg_loss=0.3660  lr_det=9.8e-05  GPU=1392MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=4.799s
2025-08-01 15:56:03 Train INFO: [Train]: Epoch 14 completed in 2179.2s (avg 41.907s/iter)
2025-08-01 15:56:03 Train INFO: [Train]: Final Loss=1.0930
2025-08-01 15:56:04 Train INFO: [Val]: Epoch 14 Loss
2025-08-01 16:35:06 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-01 16:35:07 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-08-01 16:35:07 Train INFO: training subset: 831 videos
2025-08-01 16:35:07 Train INFO: validation subset: 111 videos
2025-08-01 16:35:07 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-01 16:35:08 Train INFO: Using single GPU training...
2025-08-01 16:35:08 Train INFO: Using Model EMA...
2025-08-01 16:35:08 Train INFO: Using Automatic Mixed Precision...
2025-08-01 16:35:08 Train INFO: GPU Memory: 24.0 GB
2025-08-01 16:35:08 Train INFO: Freeze the backbone...
2025-08-01 16:35:08 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth
2025-08-01 16:35:09 Train INFO: Resume epoch is 13
2025-08-01 16:35:09 Train INFO: Training Starts...

2025-08-01 16:35:09 Train INFO: [Train]: Epoch 14 started (Total iterations: 52)
2025-08-01 16:44:42 Train INFO: [Train]: [014][00010/00051] (21.2%)  Loss=1.0902  cls_loss=0.7252  reg_loss=0.3650  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2134s  iter_time=572.576s  fwd=2.211s/bwd=0.089s/opt=0.022s
2025-08-01 16:53:31 Train INFO: [Train]: [014][00020/00051] (40.4%)  Loss=1.0892  cls_loss=0.7241  reg_loss=0.3652  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1626s  iter_time=528.963s
2025-08-01 16:59:53 Train INFO: [Train]: [014][00030/00051] (59.6%)  Loss=1.0934  cls_loss=0.7277  reg_loss=0.3657  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1005s  iter_time=382.623s
2025-08-01 17:08:45 Train INFO: [Train]: [014][00040/00051] (78.8%)  Loss=1.0953  cls_loss=0.7295  reg_loss=0.3658  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=541s  iter_time=531.078s
2025-08-01 17:14:25 Train INFO: [Train]: [014][00050/00051] (98.1%)  Loss=1.0959  cls_loss=0.7290  reg_loss=0.3669  lr_det=9.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=46s  iter_time=340.840s
2025-08-01 17:14:35 Train INFO: [Train]: [014][00051/00051] (100.0%)  Loss=1.0930  cls_loss=0.7270  reg_loss=0.3660  lr_det=9.8e-05  GPU=1392MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=9.128s
2025-08-01 17:14:36 Train INFO: [Train]: Epoch 14 completed in 2366.5s (avg 45.509s/iter)
2025-08-01 17:14:36 Train INFO: [Train]: Final Loss=1.0930
2025-08-01 17:14:36 Train INFO: [Val]: Epoch 14 Loss
2025-08-04 16:27:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-04 16:27:55 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-08-04 16:27:56 Train INFO: training subset: 831 videos
2025-08-04 16:31:12 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-04 16:31:12 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-08-04 16:31:13 Train INFO: training subset: 831 videos
2025-08-04 16:31:13 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-04 16:31:13 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-04 16:31:13 Train INFO: Using single GPU training...
2025-08-04 16:31:13 Train INFO: Using Model EMA...
2025-08-04 16:31:14 Train INFO: Using Automatic Mixed Precision...
2025-08-04 16:31:14 Train INFO: GPU Memory: 24.0 GB
2025-08-04 16:31:14 Train INFO: Freeze the backbone...
2025-08-04 16:31:14 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth
2025-08-04 16:31:15 Train INFO: Resume epoch is 13
2025-08-04 16:31:15 Train INFO: Training Starts...

2025-08-04 16:31:15 Train INFO: [Train]: Epoch 14 started (Total iterations: 52)
2025-08-04 16:39:44 Train INFO: [Train]: [014][00010/00051] (21.2%)  Loss=0.5118  cls_loss=0.3661  reg_loss=0.1457  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1896s  iter_time=508.555s  fwd=2.196s/bwd=0.090s/opt=0.024s
2025-08-04 16:47:20 Train INFO: [Train]: [014][00020/00051] (40.4%)  Loss=0.6164  cls_loss=0.4269  reg_loss=0.1894  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1425s  iter_time=456.471s
2025-08-04 16:52:41 Train INFO: [Train]: [014][00030/00051] (59.6%)  Loss=0.7129  cls_loss=0.4876  reg_loss=0.2253  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=871s  iter_time=321.237s
2025-08-04 17:00:29 Train INFO: [Train]: [014][00040/00051] (78.8%)  Loss=0.7773  cls_loss=0.5265  reg_loss=0.2508  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=471s  iter_time=467.810s
2025-08-04 17:06:07 Train INFO: [Train]: [014][00050/00051] (98.1%)  Loss=0.8147  cls_loss=0.5503  reg_loss=0.2643  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=41s  iter_time=338.286s
2025-08-04 17:06:12 Train INFO: [Train]: [014][00051/00051] (100.0%)  Loss=0.8199  cls_loss=0.5535  reg_loss=0.2664  lr_det=9.8e-05  GPU=1392MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=4.470s
2025-08-04 17:06:13 Train INFO: [Train]: Epoch 14 completed in 2097.6s (avg 40.338s/iter)
2025-08-04 17:06:13 Train INFO: [Train]: Final Loss=0.8199
2025-08-04 17:06:13 Train INFO: [Val]: Epoch 14 Loss
2025-08-04 17:24:34 Train INFO: [Val]: [014]  Loss=1.2498  cls_loss=0.8817  reg_loss=0.3682
2025-08-04 17:24:34 Train INFO: [Train]: Epoch 15 started (Total iterations: 52)
2025-08-04 17:32:04 Train INFO: [Train]: [015][00010/00051] (21.2%)  Loss=1.0726  cls_loss=0.7137  reg_loss=0.3589  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1678s  iter_time=450.246s  fwd=2.275s/bwd=0.083s/opt=0.026s
2025-08-04 17:39:11 Train INFO: [Train]: [015][00020/00051] (40.4%)  Loss=1.0579  cls_loss=0.7048  reg_loss=0.3531  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1295s  iter_time=427.018s
2025-08-04 17:44:18 Train INFO: [Train]: [015][00030/00051] (59.6%)  Loss=1.0636  cls_loss=0.7087  reg_loss=0.3550  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=802s  iter_time=307.321s
2025-08-04 17:51:41 Train INFO: [Train]: [015][00040/00051] (78.8%)  Loss=1.0652  cls_loss=0.7079  reg_loss=0.3573  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=437s  iter_time=442.969s
2025-08-04 17:56:40 Train INFO: [Train]: [015][00050/00051] (98.1%)  Loss=1.0540  cls_loss=0.7014  reg_loss=0.3525  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=38s  iter_time=299.035s
2025-08-04 17:56:42 Train INFO: [Train]: [015][00051/00051] (100.0%)  Loss=1.0522  cls_loss=0.6999  reg_loss=0.3523  lr_det=9.8e-05  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.072s
2025-08-04 17:56:43 Train INFO: [Train]: Epoch 15 completed in 1929.4s (avg 37.103s/iter)
2025-08-04 17:56:43 Train INFO: [Train]: Final Loss=1.0522
2025-08-04 17:56:44 Train INFO: Checkpoint saved at epoch 15
2025-08-04 17:56:44 Train INFO: [Train]: Epoch 16 started (Total iterations: 52)
2025-08-04 18:02:27 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-04 18:02:28 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-04 18:02:28 Train INFO: training subset: 831 videos
2025-08-04 18:02:28 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-04 18:02:28 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-04 18:02:29 Train INFO: Using single GPU training...
2025-08-04 18:02:29 Train INFO: Using Model EMA...
2025-08-04 18:02:29 Train INFO: Using Automatic Mixed Precision...
2025-08-04 18:02:29 Train INFO: GPU Memory: 24.0 GB
2025-08-04 18:02:29 Train INFO: Freeze the backbone...
2025-08-04 18:02:29 Train INFO: Training Starts...

2025-08-04 18:02:29 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-08-04 18:05:08 Train INFO: [Train]: [000][00001/00051] (3.8%)  Loss=1.3287  cls_loss=0.7139  reg_loss=0.6149  lr_det=2.0e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=3980s  iter_time=159.187s  fwd=2.306s/bwd=0.077s/opt=0.007s
2025-08-04 18:05:10 Train INFO: [Train]: [000][00002/00051] (5.8%)  Loss=1.5177  cls_loss=0.8166  reg_loss=0.7011  lr_det=3.9e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2639s  iter_time=2.354s
2025-08-04 18:05:13 Train INFO: [Train]: [000][00003/00051] (7.7%)  Loss=1.5489  cls_loss=0.8393  reg_loss=0.7096  lr_det=5.9e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1966s  iter_time=2.305s
2025-08-04 18:07:19 Train INFO: [Train]: [000][00004/00051] (9.6%)  Loss=1.4949  cls_loss=0.8135  reg_loss=0.6814  lr_det=7.8e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2730s  iter_time=126.594s
2025-08-04 18:07:25 Train INFO: [Train]: [000][00005/00051] (11.5%)  Loss=1.5739  cls_loss=0.8670  reg_loss=0.7069  lr_det=9.8e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2270s  iter_time=5.698s
2025-08-04 18:07:27 Train INFO: [Train]: [000][00006/00051] (13.5%)  Loss=1.6084  cls_loss=0.9031  reg_loss=0.7053  lr_det=1.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1919s  iter_time=2.320s
2025-08-04 18:07:30 Train INFO: [Train]: [000][00007/00051] (15.4%)  Loss=1.5991  cls_loss=0.9142  reg_loss=0.6850  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1654s  iter_time=2.311s
2025-08-04 18:09:41 Train INFO: [Train]: [000][00008/00051] (17.3%)  Loss=1.5677  cls_loss=0.9089  reg_loss=0.6588  lr_det=1.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2064s  iter_time=131.214s
2025-08-04 18:09:52 Train INFO: [Train]: [000][00009/00051] (19.2%)  Loss=1.5468  cls_loss=0.9132  reg_loss=0.6336  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1863s  iter_time=11.589s
2025-08-04 18:09:55 Train INFO: [Train]: [000][00010/00051] (21.2%)  Loss=1.5385  cls_loss=0.9270  reg_loss=0.6115  lr_det=2.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1662s  iter_time=2.326s
2025-08-04 18:09:57 Train INFO: [Train]: [000][00011/00051] (23.1%)  Loss=1.5454  cls_loss=0.9481  reg_loss=0.5973  lr_det=2.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1494s  iter_time=2.262s
2025-08-04 18:12:07 Train INFO: [Train]: [000][00012/00051] (25.0%)  Loss=1.5262  cls_loss=0.9469  reg_loss=0.5793  lr_det=2.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1734s  iter_time=129.791s
2025-08-04 18:12:17 Train INFO: [Train]: [000][00013/00051] (26.9%)  Loss=1.5226  cls_loss=0.9561  reg_loss=0.5665  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1596s  iter_time=9.990s
2025-08-04 18:12:19 Train INFO: [Train]: [000][00014/00051] (28.8%)  Loss=1.5220  cls_loss=0.9634  reg_loss=0.5586  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1456s  iter_time=2.300s
2025-08-04 18:12:22 Train INFO: [Train]: [000][00015/00051] (30.8%)  Loss=1.5001  cls_loss=0.9552  reg_loss=0.5449  lr_det=2.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1334s  iter_time=2.575s
2025-08-04 18:14:28 Train INFO: [Train]: [000][00016/00051] (32.7%)  Loss=1.4939  cls_loss=0.9572  reg_loss=0.5367  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1481s  iter_time=126.477s
2025-08-04 18:14:30 Train INFO: [Train]: [000][00017/00051] (34.6%)  Loss=1.4794  cls_loss=0.9524  reg_loss=0.5270  lr_det=3.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1363s  iter_time=2.370s
2025-08-04 18:14:33 Train INFO: [Train]: [000][00018/00051] (36.5%)  Loss=1.4699  cls_loss=0.9526  reg_loss=0.5173  lr_det=3.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1257s  iter_time=2.284s
2025-08-04 18:14:35 Train INFO: [Train]: [000][00019/00051] (38.5%)  Loss=1.4723  cls_loss=0.9595  reg_loss=0.5127  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1162s  iter_time=2.235s
2025-08-04 18:16:44 Train INFO: [Train]: [000][00020/00051] (40.4%)  Loss=1.4579  cls_loss=0.9541  reg_loss=0.5038  lr_det=3.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1262s  iter_time=128.820s
2025-08-04 18:16:46 Train INFO: [Train]: [000][00021/00051] (42.3%)  Loss=1.4498  cls_loss=0.9531  reg_loss=0.4967  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1169s  iter_time=2.320s
2025-08-04 18:16:48 Train INFO: [Train]: [000][00022/00051] (44.2%)  Loss=1.4577  cls_loss=0.9625  reg_loss=0.4952  lr_det=4.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1084s  iter_time=2.368s
2025-08-04 18:16:51 Train INFO: [Train]: [000][00023/00051] (46.2%)  Loss=1.4402  cls_loss=0.9538  reg_loss=0.4864  lr_det=4.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1006s  iter_time=2.268s
2025-08-04 18:19:07 Train INFO: [Train]: [000][00024/00051] (48.1%)  Loss=1.4333  cls_loss=0.9521  reg_loss=0.4812  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1078s  iter_time=136.623s
2025-08-04 18:19:10 Train INFO: [Train]: [000][00025/00051] (50.0%)  Loss=1.4249  cls_loss=0.9492  reg_loss=0.4757  lr_det=4.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1001s  iter_time=2.306s
2025-08-04 18:19:12 Train INFO: [Train]: [000][00026/00051] (51.9%)  Loss=1.4274  cls_loss=0.9536  reg_loss=0.4738  lr_det=5.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=929s  iter_time=2.326s
2025-08-04 18:19:14 Train INFO: [Train]: [000][00027/00051] (53.8%)  Loss=1.4260  cls_loss=0.9554  reg_loss=0.4706  lr_det=5.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=862s  iter_time=2.236s
2025-08-04 18:21:28 Train INFO: [Train]: [000][00028/00051] (55.8%)  Loss=1.4199  cls_loss=0.9534  reg_loss=0.4665  lr_det=5.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=904s  iter_time=133.863s
2025-08-04 18:21:35 Train INFO: [Train]: [000][00029/00051] (57.7%)  Loss=1.4153  cls_loss=0.9519  reg_loss=0.4633  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=840s  iter_time=6.743s
2025-08-04 18:21:37 Train INFO: [Train]: [000][00030/00051] (59.6%)  Loss=1.4069  cls_loss=0.9479  reg_loss=0.4590  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=778s  iter_time=2.358s
2025-08-04 18:21:40 Train INFO: [Train]: [000][00031/00051] (61.5%)  Loss=1.3981  cls_loss=0.9430  reg_loss=0.4551  lr_det=6.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=719s  iter_time=2.340s
2025-08-04 18:23:49 Train INFO: [Train]: [000][00032/00051] (63.5%)  Loss=1.4019  cls_loss=0.9477  reg_loss=0.4542  lr_det=6.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=737s  iter_time=129.696s
2025-08-04 18:23:52 Train INFO: [Train]: [000][00033/00051] (65.4%)  Loss=1.3942  cls_loss=0.9434  reg_loss=0.4508  lr_det=6.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=3.210s
2025-08-04 18:23:55 Train INFO: [Train]: [000][00034/00051] (67.3%)  Loss=1.3973  cls_loss=0.9470  reg_loss=0.4504  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=625s  iter_time=2.322s
2025-08-04 18:23:57 Train INFO: [Train]: [000][00035/00051] (69.2%)  Loss=1.3902  cls_loss=0.9431  reg_loss=0.4471  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=573s  iter_time=2.335s
2025-08-04 18:26:15 Train INFO: [Train]: [000][00036/00051] (71.2%)  Loss=1.3949  cls_loss=0.9470  reg_loss=0.4479  lr_det=7.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=578s  iter_time=137.613s
2025-08-04 18:26:18 Train INFO: [Train]: [000][00037/00051] (73.1%)  Loss=1.3895  cls_loss=0.9443  reg_loss=0.4452  lr_det=7.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=527s  iter_time=3.512s
2025-08-04 18:26:21 Train INFO: [Train]: [000][00038/00051] (75.0%)  Loss=1.3849  cls_loss=0.9424  reg_loss=0.4424  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=477s  iter_time=2.360s
2025-08-04 18:26:23 Train INFO: [Train]: [000][00039/00051] (76.9%)  Loss=1.3774  cls_loss=0.9379  reg_loss=0.4395  lr_det=7.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=430s  iter_time=2.412s
2025-08-04 18:28:43 Train INFO: [Train]: [000][00040/00051] (78.8%)  Loss=1.3778  cls_loss=0.9385  reg_loss=0.4392  lr_det=7.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=422s  iter_time=139.499s
2025-08-04 18:28:51 Train INFO: [Train]: [000][00041/00051] (80.8%)  Loss=1.3756  cls_loss=0.9377  reg_loss=0.4379  lr_det=8.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=377s  iter_time=8.652s
2025-08-04 18:28:54 Train INFO: [Train]: [000][00042/00051] (82.7%)  Loss=1.3771  cls_loss=0.9392  reg_loss=0.4378  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=332s  iter_time=2.395s
2025-08-04 18:28:56 Train INFO: [Train]: [000][00043/00051] (84.6%)  Loss=1.3719  cls_loss=0.9365  reg_loss=0.4354  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=289s  iter_time=2.312s
2025-08-04 18:31:07 Train INFO: [Train]: [000][00044/00051] (86.5%)  Loss=1.3677  cls_loss=0.9350  reg_loss=0.4328  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=267s  iter_time=131.476s
2025-08-04 18:31:12 Train INFO: [Train]: [000][00045/00051] (88.5%)  Loss=1.3616  cls_loss=0.9316  reg_loss=0.4300  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=225s  iter_time=4.660s
2025-08-04 18:31:14 Train INFO: [Train]: [000][00046/00051] (90.4%)  Loss=1.3580  cls_loss=0.9299  reg_loss=0.4282  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=184s  iter_time=2.320s
2025-08-04 18:31:17 Train INFO: [Train]: [000][00047/00051] (92.3%)  Loss=1.3549  cls_loss=0.9287  reg_loss=0.4262  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=144s  iter_time=2.338s
2025-08-04 18:33:30 Train INFO: [Train]: [000][00048/00051] (94.2%)  Loss=1.3515  cls_loss=0.9275  reg_loss=0.4240  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=114s  iter_time=133.259s
2025-08-04 18:33:35 Train INFO: [Train]: [000][00049/00051] (96.2%)  Loss=1.3463  cls_loss=0.9250  reg_loss=0.4214  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=75s  iter_time=4.712s
2025-08-04 18:33:37 Train INFO: [Train]: [000][00050/00051] (98.1%)  Loss=1.3477  cls_loss=0.9267  reg_loss=0.4210  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=37s  iter_time=2.236s
2025-08-04 18:33:41 Train INFO: [Train]: [000][00051/00051] (100.0%)  Loss=1.3470  cls_loss=0.9268  reg_loss=0.4202  lr_det=1.0e-04  GPU=1391MB(alloc)/4208MB(reserved)/9063MB(max)  ETA=0s  iter_time=4.530s
2025-08-04 18:33:42 Train INFO: [Train]: Epoch 0 completed in 1873.4s (avg 36.027s/iter)
2025-08-04 18:33:42 Train INFO: [Train]: Final Loss=1.3470
2025-08-04 18:33:43 Train INFO: Checkpoint saved at epoch 0
2025-08-04 18:33:43 Train INFO: [Train]: Epoch 1 started (Total iterations: 52)
2025-08-04 18:36:22 Train INFO: [Train]: [001][00001/00051] (3.8%)  Loss=1.3034  cls_loss=0.9351  reg_loss=0.3683  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=3992s  iter_time=159.680s  fwd=2.148s/bwd=0.076s/opt=0.011s
2025-08-04 18:36:25 Train INFO: [Train]: [001][00002/00051] (5.8%)  Loss=1.4200  cls_loss=1.0066  reg_loss=0.4134  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2646s  iter_time=2.338s
2025-08-04 18:36:27 Train INFO: [Train]: [001][00003/00051] (7.7%)  Loss=1.4080  cls_loss=0.9984  reg_loss=0.4096  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1971s  iter_time=2.243s
2025-08-04 18:38:44 Train INFO: [Train]: [001][00004/00051] (9.6%)  Loss=1.3817  cls_loss=0.9751  reg_loss=0.4066  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2835s  iter_time=137.368s
2025-08-04 18:38:47 Train INFO: [Train]: [001][00005/00051] (11.5%)  Loss=1.3718  cls_loss=0.9681  reg_loss=0.4037  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2331s  iter_time=2.432s
2025-08-04 18:38:49 Train INFO: [Train]: [001][00006/00051] (13.5%)  Loss=1.3762  cls_loss=0.9719  reg_loss=0.4043  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1970s  iter_time=2.315s
2025-08-04 18:38:51 Train INFO: [Train]: [001][00007/00051] (15.4%)  Loss=1.4029  cls_loss=0.9904  reg_loss=0.4125  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1698s  iter_time=2.304s
2025-08-04 18:41:03 Train INFO: [Train]: [001][00008/00051] (17.3%)  Loss=1.3808  cls_loss=0.9743  reg_loss=0.4066  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=2103s  iter_time=131.470s
2025-08-04 18:41:13 Train INFO: [Train]: [001][00009/00051] (19.2%)  Loss=1.3682  cls_loss=0.9664  reg_loss=0.4017  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1890s  iter_time=9.948s
2025-08-04 18:41:15 Train INFO: [Train]: [001][00010/00051] (21.2%)  Loss=1.3661  cls_loss=0.9658  reg_loss=0.4003  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1686s  iter_time=2.343s
2025-08-04 18:41:18 Train INFO: [Train]: [001][00011/00051] (23.1%)  Loss=1.3672  cls_loss=0.9673  reg_loss=0.4000  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1516s  iter_time=2.359s
2025-08-04 18:43:35 Train INFO: [Train]: [001][00012/00051] (25.0%)  Loss=1.3563  cls_loss=0.9604  reg_loss=0.3958  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1778s  iter_time=137.779s
2025-08-04 18:43:44 Train INFO: [Train]: [001][00013/00051] (26.9%)  Loss=1.3535  cls_loss=0.9606  reg_loss=0.3929  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1631s  iter_time=8.166s
2025-08-04 18:43:46 Train INFO: [Train]: [001][00014/00051] (28.8%)  Loss=1.3685  cls_loss=0.9706  reg_loss=0.3979  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1488s  iter_time=2.382s
2025-08-04 18:43:48 Train INFO: [Train]: [001][00015/00051] (30.8%)  Loss=1.3603  cls_loss=0.9655  reg_loss=0.3948  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1362s  iter_time=2.307s
2025-08-04 18:45:58 Train INFO: [Train]: [001][00016/00051] (32.7%)  Loss=1.3540  cls_loss=0.9620  reg_loss=0.3921  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1514s  iter_time=130.157s
2025-08-04 18:46:04 Train INFO: [Train]: [001][00017/00051] (34.6%)  Loss=1.3443  cls_loss=0.9550  reg_loss=0.3893  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1401s  iter_time=5.885s
2025-08-04 18:46:07 Train INFO: [Train]: [001][00018/00051] (36.5%)  Loss=1.3382  cls_loss=0.9518  reg_loss=0.3865  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1292s  iter_time=2.345s
2025-08-04 18:46:09 Train INFO: [Train]: [001][00019/00051] (38.5%)  Loss=1.3354  cls_loss=0.9499  reg_loss=0.3855  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1194s  iter_time=2.225s
2025-08-04 18:48:20 Train INFO: [Train]: [001][00020/00051] (40.4%)  Loss=1.3180  cls_loss=0.9375  reg_loss=0.3805  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1295s  iter_time=131.423s
2025-08-04 18:48:25 Train INFO: [Train]: [001][00021/00051] (42.3%)  Loss=1.3208  cls_loss=0.9394  reg_loss=0.3814  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1203s  iter_time=4.691s
2025-08-04 18:48:28 Train INFO: [Train]: [001][00022/00051] (44.2%)  Loss=1.3254  cls_loss=0.9431  reg_loss=0.3823  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1117s  iter_time=3.331s
2025-08-04 18:48:31 Train INFO: [Train]: [001][00023/00051] (46.2%)  Loss=1.3082  cls_loss=0.9312  reg_loss=0.3770  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1036s  iter_time=2.314s
2025-08-04 18:50:47 Train INFO: [Train]: [001][00024/00051] (48.1%)  Loss=1.3075  cls_loss=0.9314  reg_loss=0.3760  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1106s  iter_time=135.878s
2025-08-04 18:50:49 Train INFO: [Train]: [001][00025/00051] (50.0%)  Loss=1.3057  cls_loss=0.9298  reg_loss=0.3759  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1026s  iter_time=2.344s
2025-08-04 18:50:55 Train INFO: [Train]: [001][00026/00051] (51.9%)  Loss=1.3075  cls_loss=0.9315  reg_loss=0.3759  lr_det=8.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=955s  iter_time=5.686s
2025-08-04 18:50:57 Train INFO: [Train]: [001][00027/00051] (53.8%)  Loss=1.3127  cls_loss=0.9357  reg_loss=0.3769  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=886s  iter_time=2.304s
2025-08-04 18:53:11 Train INFO: [Train]: [001][00028/00051] (55.8%)  Loss=1.3047  cls_loss=0.9304  reg_loss=0.3743  lr_det=8.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=927s  iter_time=134.473s
2025-08-04 18:53:21 Train INFO: [Train]: [001][00029/00051] (57.7%)  Loss=1.3098  cls_loss=0.9341  reg_loss=0.3757  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=864s  iter_time=9.194s
2025-08-04 18:53:24 Train INFO: [Train]: [001][00030/00051] (59.6%)  Loss=1.3134  cls_loss=0.9372  reg_loss=0.3762  lr_det=8.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=800s  iter_time=3.728s
2025-08-04 18:53:27 Train INFO: [Train]: [001][00031/00051] (61.5%)  Loss=1.3078  cls_loss=0.9337  reg_loss=0.3741  lr_det=8.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=740s  iter_time=2.383s
2025-08-04 18:55:56 Train INFO: [Train]: [001][00032/00051] (63.5%)  Loss=1.3057  cls_loss=0.9324  reg_loss=0.3733  lr_det=7.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=767s  iter_time=149.053s
2025-08-04 18:56:02 Train INFO: [Train]: [001][00033/00051] (65.4%)  Loss=1.2981  cls_loss=0.9274  reg_loss=0.3706  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=709s  iter_time=6.105s
2025-08-04 18:56:08 Train INFO: [Train]: [001][00034/00051] (67.3%)  Loss=1.3069  cls_loss=0.9335  reg_loss=0.3734  lr_det=7.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=654s  iter_time=6.650s
2025-08-04 18:56:11 Train INFO: [Train]: [001][00035/00051] (69.2%)  Loss=1.3018  cls_loss=0.9304  reg_loss=0.3714  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=599s  iter_time=2.385s
2025-08-04 18:58:42 Train INFO: [Train]: [001][00036/00051] (71.2%)  Loss=1.3027  cls_loss=0.9307  reg_loss=0.3720  lr_det=7.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=608s  iter_time=151.187s
2025-08-04 18:58:46 Train INFO: [Train]: [001][00037/00051] (73.1%)  Loss=1.3044  cls_loss=0.9315  reg_loss=0.3729  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=554s  iter_time=3.613s
2025-08-04 18:58:52 Train INFO: [Train]: [001][00038/00051] (75.0%)  Loss=1.3045  cls_loss=0.9313  reg_loss=0.3732  lr_det=7.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=503s  iter_time=6.118s
2025-08-04 18:58:54 Train INFO: [Train]: [001][00039/00051] (76.9%)  Loss=1.3056  cls_loss=0.9322  reg_loss=0.3735  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=453s  iter_time=2.387s
2025-08-04 19:01:23 Train INFO: [Train]: [001][00040/00051] (78.8%)  Loss=1.3043  cls_loss=0.9317  reg_loss=0.3726  lr_det=6.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=445s  iter_time=149.068s
2025-08-04 19:01:31 Train INFO: [Train]: [001][00041/00051] (80.8%)  Loss=1.2999  cls_loss=0.9286  reg_loss=0.3713  lr_det=6.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=397s  iter_time=7.919s
2025-08-04 19:01:35 Train INFO: [Train]: [001][00042/00051] (82.7%)  Loss=1.2984  cls_loss=0.9276  reg_loss=0.3709  lr_det=6.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=350s  iter_time=3.455s
2025-08-04 19:01:37 Train INFO: [Train]: [001][00043/00051] (84.6%)  Loss=1.2971  cls_loss=0.9265  reg_loss=0.3706  lr_det=6.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=304s  iter_time=2.369s
2025-08-04 19:04:10 Train INFO: [Train]: [001][00044/00051] (86.5%)  Loss=1.2977  cls_loss=0.9262  reg_loss=0.3715  lr_det=6.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=284s  iter_time=152.568s
2025-08-04 19:04:15 Train INFO: [Train]: [001][00045/00051] (88.5%)  Loss=1.2923  cls_loss=0.9222  reg_loss=0.3702  lr_det=6.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=239s  iter_time=5.138s
2025-08-04 19:04:23 Train INFO: [Train]: [001][00046/00051] (90.4%)  Loss=1.2879  cls_loss=0.9189  reg_loss=0.3691  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=196s  iter_time=8.806s
2025-08-04 19:04:26 Train INFO: [Train]: [001][00047/00051] (92.3%)  Loss=1.2836  cls_loss=0.9156  reg_loss=0.3680  lr_det=5.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=154s  iter_time=2.410s
2025-08-04 19:06:54 Train INFO: [Train]: [001][00048/00051] (94.2%)  Loss=1.2784  cls_loss=0.9119  reg_loss=0.3665  lr_det=5.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=122s  iter_time=148.513s
2025-08-04 19:06:59 Train INFO: [Train]: [001][00049/00051] (96.2%)  Loss=1.2776  cls_loss=0.9116  reg_loss=0.3660  lr_det=5.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=80s  iter_time=4.640s
2025-08-04 19:07:02 Train INFO: [Train]: [001][00050/00051] (98.1%)  Loss=1.2777  cls_loss=0.9115  reg_loss=0.3662  lr_det=5.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=39s  iter_time=2.588s
2025-08-04 19:07:04 Train INFO: [Train]: [001][00051/00051] (100.0%)  Loss=1.2749  cls_loss=0.9096  reg_loss=0.3654  lr_det=5.2e-05  GPU=1391MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=2.229s
2025-08-04 19:07:05 Train INFO: [Train]: Epoch 1 completed in 2002.3s (avg 38.505s/iter)
2025-08-04 19:07:05 Train INFO: [Train]: Final Loss=1.2749
2025-08-04 19:07:05 Train INFO: [Val]: Epoch 1 Loss
2025-08-04 19:26:34 Train INFO: [Val]: [001]  Loss=1.8056  cls_loss=1.1294  reg_loss=0.6762
2025-08-04 19:26:35 Train INFO: Checkpoint saved at epoch 1
2025-08-04 19:26:35 Train INFO: [Train]: Epoch 2 started (Total iterations: 52)
2025-08-04 19:29:26 Train INFO: [Train]: [002][00001/00051] (3.8%)  Loss=1.1593  cls_loss=0.8154  reg_loss=0.3439  lr_det=4.8e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=4271s  iter_time=170.844s  fwd=2.274s/bwd=0.051s/opt=0.009s
2025-08-04 19:29:29 Train INFO: [Train]: [002][00002/00051] (5.8%)  Loss=1.2325  cls_loss=0.8677  reg_loss=0.3647  lr_det=4.7e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=2832s  iter_time=2.529s
2025-08-04 19:29:31 Train INFO: [Train]: [002][00003/00051] (7.7%)  Loss=1.2262  cls_loss=0.8626  reg_loss=0.3636  lr_det=4.5e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=2109s  iter_time=2.378s
2025-08-04 19:31:44 Train INFO: [Train]: [002][00004/00051] (9.6%)  Loss=1.2049  cls_loss=0.8483  reg_loss=0.3566  lr_det=4.4e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=2903s  iter_time=133.074s
2025-08-04 19:31:52 Train INFO: [Train]: [002][00005/00051] (11.5%)  Loss=1.2476  cls_loss=0.8749  reg_loss=0.3726  lr_det=4.2e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=2428s  iter_time=7.875s
2025-08-04 19:31:54 Train INFO: [Train]: [002][00006/00051] (13.5%)  Loss=1.2352  cls_loss=0.8649  reg_loss=0.3703  lr_det=4.1e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=2051s  iter_time=2.368s
2025-08-04 19:31:57 Train INFO: [Train]: [002][00007/00051] (15.4%)  Loss=1.2357  cls_loss=0.8660  reg_loss=0.3696  lr_det=4.0e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1768s  iter_time=2.375s
2025-08-04 19:34:03 Train INFO: [Train]: [002][00008/00051] (17.3%)  Loss=1.2298  cls_loss=0.8571  reg_loss=0.3726  lr_det=3.8e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=2139s  iter_time=126.333s
2025-08-04 19:34:16 Train INFO: [Train]: [002][00009/00051] (19.2%)  Loss=1.2097  cls_loss=0.8460  reg_loss=0.3638  lr_det=3.7e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1937s  iter_time=13.354s
2025-08-04 19:34:19 Train INFO: [Train]: [002][00010/00051] (21.2%)  Loss=1.2129  cls_loss=0.8458  reg_loss=0.3671  lr_det=3.5e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1728s  iter_time=2.373s
2025-08-04 19:34:21 Train INFO: [Train]: [002][00011/00051] (23.1%)  Loss=1.2341  cls_loss=0.8583  reg_loss=0.3758  lr_det=3.4e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1553s  iter_time=2.375s
2025-08-04 19:36:28 Train INFO: [Train]: [002][00012/00051] (25.0%)  Loss=1.2108  cls_loss=0.8429  reg_loss=0.3678  lr_det=3.2e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1777s  iter_time=126.557s
2025-08-04 19:36:42 Train INFO: [Train]: [002][00013/00051] (26.9%)  Loss=1.2163  cls_loss=0.8477  reg_loss=0.3685  lr_det=3.1e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1647s  iter_time=14.362s
2025-08-04 19:36:44 Train INFO: [Train]: [002][00014/00051] (28.8%)  Loss=1.2194  cls_loss=0.8495  reg_loss=0.3699  lr_det=2.9e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1503s  iter_time=2.381s
2025-08-04 19:36:47 Train INFO: [Train]: [002][00015/00051] (30.8%)  Loss=1.2120  cls_loss=0.8441  reg_loss=0.3679  lr_det=2.8e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1376s  iter_time=2.365s
2025-08-04 19:38:56 Train INFO: [Train]: [002][00016/00051] (32.7%)  Loss=1.2043  cls_loss=0.8377  reg_loss=0.3666  lr_det=2.7e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1526s  iter_time=129.391s
2025-08-04 19:39:01 Train INFO: [Train]: [002][00017/00051] (34.6%)  Loss=1.1927  cls_loss=0.8293  reg_loss=0.3634  lr_det=2.5e-05  GPU=1429MB(alloc)/10846MB(reserved)/9063MB(max)  ETA=1408s  iter_time=4.434s
2025-08-04 19:43:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-04 19:43:20 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-04 19:43:20 Train INFO: training subset: 831 videos
2025-08-04 19:43:20 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-04 19:43:20 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-04 19:43:21 Train INFO: Using single GPU training...
2025-08-04 19:43:21 Train INFO: Using Model EMA...
2025-08-04 19:43:21 Train INFO: Using Automatic Mixed Precision...
2025-08-04 19:43:21 Train INFO: GPU Memory: 24.0 GB
2025-08-04 19:43:21 Train INFO: Freeze the backbone...
2025-08-04 19:43:21 Train INFO: Training Starts...

2025-08-04 19:43:21 Train INFO: Running sanity check with 1 validation steps...
2025-08-04 19:43:21 Train INFO: [Val]: Epoch -1 Loss
2025-08-04 20:01:30 Train WARNING: mAP evaluation failed: mAP_PKU_MMD.__init__() missing 1 required positional argument: 'prediction_filename'
2025-08-04 20:01:30 Train INFO: [Val]: [-01]  Loss=2.1280  cls_loss=1.1664  reg_loss=0.9615
2025-08-04 20:01:30 Train INFO: Sanity check completed.

2025-08-04 20:01:30 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-08-05 09:25:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 09:25:50 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 09:25:51 Train INFO: training subset: 831 videos
2025-08-05 09:25:51 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 09:25:51 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 09:25:51 Train INFO: Using single GPU training...
2025-08-05 09:25:51 Train INFO: Using Model EMA...
2025-08-05 09:25:51 Train INFO: Using Automatic Mixed Precision...
2025-08-05 09:25:51 Train INFO: GPU Memory: 24.0 GB
2025-08-05 09:25:51 Train INFO: Freeze the backbone...
2025-08-05 09:25:51 Train INFO: Training Starts...

2025-08-05 09:25:51 Train INFO: Running sanity check with 1 validation steps...
2025-08-05 09:25:51 Train INFO: [Val]: Epoch -1 Loss
2025-08-05 09:31:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 09:31:16 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 09:31:16 Train INFO: training subset: 831 videos
2025-08-05 09:31:16 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 09:31:16 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 09:31:17 Train INFO: Using single GPU training...
2025-08-05 09:31:17 Train INFO: Using Model EMA...
2025-08-05 09:31:17 Train INFO: Using Automatic Mixed Precision...
2025-08-05 09:31:17 Train INFO: GPU Memory: 24.0 GB
2025-08-05 09:31:17 Train INFO: Freeze the backbone...
2025-08-05 09:31:17 Train INFO: Training Starts...

2025-08-05 09:31:17 Train INFO: Running sanity check with 1 validation steps...
2025-08-05 09:31:17 Train INFO: [Val]: Epoch -1 Loss
2025-08-05 09:34:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 09:34:26 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 09:34:26 Train INFO: training subset: 831 videos
2025-08-05 09:34:26 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 09:34:26 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 09:34:27 Train INFO: Using single GPU training...
2025-08-05 09:34:27 Train INFO: Using Model EMA...
2025-08-05 09:34:27 Train INFO: Using Automatic Mixed Precision...
2025-08-05 09:34:27 Train INFO: GPU Memory: 24.0 GB
2025-08-05 09:34:27 Train INFO: Freeze the backbone...
2025-08-05 09:34:27 Train INFO: Training Starts...

2025-08-05 09:34:27 Train INFO: Running sanity check with 1 validation steps...
2025-08-05 09:34:27 Train INFO: [Val]: Epoch -1 Loss
2025-08-05 09:37:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 09:37:19 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 09:37:19 Train INFO: training subset: 831 videos
2025-08-05 09:37:19 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 09:37:19 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 09:37:20 Train INFO: Using single GPU training...
2025-08-05 09:37:20 Train INFO: Using Model EMA...
2025-08-05 09:37:20 Train INFO: Using Automatic Mixed Precision...
2025-08-05 09:37:20 Train INFO: GPU Memory: 24.0 GB
2025-08-05 09:37:20 Train INFO: Freeze the backbone...
2025-08-05 09:37:20 Train INFO: Training Starts...

2025-08-05 09:37:20 Train INFO: Running sanity check with 1 validation steps...
2025-08-05 09:37:20 Train INFO: [Val]: Epoch -1 Loss
2025-08-05 09:54:01 Train WARNING: mAP evaluation failed: dict() got multiple values for keyword argument 'prediction_filename'
2025-08-05 09:54:01 Train INFO: [Val]: [-01]  Loss=2.1280  cls_loss=1.1664  reg_loss=0.9615
2025-08-05 09:54:01 Train INFO: Sanity check completed.

2025-08-05 09:54:01 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-08-05 09:57:51 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 09:57:51 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 09:57:51 Train INFO: training subset: 831 videos
2025-08-05 09:57:52 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 09:57:52 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 09:57:52 Train INFO: Using single GPU training...
2025-08-05 09:57:52 Train INFO: Using Model EMA...
2025-08-05 09:57:52 Train INFO: Using Automatic Mixed Precision...
2025-08-05 09:57:52 Train INFO: GPU Memory: 24.0 GB
2025-08-05 09:57:52 Train INFO: Freeze the backbone...
2025-08-05 09:57:52 Train INFO: Training Starts...

2025-08-05 09:57:52 Train INFO: Running sanity check with 1 validation steps...
2025-08-05 09:57:52 Train INFO: [Val]: Epoch -1 Loss
2025-08-05 10:17:23 Train INFO: [Val]: [-01]  Loss=2.1280  cls_loss=1.1664  reg_loss=0.9615  Average-mAP=0.00%
2025-08-05 10:17:23 Train INFO: Sanity check completed.

2025-08-05 10:17:23 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-08-05 10:20:14 Train INFO: [Train]: [000][00001/00051] (3.8%)  Loss=1.4875  cls_loss=0.8064  reg_loss=0.6811  lr_det=2.0e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=4272s  iter_time=170.874s  fwd=2.414s/bwd=0.060s/opt=0.010s
2025-08-05 10:20:17 Train INFO: [Train]: [000][00002/00051] (5.8%)  Loss=1.6347  cls_loss=0.8918  reg_loss=0.7429  lr_det=3.9e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2832s  iter_time=2.503s
2025-08-05 10:20:19 Train INFO: [Train]: [000][00003/00051] (7.7%)  Loss=1.6418  cls_loss=0.8979  reg_loss=0.7439  lr_det=5.9e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2111s  iter_time=2.541s
2025-08-05 10:22:31 Train INFO: [Train]: [000][00004/00051] (9.6%)  Loss=1.6097  cls_loss=0.8868  reg_loss=0.7229  lr_det=7.8e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2892s  iter_time=131.698s
2025-08-05 10:22:36 Train INFO: [Train]: [000][00005/00051] (11.5%)  Loss=1.6080  cls_loss=0.8951  reg_loss=0.7129  lr_det=9.8e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2395s  iter_time=4.773s
2025-08-05 10:22:38 Train INFO: [Train]: [000][00006/00051] (13.5%)  Loss=1.6155  cls_loss=0.9107  reg_loss=0.7047  lr_det=1.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2023s  iter_time=2.324s
2025-08-05 10:22:40 Train INFO: [Train]: [000][00007/00051] (15.4%)  Loss=1.6329  cls_loss=0.9337  reg_loss=0.6993  lr_det=1.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1744s  iter_time=2.294s
2025-08-05 10:25:00 Train INFO: [Train]: [000][00008/00051] (17.3%)  Loss=1.5996  cls_loss=0.9257  reg_loss=0.6739  lr_det=1.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2181s  iter_time=139.480s
2025-08-05 10:25:11 Train INFO: [Train]: [000][00009/00051] (19.2%)  Loss=1.5669  cls_loss=0.9215  reg_loss=0.6454  lr_det=1.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1963s  iter_time=10.790s
2025-08-05 10:25:13 Train INFO: [Train]: [000][00010/00051] (21.2%)  Loss=1.5547  cls_loss=0.9281  reg_loss=0.6266  lr_det=2.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1751s  iter_time=2.407s
2025-08-05 10:25:15 Train INFO: [Train]: [000][00011/00051] (23.1%)  Loss=1.5498  cls_loss=0.9389  reg_loss=0.6109  lr_det=2.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1574s  iter_time=2.355s
2025-08-05 10:27:39 Train INFO: [Train]: [000][00012/00051] (25.0%)  Loss=1.5334  cls_loss=0.9399  reg_loss=0.5935  lr_det=2.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1846s  iter_time=143.324s
2025-08-05 10:27:53 Train INFO: [Train]: [000][00013/00051] (26.9%)  Loss=1.5167  cls_loss=0.9405  reg_loss=0.5762  lr_det=2.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1709s  iter_time=14.099s
2025-08-05 10:27:55 Train INFO: [Train]: [000][00014/00051] (28.8%)  Loss=1.5284  cls_loss=0.9572  reg_loss=0.5712  lr_det=2.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1558s  iter_time=2.315s
2025-08-05 10:27:57 Train INFO: [Train]: [000][00015/00051] (30.8%)  Loss=1.5172  cls_loss=0.9568  reg_loss=0.5604  lr_det=2.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1427s  iter_time=2.391s
2025-08-05 10:30:18 Train INFO: [Train]: [000][00016/00051] (32.7%)  Loss=1.5084  cls_loss=0.9578  reg_loss=0.5506  lr_det=3.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1596s  iter_time=140.814s
2025-08-05 10:30:29 Train INFO: [Train]: [000][00017/00051] (34.6%)  Loss=1.4946  cls_loss=0.9520  reg_loss=0.5426  lr_det=3.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1484s  iter_time=10.530s
2025-08-05 10:30:31 Train INFO: [Train]: [000][00018/00051] (36.5%)  Loss=1.4830  cls_loss=0.9504  reg_loss=0.5325  lr_det=3.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1369s  iter_time=2.378s
2025-08-05 10:30:34 Train INFO: [Train]: [000][00019/00051] (38.5%)  Loss=1.4760  cls_loss=0.9498  reg_loss=0.5262  lr_det=3.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1264s  iter_time=2.355s
2025-08-05 10:32:44 Train INFO: [Train]: [000][00020/00051] (40.4%)  Loss=1.4528  cls_loss=0.9387  reg_loss=0.5141  lr_det=3.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1359s  iter_time=130.162s
2025-08-05 10:32:54 Train INFO: [Train]: [000][00021/00051] (42.3%)  Loss=1.4510  cls_loss=0.9414  reg_loss=0.5096  lr_det=4.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1269s  iter_time=10.228s
2025-08-05 10:32:56 Train INFO: [Train]: [000][00022/00051] (44.2%)  Loss=1.4525  cls_loss=0.9478  reg_loss=0.5047  lr_det=4.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1176s  iter_time=2.290s
2025-08-05 10:32:58 Train INFO: [Train]: [000][00023/00051] (46.2%)  Loss=1.4299  cls_loss=0.9358  reg_loss=0.4941  lr_det=4.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1091s  iter_time=2.263s
2025-08-05 10:35:12 Train INFO: [Train]: [000][00024/00051] (48.1%)  Loss=1.4258  cls_loss=0.9373  reg_loss=0.4885  lr_det=4.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1155s  iter_time=133.784s
2025-08-05 10:35:23 Train INFO: [Train]: [000][00025/00051] (50.0%)  Loss=1.4196  cls_loss=0.9360  reg_loss=0.4836  lr_det=4.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1080s  iter_time=11.097s
2025-08-05 10:35:26 Train INFO: [Train]: [000][00026/00051] (51.9%)  Loss=1.4175  cls_loss=0.9374  reg_loss=0.4801  lr_det=5.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1002s  iter_time=2.402s
2025-08-05 10:35:28 Train INFO: [Train]: [000][00027/00051] (53.8%)  Loss=1.4199  cls_loss=0.9413  reg_loss=0.4786  lr_det=5.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=930s  iter_time=2.368s
2025-08-05 10:37:29 Train INFO: [Train]: [000][00028/00051] (55.8%)  Loss=1.4084  cls_loss=0.9356  reg_loss=0.4728  lr_det=5.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=957s  iter_time=121.265s
2025-08-05 10:37:44 Train INFO: [Train]: [000][00029/00051] (57.7%)  Loss=1.4102  cls_loss=0.9395  reg_loss=0.4706  lr_det=5.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=895s  iter_time=14.143s
2025-08-05 10:37:46 Train INFO: [Train]: [000][00030/00051] (59.6%)  Loss=1.4103  cls_loss=0.9426  reg_loss=0.4677  lr_det=5.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=828s  iter_time=2.262s
2025-08-05 10:37:48 Train INFO: [Train]: [000][00031/00051] (61.5%)  Loss=1.4016  cls_loss=0.9388  reg_loss=0.4628  lr_det=6.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=766s  iter_time=2.348s
2025-08-05 10:39:50 Train INFO: [Train]: [000][00032/00051] (63.5%)  Loss=1.3967  cls_loss=0.9375  reg_loss=0.4592  lr_det=6.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=775s  iter_time=121.883s
2025-08-05 10:40:04 Train INFO: [Train]: [000][00033/00051] (65.4%)  Loss=1.3866  cls_loss=0.9324  reg_loss=0.4541  lr_det=6.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=721s  iter_time=14.166s
2025-08-05 10:40:07 Train INFO: [Train]: [000][00034/00051] (67.3%)  Loss=1.3934  cls_loss=0.9389  reg_loss=0.4545  lr_det=6.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=662s  iter_time=2.506s
2025-08-05 10:40:09 Train INFO: [Train]: [000][00035/00051] (69.2%)  Loss=1.3859  cls_loss=0.9357  reg_loss=0.4503  lr_det=6.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=607s  iter_time=2.487s
2025-08-05 10:42:20 Train INFO: [Train]: [000][00036/00051] (71.2%)  Loss=1.3854  cls_loss=0.9366  reg_loss=0.4488  lr_det=7.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=607s  iter_time=131.285s
2025-08-05 10:42:33 Train INFO: [Train]: [000][00037/00051] (73.1%)  Loss=1.3860  cls_loss=0.9382  reg_loss=0.4478  lr_det=7.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=556s  iter_time=12.957s
2025-08-05 10:42:36 Train INFO: [Train]: [000][00038/00051] (75.0%)  Loss=1.3858  cls_loss=0.9391  reg_loss=0.4467  lr_det=7.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=504s  iter_time=2.514s
2025-08-05 10:42:38 Train INFO: [Train]: [000][00039/00051] (76.9%)  Loss=1.3860  cls_loss=0.9407  reg_loss=0.4453  lr_det=7.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=455s  iter_time=2.372s
2025-08-05 10:44:41 Train INFO: [Train]: [000][00040/00051] (78.8%)  Loss=1.3834  cls_loss=0.9407  reg_loss=0.4427  lr_det=7.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=439s  iter_time=122.689s
2025-08-05 10:44:56 Train INFO: [Train]: [000][00041/00051] (80.8%)  Loss=1.3782  cls_loss=0.9382  reg_loss=0.4401  lr_det=8.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=393s  iter_time=14.704s
2025-08-05 10:44:58 Train INFO: [Train]: [000][00042/00051] (82.7%)  Loss=1.3760  cls_loss=0.9380  reg_loss=0.4380  lr_det=8.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=346s  iter_time=2.368s
2025-08-05 10:45:00 Train INFO: [Train]: [000][00043/00051] (84.6%)  Loss=1.3736  cls_loss=0.9374  reg_loss=0.4362  lr_det=8.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=301s  iter_time=2.309s
2025-08-05 10:47:02 Train INFO: [Train]: [000][00044/00051] (86.5%)  Loss=1.3739  cls_loss=0.9380  reg_loss=0.4359  lr_det=8.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=277s  iter_time=121.112s
2025-08-05 10:47:16 Train INFO: [Train]: [000][00045/00051] (88.5%)  Loss=1.3679  cls_loss=0.9347  reg_loss=0.4332  lr_det=8.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=234s  iter_time=14.114s
2025-08-05 10:47:18 Train INFO: [Train]: [000][00046/00051] (90.4%)  Loss=1.3626  cls_loss=0.9318  reg_loss=0.4308  lr_det=9.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=191s  iter_time=2.516s
2025-08-05 10:47:21 Train INFO: [Train]: [000][00047/00051] (92.3%)  Loss=1.3576  cls_loss=0.9291  reg_loss=0.4285  lr_det=9.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=150s  iter_time=2.514s
2025-08-05 10:49:25 Train INFO: [Train]: [000][00048/00051] (94.2%)  Loss=1.3519  cls_loss=0.9261  reg_loss=0.4258  lr_det=9.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=118s  iter_time=124.729s
2025-08-05 10:49:35 Train INFO: [Train]: [000][00049/00051] (96.2%)  Loss=1.3512  cls_loss=0.9269  reg_loss=0.4243  lr_det=9.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=77s  iter_time=10.071s
2025-08-05 10:49:38 Train INFO: [Train]: [000][00050/00051] (98.1%)  Loss=1.3509  cls_loss=0.9276  reg_loss=0.4233  lr_det=9.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=38s  iter_time=2.250s
2025-08-05 10:49:42 Train INFO: [Train]: [000][00051/00051] (100.0%)  Loss=1.3474  cls_loss=0.9261  reg_loss=0.4213  lr_det=1.0e-04  GPU=1392MB(alloc)/3012MB(reserved)/9095MB(max)  ETA=0s  iter_time=4.614s
2025-08-05 10:49:43 Train INFO: [Train]: Epoch 0 completed in 1939.9s (avg 37.306s/iter)
2025-08-05 10:49:43 Train INFO: [Train]: Final Loss=1.3474
2025-08-05 10:49:44 Train INFO: Checkpoint saved at epoch 0
2025-08-05 10:49:44 Train INFO: [Train]: Epoch 1 started (Total iterations: 52)
2025-08-05 10:52:26 Train INFO: [Train]: [001][00001/00051] (3.8%)  Loss=1.2504  cls_loss=0.8965  reg_loss=0.3539  lr_det=1.0e-04  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=4044s  iter_time=161.760s  fwd=2.440s/bwd=0.043s/opt=0.007s
2025-08-05 10:52:28 Train INFO: [Train]: [001][00002/00051] (5.8%)  Loss=1.3152  cls_loss=0.9347  reg_loss=0.3805  lr_det=1.0e-04  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2682s  iter_time=2.449s
2025-08-05 10:52:31 Train INFO: [Train]: [001][00003/00051] (7.7%)  Loss=1.2955  cls_loss=0.9264  reg_loss=0.3691  lr_det=1.0e-04  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2000s  iter_time=2.445s
2025-08-05 10:54:41 Train INFO: [Train]: [001][00004/00051] (9.6%)  Loss=1.2819  cls_loss=0.9140  reg_loss=0.3679  lr_det=1.0e-04  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2795s  iter_time=130.691s
2025-08-05 10:54:48 Train INFO: [Train]: [001][00005/00051] (11.5%)  Loss=1.3212  cls_loss=0.9447  reg_loss=0.3764  lr_det=9.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2331s  iter_time=6.642s
2025-08-05 10:54:51 Train INFO: [Train]: [001][00006/00051] (13.5%)  Loss=1.3347  cls_loss=0.9551  reg_loss=0.3796  lr_det=9.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1970s  iter_time=2.449s
2025-08-05 10:54:53 Train INFO: [Train]: [001][00007/00051] (15.4%)  Loss=1.3528  cls_loss=0.9644  reg_loss=0.3884  lr_det=9.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1699s  iter_time=2.440s
2025-08-05 10:57:00 Train INFO: [Train]: [001][00008/00051] (17.3%)  Loss=1.3501  cls_loss=0.9626  reg_loss=0.3875  lr_det=9.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2083s  iter_time=127.061s
2025-08-05 10:57:11 Train INFO: [Train]: [001][00009/00051] (19.2%)  Loss=1.3365  cls_loss=0.9557  reg_loss=0.3808  lr_det=9.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1879s  iter_time=11.364s
2025-08-05 10:57:14 Train INFO: [Train]: [001][00010/00051] (21.2%)  Loss=1.3172  cls_loss=0.9450  reg_loss=0.3722  lr_det=9.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1678s  iter_time=2.939s
2025-08-05 10:57:17 Train INFO: [Train]: [001][00011/00051] (23.1%)  Loss=1.3251  cls_loss=0.9502  reg_loss=0.3749  lr_det=9.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1509s  iter_time=2.407s
2025-08-05 10:59:22 Train INFO: [Train]: [001][00012/00051] (25.0%)  Loss=1.3278  cls_loss=0.9529  reg_loss=0.3748  lr_det=9.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1733s  iter_time=125.108s
2025-08-05 10:59:34 Train INFO: [Train]: [001][00013/00051] (26.9%)  Loss=1.3222  cls_loss=0.9501  reg_loss=0.3721  lr_det=9.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1600s  iter_time=11.862s
2025-08-05 10:59:37 Train INFO: [Train]: [001][00014/00051] (28.8%)  Loss=1.3283  cls_loss=0.9539  reg_loss=0.3744  lr_det=9.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1463s  iter_time=3.485s
2025-08-05 10:59:40 Train INFO: [Train]: [001][00015/00051] (30.8%)  Loss=1.3236  cls_loss=0.9503  reg_loss=0.3733  lr_det=9.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1340s  iter_time=2.488s
2025-08-05 11:01:46 Train INFO: [Train]: [001][00016/00051] (32.7%)  Loss=1.3139  cls_loss=0.9439  reg_loss=0.3700  lr_det=9.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1486s  iter_time=126.224s
2025-08-05 11:01:51 Train INFO: [Train]: [001][00017/00051] (34.6%)  Loss=1.3071  cls_loss=0.9384  reg_loss=0.3687  lr_det=9.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1374s  iter_time=5.387s
2025-08-05 11:01:57 Train INFO: [Train]: [001][00018/00051] (36.5%)  Loss=1.3030  cls_loss=0.9363  reg_loss=0.3667  lr_det=9.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1273s  iter_time=5.596s
2025-08-05 11:01:59 Train INFO: [Train]: [001][00019/00051] (38.5%)  Loss=1.3024  cls_loss=0.9355  reg_loss=0.3669  lr_det=9.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1176s  iter_time=2.401s
2025-08-05 11:04:08 Train INFO: [Train]: [001][00020/00051] (40.4%)  Loss=1.2948  cls_loss=0.9306  reg_loss=0.3642  lr_det=9.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1275s  iter_time=128.253s
2025-08-05 11:04:10 Train INFO: [Train]: [001][00021/00051] (42.3%)  Loss=1.2926  cls_loss=0.9284  reg_loss=0.3642  lr_det=9.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1181s  iter_time=2.502s
2025-08-05 11:04:19 Train INFO: [Train]: [001][00022/00051] (44.2%)  Loss=1.3064  cls_loss=0.9380  reg_loss=0.3685  lr_det=8.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1103s  iter_time=9.117s
2025-08-05 11:04:22 Train INFO: [Train]: [001][00023/00051] (46.2%)  Loss=1.2998  cls_loss=0.9335  reg_loss=0.3663  lr_det=8.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1024s  iter_time=2.444s
2025-08-05 11:06:29 Train INFO: [Train]: [001][00024/00051] (48.1%)  Loss=1.3052  cls_loss=0.9378  reg_loss=0.3674  lr_det=8.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1085s  iter_time=127.089s
2025-08-05 11:06:36 Train INFO: [Train]: [001][00025/00051] (50.0%)  Loss=1.3062  cls_loss=0.9378  reg_loss=0.3684  lr_det=8.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1012s  iter_time=7.326s
2025-08-05 11:06:42 Train INFO: [Train]: [001][00026/00051] (51.9%)  Loss=1.3160  cls_loss=0.9446  reg_loss=0.3714  lr_det=8.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=943s  iter_time=6.091s
2025-08-05 11:06:45 Train INFO: [Train]: [001][00027/00051] (53.8%)  Loss=1.3150  cls_loss=0.9440  reg_loss=0.3710  lr_det=8.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=875s  iter_time=2.448s
2025-08-05 11:08:46 Train INFO: [Train]: [001][00028/00051] (55.8%)  Loss=1.3074  cls_loss=0.9389  reg_loss=0.3685  lr_det=8.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=905s  iter_time=120.971s
2025-08-05 11:08:57 Train INFO: [Train]: [001][00029/00051] (57.7%)  Loss=1.3052  cls_loss=0.9371  reg_loss=0.3682  lr_det=8.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=846s  iter_time=11.582s
2025-08-05 11:09:00 Train INFO: [Train]: [001][00030/00051] (59.6%)  Loss=1.2979  cls_loss=0.9323  reg_loss=0.3656  lr_det=8.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=783s  iter_time=3.304s
2025-08-05 11:09:03 Train INFO: [Train]: [001][00031/00051] (61.5%)  Loss=1.2917  cls_loss=0.9281  reg_loss=0.3636  lr_det=8.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=724s  iter_time=2.504s
2025-08-05 11:11:04 Train INFO: [Train]: [001][00032/00051] (63.5%)  Loss=1.2962  cls_loss=0.9305  reg_loss=0.3656  lr_det=7.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=737s  iter_time=121.425s
2025-08-05 11:11:15 Train INFO: [Train]: [001][00033/00051] (65.4%)  Loss=1.2911  cls_loss=0.9268  reg_loss=0.3643  lr_det=7.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=683s  iter_time=10.133s
2025-08-05 11:11:20 Train INFO: [Train]: [001][00034/00051] (67.3%)  Loss=1.2975  cls_loss=0.9317  reg_loss=0.3658  lr_det=7.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=629s  iter_time=5.567s
2025-08-05 11:11:23 Train INFO: [Train]: [001][00035/00051] (69.2%)  Loss=1.2989  cls_loss=0.9326  reg_loss=0.3663  lr_det=7.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=577s  iter_time=2.471s
2025-08-05 11:13:28 Train INFO: [Train]: [001][00036/00051] (71.2%)  Loss=1.3018  cls_loss=0.9339  reg_loss=0.3679  lr_det=7.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=577s  iter_time=125.023s
2025-08-05 11:13:38 Train INFO: [Train]: [001][00037/00051] (73.1%)  Loss=1.2989  cls_loss=0.9315  reg_loss=0.3675  lr_det=7.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=528s  iter_time=10.853s
2025-08-05 11:13:44 Train INFO: [Train]: [001][00038/00051] (75.0%)  Loss=1.3012  cls_loss=0.9324  reg_loss=0.3688  lr_det=7.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=480s  iter_time=5.325s
2025-08-05 11:13:46 Train INFO: [Train]: [001][00039/00051] (76.9%)  Loss=1.2998  cls_loss=0.9306  reg_loss=0.3693  lr_det=6.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=433s  iter_time=2.322s
2025-08-05 11:15:49 Train INFO: [Train]: [001][00040/00051] (78.8%)  Loss=1.2963  cls_loss=0.9281  reg_loss=0.3682  lr_det=6.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=420s  iter_time=122.950s
2025-08-05 11:15:58 Train INFO: [Train]: [001][00041/00051] (80.8%)  Loss=1.2952  cls_loss=0.9273  reg_loss=0.3678  lr_det=6.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=375s  iter_time=8.543s
2025-08-05 11:16:04 Train INFO: [Train]: [001][00042/00051] (82.7%)  Loss=1.2957  cls_loss=0.9281  reg_loss=0.3677  lr_det=6.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=331s  iter_time=6.336s
2025-08-05 11:16:06 Train INFO: [Train]: [001][00043/00051] (84.6%)  Loss=1.2899  cls_loss=0.9237  reg_loss=0.3662  lr_det=6.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=288s  iter_time=2.319s
2025-08-05 11:18:10 Train INFO: [Train]: [001][00044/00051] (86.5%)  Loss=1.2836  cls_loss=0.9189  reg_loss=0.3647  lr_det=6.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=265s  iter_time=124.113s
2025-08-05 11:18:17 Train INFO: [Train]: [001][00045/00051] (88.5%)  Loss=1.2834  cls_loss=0.9181  reg_loss=0.3653  lr_det=6.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=223s  iter_time=6.634s
2025-08-05 11:18:29 Train INFO: [Train]: [001][00046/00051] (90.4%)  Loss=1.2831  cls_loss=0.9174  reg_loss=0.3658  lr_det=5.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=183s  iter_time=11.583s
2025-08-05 11:18:31 Train INFO: [Train]: [001][00047/00051] (92.3%)  Loss=1.2809  cls_loss=0.9158  reg_loss=0.3651  lr_det=5.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=144s  iter_time=2.347s
2025-08-05 11:20:30 Train INFO: [Train]: [001][00048/00051] (94.2%)  Loss=1.2781  cls_loss=0.9138  reg_loss=0.3643  lr_det=5.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=113s  iter_time=118.655s
2025-08-05 11:20:33 Train INFO: [Train]: [001][00049/00051] (96.2%)  Loss=1.2768  cls_loss=0.9132  reg_loss=0.3636  lr_det=5.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=74s  iter_time=3.735s
2025-08-05 11:20:40 Train INFO: [Train]: [001][00050/00051] (98.1%)  Loss=1.2743  cls_loss=0.9111  reg_loss=0.3632  lr_det=5.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=36s  iter_time=6.381s
2025-08-05 11:20:42 Train INFO: [Train]: [001][00051/00051] (100.0%)  Loss=1.2714  cls_loss=0.9087  reg_loss=0.3627  lr_det=5.2e-05  GPU=1392MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=0s  iter_time=2.023s
2025-08-05 11:20:42 Train INFO: [Train]: Epoch 1 completed in 1858.3s (avg 35.737s/iter)
2025-08-05 11:20:42 Train INFO: [Train]: Final Loss=1.2714
2025-08-05 11:20:42 Train INFO: [Val]: Epoch 1 Loss
2025-08-05 11:39:59 Train INFO: [Val]: [001]  Loss=1.8128  cls_loss=1.1290  reg_loss=0.6838  Average-mAP=0.01%
2025-08-05 11:40:01 Train INFO: Checkpoint saved at epoch 1
2025-08-05 11:40:01 Train INFO: [Train]: Epoch 2 started (Total iterations: 52)
2025-08-05 11:42:46 Train INFO: [Train]: [002][00001/00051] (3.8%)  Loss=1.0463  cls_loss=0.7470  reg_loss=0.2993  lr_det=4.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=4135s  iter_time=165.400s  fwd=2.115s/bwd=0.059s/opt=0.010s
2025-08-05 11:42:48 Train INFO: [Train]: [002][00002/00051] (5.8%)  Loss=1.1492  cls_loss=0.8052  reg_loss=0.3440  lr_det=4.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2738s  iter_time=2.234s
2025-08-05 11:42:51 Train INFO: [Train]: [002][00003/00051] (7.7%)  Loss=1.1320  cls_loss=0.7982  reg_loss=0.3338  lr_det=4.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2039s  iter_time=2.319s
2025-08-05 11:45:08 Train INFO: [Train]: [002][00004/00051] (9.6%)  Loss=1.1150  cls_loss=0.7884  reg_loss=0.3267  lr_det=4.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2889s  iter_time=137.397s
2025-08-05 11:45:13 Train INFO: [Train]: [002][00005/00051] (11.5%)  Loss=1.1406  cls_loss=0.8037  reg_loss=0.3368  lr_det=4.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2393s  iter_time=4.757s
2025-08-05 11:45:15 Train INFO: [Train]: [002][00006/00051] (13.5%)  Loss=1.1704  cls_loss=0.8249  reg_loss=0.3455  lr_det=4.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2021s  iter_time=2.225s
2025-08-05 11:45:17 Train INFO: [Train]: [002][00007/00051] (15.4%)  Loss=1.1873  cls_loss=0.8386  reg_loss=0.3487  lr_det=4.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1742s  iter_time=2.352s
2025-08-05 11:47:29 Train INFO: [Train]: [002][00008/00051] (17.3%)  Loss=1.1784  cls_loss=0.8310  reg_loss=0.3474  lr_det=3.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=2140s  iter_time=131.233s
2025-08-05 11:47:38 Train INFO: [Train]: [002][00009/00051] (19.2%)  Loss=1.1743  cls_loss=0.8296  reg_loss=0.3447  lr_det=3.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1922s  iter_time=9.771s
2025-08-05 11:47:41 Train INFO: [Train]: [002][00010/00051] (21.2%)  Loss=1.1885  cls_loss=0.8366  reg_loss=0.3520  lr_det=3.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1717s  iter_time=2.998s
2025-08-05 11:47:44 Train INFO: [Train]: [002][00011/00051] (23.1%)  Loss=1.1969  cls_loss=0.8407  reg_loss=0.3562  lr_det=3.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1543s  iter_time=2.220s
2025-08-05 11:49:53 Train INFO: [Train]: [002][00012/00051] (25.0%)  Loss=1.1839  cls_loss=0.8317  reg_loss=0.3522  lr_det=3.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1776s  iter_time=128.945s
2025-08-05 11:50:03 Train INFO: [Train]: [002][00013/00051] (26.9%)  Loss=1.1791  cls_loss=0.8279  reg_loss=0.3511  lr_det=3.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1636s  iter_time=10.866s
2025-08-05 11:50:06 Train INFO: [Train]: [002][00014/00051] (28.8%)  Loss=1.1741  cls_loss=0.8240  reg_loss=0.3501  lr_det=2.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1492s  iter_time=2.328s
2025-08-05 11:50:08 Train INFO: [Train]: [002][00015/00051] (30.8%)  Loss=1.1683  cls_loss=0.8194  reg_loss=0.3489  lr_det=2.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1367s  iter_time=2.291s
2025-08-05 11:52:19 Train INFO: [Train]: [002][00016/00051] (32.7%)  Loss=1.1585  cls_loss=0.8111  reg_loss=0.3474  lr_det=2.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1520s  iter_time=131.099s
2025-08-05 11:52:22 Train INFO: [Train]: [002][00017/00051] (34.6%)  Loss=1.1615  cls_loss=0.8118  reg_loss=0.3498  lr_det=2.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1400s  iter_time=2.605s
2025-08-05 11:52:24 Train INFO: [Train]: [002][00018/00051] (36.5%)  Loss=1.1564  cls_loss=0.8077  reg_loss=0.3487  lr_det=2.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1291s  iter_time=2.246s
2025-08-05 11:52:26 Train INFO: [Train]: [002][00019/00051] (38.5%)  Loss=1.1637  cls_loss=0.8124  reg_loss=0.3513  lr_det=2.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1193s  iter_time=2.274s
2025-08-05 11:54:39 Train INFO: [Train]: [002][00020/00051] (40.4%)  Loss=1.1566  cls_loss=0.8067  reg_loss=0.3499  lr_det=2.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1297s  iter_time=132.837s
2025-08-05 11:54:43 Train INFO: [Train]: [002][00021/00051] (42.3%)  Loss=1.1570  cls_loss=0.8052  reg_loss=0.3517  lr_det=2.0e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1203s  iter_time=3.714s
2025-08-05 11:54:45 Train INFO: [Train]: [002][00022/00051] (44.2%)  Loss=1.1701  cls_loss=0.8126  reg_loss=0.3574  lr_det=1.9e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1115s  iter_time=2.293s
2025-08-05 11:54:47 Train INFO: [Train]: [002][00023/00051] (46.2%)  Loss=1.1592  cls_loss=0.8050  reg_loss=0.3542  lr_det=1.8e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1035s  iter_time=2.288s
2025-08-05 11:57:01 Train INFO: [Train]: [002][00024/00051] (48.1%)  Loss=1.1624  cls_loss=0.8075  reg_loss=0.3549  lr_det=1.7e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1102s  iter_time=134.071s
2025-08-05 11:57:09 Train INFO: [Train]: [002][00025/00051] (50.0%)  Loss=1.1639  cls_loss=0.8083  reg_loss=0.3556  lr_det=1.6e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=1029s  iter_time=7.814s
2025-08-05 11:57:11 Train INFO: [Train]: [002][00026/00051] (51.9%)  Loss=1.1668  cls_loss=0.8097  reg_loss=0.3572  lr_det=1.5e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=954s  iter_time=2.239s
2025-08-05 11:57:14 Train INFO: [Train]: [002][00027/00051] (53.8%)  Loss=1.1669  cls_loss=0.8094  reg_loss=0.3575  lr_det=1.4e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=886s  iter_time=2.318s
2025-08-05 11:59:20 Train INFO: [Train]: [002][00028/00051] (55.8%)  Loss=1.1573  cls_loss=0.8022  reg_loss=0.3551  lr_det=1.3e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=919s  iter_time=125.940s
2025-08-05 11:59:33 Train INFO: [Train]: [002][00029/00051] (57.7%)  Loss=1.1582  cls_loss=0.8024  reg_loss=0.3558  lr_det=1.2e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=860s  iter_time=13.432s
2025-08-05 11:59:35 Train INFO: [Train]: [002][00030/00051] (59.6%)  Loss=1.1571  cls_loss=0.8004  reg_loss=0.3567  lr_det=1.1e-05  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=796s  iter_time=2.282s
2025-08-05 11:59:38 Train INFO: [Train]: [002][00031/00051] (61.5%)  Loss=1.1482  cls_loss=0.7942  reg_loss=0.3540  lr_det=9.7e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=736s  iter_time=2.277s
2025-08-05 12:01:42 Train INFO: [Train]: [002][00032/00051] (63.5%)  Loss=1.1442  cls_loss=0.7917  reg_loss=0.3525  lr_det=8.9e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=749s  iter_time=124.274s
2025-08-05 12:01:55 Train INFO: [Train]: [002][00033/00051] (65.4%)  Loss=1.1428  cls_loss=0.7902  reg_loss=0.3526  lr_det=8.0e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=696s  iter_time=13.341s
2025-08-05 12:01:58 Train INFO: [Train]: [002][00034/00051] (67.3%)  Loss=1.1477  cls_loss=0.7929  reg_loss=0.3548  lr_det=7.2e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=640s  iter_time=2.340s
2025-08-05 12:02:00 Train INFO: [Train]: [002][00035/00051] (69.2%)  Loss=1.1420  cls_loss=0.7888  reg_loss=0.3531  lr_det=6.5e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=586s  iter_time=2.225s
2025-08-05 12:04:09 Train INFO: [Train]: [002][00036/00051] (71.2%)  Loss=1.1400  cls_loss=0.7871  reg_loss=0.3530  lr_det=5.7e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=587s  iter_time=128.562s
2025-08-05 12:04:24 Train INFO: [Train]: [002][00037/00051] (73.1%)  Loss=1.1327  cls_loss=0.7817  reg_loss=0.3510  lr_det=5.1e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=539s  iter_time=15.972s
2025-08-05 12:04:27 Train INFO: [Train]: [002][00038/00051] (75.0%)  Loss=1.1346  cls_loss=0.7824  reg_loss=0.3523  lr_det=4.4e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=489s  iter_time=2.329s
2025-08-05 12:04:29 Train INFO: [Train]: [002][00039/00051] (76.9%)  Loss=1.1333  cls_loss=0.7810  reg_loss=0.3523  lr_det=3.8e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=441s  iter_time=2.288s
2025-08-05 12:06:29 Train INFO: [Train]: [002][00040/00051] (78.8%)  Loss=1.1304  cls_loss=0.7791  reg_loss=0.3513  lr_det=3.3e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=426s  iter_time=120.331s
2025-08-05 12:06:48 Train INFO: [Train]: [002][00041/00051] (80.8%)  Loss=1.1311  cls_loss=0.7796  reg_loss=0.3516  lr_det=2.7e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=383s  iter_time=18.584s
2025-08-05 12:06:50 Train INFO: [Train]: [002][00042/00051] (82.7%)  Loss=1.1326  cls_loss=0.7806  reg_loss=0.3520  lr_det=2.3e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=337s  iter_time=2.273s
2025-08-05 12:06:53 Train INFO: [Train]: [002][00043/00051] (84.6%)  Loss=1.1354  cls_loss=0.7820  reg_loss=0.3534  lr_det=1.8e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=293s  iter_time=2.241s
2025-08-05 12:09:00 Train INFO: [Train]: [002][00044/00051] (86.5%)  Loss=1.1349  cls_loss=0.7810  reg_loss=0.3539  lr_det=1.5e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=271s  iter_time=127.430s
2025-08-05 12:09:14 Train INFO: [Train]: [002][00045/00051] (88.5%)  Loss=1.1306  cls_loss=0.7778  reg_loss=0.3529  lr_det=1.1e-06  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=229s  iter_time=13.651s
2025-08-05 12:09:16 Train INFO: [Train]: [002][00046/00051] (90.4%)  Loss=1.1316  cls_loss=0.7773  reg_loss=0.3543  lr_det=8.3e-07  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=187s  iter_time=2.317s
2025-08-05 12:09:18 Train INFO: [Train]: [002][00047/00051] (92.3%)  Loss=1.1300  cls_loss=0.7761  reg_loss=0.3539  lr_det=5.8e-07  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=146s  iter_time=2.332s
2025-08-05 12:11:24 Train INFO: [Train]: [002][00048/00051] (94.2%)  Loss=1.1297  cls_loss=0.7758  reg_loss=0.3539  lr_det=3.7e-07  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=115s  iter_time=125.367s
2025-08-05 12:11:35 Train INFO: [Train]: [002][00049/00051] (96.2%)  Loss=1.1282  cls_loss=0.7749  reg_loss=0.3533  lr_det=2.2e-07  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=76s  iter_time=11.558s
2025-08-05 12:11:37 Train INFO: [Train]: [002][00050/00051] (98.1%)  Loss=1.1278  cls_loss=0.7742  reg_loss=0.3535  lr_det=1.0e-07  GPU=1429MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=37s  iter_time=2.154s
2025-08-05 12:11:39 Train INFO: [Train]: [002][00051/00051] (100.0%)  Loss=1.1277  cls_loss=0.7740  reg_loss=0.3538  lr_det=3.3e-08  GPU=1392MB(alloc)/10246MB(reserved)/9095MB(max)  ETA=0s  iter_time=2.015s
2025-08-05 12:11:40 Train INFO: [Train]: Epoch 2 completed in 1899.4s (avg 36.528s/iter)
2025-08-05 12:11:40 Train INFO: [Train]: Final Loss=1.1277
2025-08-05 12:11:40 Train INFO: [Val]: Epoch 2 Loss
2025-08-05 12:31:12 Train INFO: [Val]: [002]  Loss=1.6582  cls_loss=1.1035  reg_loss=0.5548  Average-mAP=0.02%
2025-08-05 12:31:14 Train INFO: Checkpoint saved at epoch 2
2025-08-05 12:31:14 Train INFO: Training Over...

2025-08-05 12:57:38 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 12:57:39 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 12:57:39 Train INFO: training subset: 831 videos
2025-08-05 12:57:39 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 12:57:39 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 12:57:40 Train INFO: Using single GPU training...
2025-08-05 12:57:40 Train INFO: Using Model EMA...
2025-08-05 12:57:40 Train INFO: Using Automatic Mixed Precision...
2025-08-05 12:57:40 Train INFO: GPU Memory: 24.0 GB
2025-08-05 12:57:40 Train INFO: Freeze the backbone...
2025-08-05 12:57:40 Train INFO: Training Starts...

2025-08-05 12:57:40 Train INFO: Running sanity check with 1 validation steps...
2025-08-05 12:57:40 Train INFO: [Val]: Epoch -1 Loss
2025-08-05 13:17:53 Train INFO: [Val]: [-01]  Loss=2.1280  cls_loss=1.1666  reg_loss=0.9614  Average-mAP=0.00%
2025-08-05 13:17:53 Train INFO: Sanity check completed.

2025-08-05 13:17:53 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-08-05 13:20:30 Train INFO: [Train]: [000][00001/00051] (3.8%)  Loss=1.4875  cls_loss=0.8064  reg_loss=0.6811  lr_det=2.0e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=3922s  iter_time=156.884s  fwd=2.297s/bwd=0.068s/opt=0.011s
2025-08-05 13:20:32 Train INFO: [Train]: [000][00002/00051] (5.8%)  Loss=1.6348  cls_loss=0.8918  reg_loss=0.7430  lr_det=3.9e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2600s  iter_time=2.325s
2025-08-05 13:20:34 Train INFO: [Train]: [000][00003/00051] (7.7%)  Loss=1.6419  cls_loss=0.8979  reg_loss=0.7439  lr_det=5.9e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1938s  iter_time=2.260s
2025-08-05 13:22:54 Train INFO: [Train]: [000][00004/00051] (9.6%)  Loss=1.6097  cls_loss=0.8868  reg_loss=0.7229  lr_det=7.8e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2828s  iter_time=139.361s
2025-08-05 13:22:59 Train INFO: [Train]: [000][00005/00051] (11.5%)  Loss=1.6080  cls_loss=0.8951  reg_loss=0.7129  lr_det=9.8e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2349s  iter_time=5.557s
2025-08-05 13:23:02 Train INFO: [Train]: [000][00006/00051] (13.5%)  Loss=1.6155  cls_loss=0.9107  reg_loss=0.7047  lr_det=1.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1984s  iter_time=2.276s
2025-08-05 13:23:04 Train INFO: [Train]: [000][00007/00051] (15.4%)  Loss=1.6330  cls_loss=0.9337  reg_loss=0.6993  lr_det=1.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1711s  iter_time=2.381s
2025-08-05 13:25:19 Train INFO: [Train]: [000][00008/00051] (17.3%)  Loss=1.5996  cls_loss=0.9257  reg_loss=0.6740  lr_det=1.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2129s  iter_time=134.621s
2025-08-05 13:25:29 Train INFO: [Train]: [000][00009/00051] (19.2%)  Loss=1.5669  cls_loss=0.9216  reg_loss=0.6454  lr_det=1.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1915s  iter_time=10.189s
2025-08-05 13:25:31 Train INFO: [Train]: [000][00010/00051] (21.2%)  Loss=1.5547  cls_loss=0.9281  reg_loss=0.6266  lr_det=2.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1708s  iter_time=2.324s
2025-08-05 13:25:34 Train INFO: [Train]: [000][00011/00051] (23.1%)  Loss=1.5498  cls_loss=0.9389  reg_loss=0.6109  lr_det=2.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1535s  iter_time=2.405s
2025-08-05 13:27:45 Train INFO: [Train]: [000][00012/00051] (25.0%)  Loss=1.5334  cls_loss=0.9399  reg_loss=0.5935  lr_det=2.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1777s  iter_time=131.586s
2025-08-05 13:27:53 Train INFO: [Train]: [000][00013/00051] (26.9%)  Loss=1.5167  cls_loss=0.9405  reg_loss=0.5763  lr_det=2.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1630s  iter_time=8.188s
2025-08-05 13:27:56 Train INFO: [Train]: [000][00014/00051] (28.8%)  Loss=1.5284  cls_loss=0.9572  reg_loss=0.5712  lr_det=2.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1487s  iter_time=2.289s
2025-08-05 13:27:58 Train INFO: [Train]: [000][00015/00051] (30.8%)  Loss=1.5172  cls_loss=0.9568  reg_loss=0.5604  lr_det=2.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1361s  iter_time=2.259s
2025-08-05 13:30:08 Train INFO: [Train]: [000][00016/00051] (32.7%)  Loss=1.5084  cls_loss=0.9578  reg_loss=0.5506  lr_det=3.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1513s  iter_time=129.972s
2025-08-05 13:30:10 Train INFO: [Train]: [000][00017/00051] (34.6%)  Loss=1.4946  cls_loss=0.9520  reg_loss=0.5426  lr_det=3.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1393s  iter_time=2.357s
2025-08-05 13:30:13 Train INFO: [Train]: [000][00018/00051] (36.5%)  Loss=1.4830  cls_loss=0.9505  reg_loss=0.5326  lr_det=3.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1284s  iter_time=2.278s
2025-08-05 13:30:15 Train INFO: [Train]: [000][00019/00051] (38.5%)  Loss=1.4760  cls_loss=0.9498  reg_loss=0.5262  lr_det=3.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1187s  iter_time=2.245s
2025-08-05 13:32:30 Train INFO: [Train]: [000][00020/00051] (40.4%)  Loss=1.4529  cls_loss=0.9387  reg_loss=0.5142  lr_det=3.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1295s  iter_time=135.161s
2025-08-05 13:32:32 Train INFO: [Train]: [000][00021/00051] (42.3%)  Loss=1.4510  cls_loss=0.9414  reg_loss=0.5097  lr_det=4.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1199s  iter_time=2.342s
2025-08-05 13:32:35 Train INFO: [Train]: [000][00022/00051] (44.2%)  Loss=1.4526  cls_loss=0.9478  reg_loss=0.5048  lr_det=4.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1112s  iter_time=2.313s
2025-08-05 13:32:37 Train INFO: [Train]: [000][00023/00051] (46.2%)  Loss=1.4300  cls_loss=0.9358  reg_loss=0.4941  lr_det=4.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1031s  iter_time=2.190s
2025-08-05 13:34:56 Train INFO: [Train]: [000][00024/00051] (48.1%)  Loss=1.4259  cls_loss=0.9373  reg_loss=0.4886  lr_det=4.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1105s  iter_time=139.057s
2025-08-05 13:34:58 Train INFO: [Train]: [000][00025/00051] (50.0%)  Loss=1.4197  cls_loss=0.9361  reg_loss=0.4837  lr_det=4.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1025s  iter_time=2.316s
2025-08-05 13:35:00 Train INFO: [Train]: [000][00026/00051] (51.9%)  Loss=1.4176  cls_loss=0.9375  reg_loss=0.4802  lr_det=5.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=951s  iter_time=2.279s
2025-08-05 13:35:03 Train INFO: [Train]: [000][00027/00051] (53.8%)  Loss=1.4201  cls_loss=0.9414  reg_loss=0.4787  lr_det=5.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=883s  iter_time=2.300s
2025-08-05 13:37:17 Train INFO: [Train]: [000][00028/00051] (55.8%)  Loss=1.4086  cls_loss=0.9357  reg_loss=0.4729  lr_det=5.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=923s  iter_time=134.470s
2025-08-05 13:37:24 Train INFO: [Train]: [000][00029/00051] (57.7%)  Loss=1.4103  cls_loss=0.9396  reg_loss=0.4707  lr_det=5.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=859s  iter_time=6.637s
2025-08-05 13:37:26 Train INFO: [Train]: [000][00030/00051] (59.6%)  Loss=1.4105  cls_loss=0.9427  reg_loss=0.4678  lr_det=5.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=795s  iter_time=2.339s
2025-08-05 13:37:29 Train INFO: [Train]: [000][00031/00051] (61.5%)  Loss=1.4017  cls_loss=0.9389  reg_loss=0.4629  lr_det=6.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=735s  iter_time=2.314s
2025-08-05 13:39:40 Train INFO: [Train]: [000][00032/00051] (63.5%)  Loss=1.3969  cls_loss=0.9376  reg_loss=0.4593  lr_det=6.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=752s  iter_time=130.991s
2025-08-05 13:39:44 Train INFO: [Train]: [000][00033/00051] (65.4%)  Loss=1.3867  cls_loss=0.9325  reg_loss=0.4542  lr_det=6.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=694s  iter_time=4.364s
2025-08-05 13:39:46 Train INFO: [Train]: [000][00034/00051] (67.3%)  Loss=1.3935  cls_loss=0.9390  reg_loss=0.4545  lr_det=6.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=638s  iter_time=2.216s
2025-08-05 13:39:48 Train INFO: [Train]: [000][00035/00051] (69.2%)  Loss=1.3860  cls_loss=0.9357  reg_loss=0.4502  lr_det=6.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=585s  iter_time=2.317s
2025-08-05 13:42:07 Train INFO: [Train]: [000][00036/00051] (71.2%)  Loss=1.3854  cls_loss=0.9367  reg_loss=0.4487  lr_det=7.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=589s  iter_time=138.102s
2025-08-05 13:42:09 Train INFO: [Train]: [000][00037/00051] (73.1%)  Loss=1.3860  cls_loss=0.9383  reg_loss=0.4477  lr_det=7.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=536s  iter_time=2.277s
2025-08-05 13:42:11 Train INFO: [Train]: [000][00038/00051] (75.0%)  Loss=1.3858  cls_loss=0.9392  reg_loss=0.4466  lr_det=7.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=486s  iter_time=2.334s
2025-08-05 13:42:13 Train INFO: [Train]: [000][00039/00051] (76.9%)  Loss=1.3859  cls_loss=0.9408  reg_loss=0.4452  lr_det=7.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=438s  iter_time=2.281s
2025-08-05 13:44:28 Train INFO: [Train]: [000][00040/00051] (78.8%)  Loss=1.3835  cls_loss=0.9409  reg_loss=0.4426  lr_det=7.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=428s  iter_time=134.861s
2025-08-05 13:44:31 Train INFO: [Train]: [000][00041/00051] (80.8%)  Loss=1.3783  cls_loss=0.9384  reg_loss=0.4400  lr_det=8.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=380s  iter_time=2.459s
2025-08-05 13:44:33 Train INFO: [Train]: [000][00042/00051] (82.7%)  Loss=1.3761  cls_loss=0.9382  reg_loss=0.4379  lr_det=8.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=335s  iter_time=2.279s
2025-08-05 13:44:35 Train INFO: [Train]: [000][00043/00051] (84.6%)  Loss=1.3736  cls_loss=0.9375  reg_loss=0.4361  lr_det=8.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=291s  iter_time=2.200s
2025-08-05 13:46:53 Train INFO: [Train]: [000][00044/00051] (86.5%)  Loss=1.3740  cls_loss=0.9382  reg_loss=0.4357  lr_det=8.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=271s  iter_time=138.207s
2025-08-05 13:46:56 Train INFO: [Train]: [000][00045/00051] (88.5%)  Loss=1.3680  cls_loss=0.9349  reg_loss=0.4331  lr_det=8.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=227s  iter_time=2.289s
2025-08-05 13:46:58 Train INFO: [Train]: [000][00046/00051] (90.4%)  Loss=1.3627  cls_loss=0.9321  reg_loss=0.4306  lr_det=9.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=186s  iter_time=2.357s
2025-08-05 13:47:00 Train INFO: [Train]: [000][00047/00051] (92.3%)  Loss=1.3578  cls_loss=0.9294  reg_loss=0.4283  lr_det=9.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=146s  iter_time=2.271s
2025-08-05 13:49:14 Train INFO: [Train]: [000][00048/00051] (94.2%)  Loss=1.3521  cls_loss=0.9264  reg_loss=0.4256  lr_det=9.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=115s  iter_time=133.491s
2025-08-05 13:49:16 Train INFO: [Train]: [000][00049/00051] (96.2%)  Loss=1.3513  cls_loss=0.9272  reg_loss=0.4241  lr_det=9.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=75s  iter_time=2.148s
2025-08-05 13:49:18 Train INFO: [Train]: [000][00050/00051] (98.1%)  Loss=1.3510  cls_loss=0.9278  reg_loss=0.4232  lr_det=9.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=37s  iter_time=2.156s
2025-08-05 13:49:23 Train INFO: [Train]: [000][00051/00051] (100.0%)  Loss=1.3476  cls_loss=0.9264  reg_loss=0.4213  lr_det=1.0e-04  GPU=1393MB(alloc)/4592MB(reserved)/9292MB(max)  ETA=0s  iter_time=4.424s
2025-08-05 13:49:23 Train INFO: [Train]: Epoch 0 completed in 1890.3s (avg 36.351s/iter)
2025-08-05 13:49:23 Train INFO: [Train]: Final Loss=1.3476
2025-08-05 13:49:24 Train INFO: Checkpoint saved at epoch 0
2025-08-05 13:49:24 Train INFO: [Train]: Epoch 1 started (Total iterations: 52)
2025-08-05 13:52:07 Train INFO: [Train]: [001][00001/00051] (3.8%)  Loss=1.2505  cls_loss=0.8965  reg_loss=0.3540  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=4069s  iter_time=162.759s  fwd=2.107s/bwd=0.037s/opt=0.013s
2025-08-05 13:52:09 Train INFO: [Train]: [001][00002/00051] (5.8%)  Loss=1.3133  cls_loss=0.9343  reg_loss=0.3790  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2696s  iter_time=2.273s
2025-08-05 13:52:11 Train INFO: [Train]: [001][00003/00051] (7.7%)  Loss=1.2953  cls_loss=0.9264  reg_loss=0.3689  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2008s  iter_time=2.274s
2025-08-05 13:54:29 Train INFO: [Train]: [001][00004/00051] (9.6%)  Loss=1.2818  cls_loss=0.9136  reg_loss=0.3682  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2863s  iter_time=137.232s
2025-08-05 13:54:31 Train INFO: [Train]: [001][00005/00051] (11.5%)  Loss=1.3192  cls_loss=0.9430  reg_loss=0.3762  lr_det=9.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2352s  iter_time=2.264s
2025-08-05 13:54:33 Train INFO: [Train]: [001][00006/00051] (13.5%)  Loss=1.3310  cls_loss=0.9521  reg_loss=0.3789  lr_det=9.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1987s  iter_time=2.287s
2025-08-05 13:54:35 Train INFO: [Train]: [001][00007/00051] (15.4%)  Loss=1.3478  cls_loss=0.9604  reg_loss=0.3875  lr_det=9.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1712s  iter_time=2.244s
2025-08-05 13:56:46 Train INFO: [Train]: [001][00008/00051] (17.3%)  Loss=1.3465  cls_loss=0.9595  reg_loss=0.3870  lr_det=9.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2113s  iter_time=130.980s
2025-08-05 13:56:55 Train INFO: [Train]: [001][00009/00051] (19.2%)  Loss=1.3337  cls_loss=0.9532  reg_loss=0.3806  lr_det=9.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1895s  iter_time=8.835s
2025-08-05 13:56:58 Train INFO: [Train]: [001][00010/00051] (21.2%)  Loss=1.3144  cls_loss=0.9424  reg_loss=0.3719  lr_det=9.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1690s  iter_time=2.333s
2025-08-05 13:57:00 Train INFO: [Train]: [001][00011/00051] (23.1%)  Loss=1.3227  cls_loss=0.9484  reg_loss=0.3743  lr_det=9.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1519s  iter_time=2.335s
2025-08-05 13:59:10 Train INFO: [Train]: [001][00012/00051] (25.0%)  Loss=1.3264  cls_loss=0.9518  reg_loss=0.3745  lr_det=9.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1756s  iter_time=129.629s
2025-08-05 13:59:20 Train INFO: [Train]: [001][00013/00051] (26.9%)  Loss=1.3210  cls_loss=0.9491  reg_loss=0.3719  lr_det=9.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1619s  iter_time=10.864s
2025-08-05 13:59:23 Train INFO: [Train]: [001][00014/00051] (28.8%)  Loss=1.3268  cls_loss=0.9527  reg_loss=0.3741  lr_det=9.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1477s  iter_time=2.273s
2025-08-05 13:59:25 Train INFO: [Train]: [001][00015/00051] (30.8%)  Loss=1.3229  cls_loss=0.9499  reg_loss=0.3730  lr_det=9.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1352s  iter_time=2.222s
2025-08-05 14:01:36 Train INFO: [Train]: [001][00016/00051] (32.7%)  Loss=1.3128  cls_loss=0.9434  reg_loss=0.3694  lr_det=9.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1506s  iter_time=130.595s
2025-08-05 14:01:38 Train INFO: [Train]: [001][00017/00051] (34.6%)  Loss=1.3055  cls_loss=0.9376  reg_loss=0.3679  lr_det=9.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1386s  iter_time=2.264s
2025-08-05 14:01:40 Train INFO: [Train]: [001][00018/00051] (36.5%)  Loss=1.3017  cls_loss=0.9355  reg_loss=0.3662  lr_det=9.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1278s  iter_time=2.315s
2025-08-05 14:01:42 Train INFO: [Train]: [001][00019/00051] (38.5%)  Loss=1.3014  cls_loss=0.9350  reg_loss=0.3664  lr_det=9.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1181s  iter_time=2.245s
2025-08-05 14:03:53 Train INFO: [Train]: [001][00020/00051] (40.4%)  Loss=1.2941  cls_loss=0.9302  reg_loss=0.3639  lr_det=9.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1282s  iter_time=130.258s
2025-08-05 14:03:55 Train INFO: [Train]: [001][00021/00051] (42.3%)  Loss=1.2920  cls_loss=0.9284  reg_loss=0.3636  lr_det=9.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1187s  iter_time=2.322s
2025-08-05 14:03:57 Train INFO: [Train]: [001][00022/00051] (44.2%)  Loss=1.3056  cls_loss=0.9380  reg_loss=0.3677  lr_det=8.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1101s  iter_time=2.245s
2025-08-05 14:03:59 Train INFO: [Train]: [001][00023/00051] (46.2%)  Loss=1.2989  cls_loss=0.9334  reg_loss=0.3655  lr_det=8.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1021s  iter_time=2.259s
2025-08-05 14:06:13 Train INFO: [Train]: [001][00024/00051] (48.1%)  Loss=1.3043  cls_loss=0.9377  reg_loss=0.3666  lr_det=8.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1089s  iter_time=133.217s
2025-08-05 14:06:15 Train INFO: [Train]: [001][00025/00051] (50.0%)  Loss=1.3054  cls_loss=0.9376  reg_loss=0.3678  lr_det=8.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1011s  iter_time=2.293s
2025-08-05 14:06:18 Train INFO: [Train]: [001][00026/00051] (51.9%)  Loss=1.3154  cls_loss=0.9446  reg_loss=0.3709  lr_det=8.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=939s  iter_time=3.168s
2025-08-05 14:06:20 Train INFO: [Train]: [001][00027/00051] (53.8%)  Loss=1.3144  cls_loss=0.9440  reg_loss=0.3704  lr_det=8.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=871s  iter_time=2.319s
2025-08-05 14:08:27 Train INFO: [Train]: [001][00028/00051] (55.8%)  Loss=1.3069  cls_loss=0.9389  reg_loss=0.3680  lr_det=8.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=907s  iter_time=126.786s
2025-08-05 14:08:33 Train INFO: [Train]: [001][00029/00051] (57.7%)  Loss=1.3046  cls_loss=0.9369  reg_loss=0.3676  lr_det=8.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=842s  iter_time=5.469s
2025-08-05 14:08:35 Train INFO: [Train]: [001][00030/00051] (59.6%)  Loss=1.2972  cls_loss=0.9321  reg_loss=0.3651  lr_det=8.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=780s  iter_time=2.286s
2025-08-05 14:08:37 Train INFO: [Train]: [001][00031/00051] (61.5%)  Loss=1.2910  cls_loss=0.9278  reg_loss=0.3632  lr_det=8.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=721s  iter_time=2.278s
2025-08-05 14:10:49 Train INFO: [Train]: [001][00032/00051] (63.5%)  Loss=1.2958  cls_loss=0.9304  reg_loss=0.3654  lr_det=7.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=740s  iter_time=131.689s
2025-08-05 14:10:52 Train INFO: [Train]: [001][00033/00051] (65.4%)  Loss=1.2907  cls_loss=0.9267  reg_loss=0.3641  lr_det=7.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=682s  iter_time=2.588s
2025-08-05 14:10:56 Train INFO: [Train]: [001][00034/00051] (67.3%)  Loss=1.2971  cls_loss=0.9316  reg_loss=0.3655  lr_det=7.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=628s  iter_time=4.879s
2025-08-05 14:10:59 Train INFO: [Train]: [001][00035/00051] (69.2%)  Loss=1.2984  cls_loss=0.9324  reg_loss=0.3660  lr_det=7.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=575s  iter_time=2.325s
2025-08-05 14:13:07 Train INFO: [Train]: [001][00036/00051] (71.2%)  Loss=1.3012  cls_loss=0.9337  reg_loss=0.3675  lr_det=7.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=577s  iter_time=128.151s
2025-08-05 14:13:09 Train INFO: [Train]: [001][00037/00051] (73.1%)  Loss=1.2985  cls_loss=0.9313  reg_loss=0.3671  lr_det=7.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=525s  iter_time=2.366s
2025-08-05 14:13:15 Train INFO: [Train]: [001][00038/00051] (75.0%)  Loss=1.3011  cls_loss=0.9324  reg_loss=0.3687  lr_det=7.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=477s  iter_time=5.418s
2025-08-05 14:13:17 Train INFO: [Train]: [001][00039/00051] (76.9%)  Loss=1.2997  cls_loss=0.9306  reg_loss=0.3691  lr_det=6.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=430s  iter_time=2.328s
2025-08-05 14:15:26 Train INFO: [Train]: [001][00040/00051] (78.8%)  Loss=1.2963  cls_loss=0.9282  reg_loss=0.3681  lr_det=6.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=419s  iter_time=128.531s
2025-08-05 14:15:28 Train INFO: [Train]: [001][00041/00051] (80.8%)  Loss=1.2951  cls_loss=0.9276  reg_loss=0.3676  lr_det=6.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=372s  iter_time=2.286s
2025-08-05 14:15:31 Train INFO: [Train]: [001][00042/00051] (82.7%)  Loss=1.2957  cls_loss=0.9284  reg_loss=0.3673  lr_det=6.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=328s  iter_time=3.554s
2025-08-05 14:15:34 Train INFO: [Train]: [001][00043/00051] (84.6%)  Loss=1.2899  cls_loss=0.9240  reg_loss=0.3659  lr_det=6.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=285s  iter_time=2.355s
2025-08-05 14:17:45 Train INFO: [Train]: [001][00044/00051] (86.5%)  Loss=1.2838  cls_loss=0.9194  reg_loss=0.3644  lr_det=6.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=265s  iter_time=131.449s
2025-08-05 14:17:48 Train INFO: [Train]: [001][00045/00051] (88.5%)  Loss=1.2835  cls_loss=0.9187  reg_loss=0.3649  lr_det=6.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=222s  iter_time=2.327s
2025-08-05 14:17:50 Train INFO: [Train]: [001][00046/00051] (90.4%)  Loss=1.2830  cls_loss=0.9178  reg_loss=0.3652  lr_det=5.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=181s  iter_time=2.336s
2025-08-05 14:17:52 Train INFO: [Train]: [001][00047/00051] (92.3%)  Loss=1.2807  cls_loss=0.9163  reg_loss=0.3644  lr_det=5.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=142s  iter_time=2.324s
2025-08-05 14:19:58 Train INFO: [Train]: [001][00048/00051] (94.2%)  Loss=1.2780  cls_loss=0.9143  reg_loss=0.3636  lr_det=5.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=112s  iter_time=126.054s
2025-08-05 14:20:00 Train INFO: [Train]: [001][00049/00051] (96.2%)  Loss=1.2769  cls_loss=0.9138  reg_loss=0.3631  lr_det=5.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=73s  iter_time=2.165s
2025-08-05 14:20:03 Train INFO: [Train]: [001][00050/00051] (98.1%)  Loss=1.2745  cls_loss=0.9118  reg_loss=0.3627  lr_det=5.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=36s  iter_time=2.178s
2025-08-05 14:20:05 Train INFO: [Train]: [001][00051/00051] (100.0%)  Loss=1.2717  cls_loss=0.9094  reg_loss=0.3623  lr_det=5.2e-05  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.048s
2025-08-05 14:20:05 Train INFO: [Train]: Epoch 1 completed in 1841.2s (avg 35.408s/iter)
2025-08-05 14:20:05 Train INFO: [Train]: Final Loss=1.2717
2025-08-05 14:20:05 Train INFO: [Val]: Epoch 1 Loss
2025-08-05 14:38:47 Train INFO: [Val]: [001]  Loss=1.8133  cls_loss=1.1294  reg_loss=0.6839  Average-mAP=0.01%
2025-08-05 14:38:48 Train INFO: Checkpoint saved at epoch 1
2025-08-05 14:38:48 Train INFO: [Train]: Epoch 2 started (Total iterations: 52)
2025-08-05 14:41:24 Train INFO: [Train]: [002][00001/00051] (3.8%)  Loss=1.0512  cls_loss=0.7512  reg_loss=0.3000  lr_det=4.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=3883s  iter_time=155.313s  fwd=2.152s/bwd=0.058s/opt=0.008s
2025-08-05 14:41:26 Train INFO: [Train]: [002][00002/00051] (5.8%)  Loss=1.1562  cls_loss=0.8108  reg_loss=0.3454  lr_det=4.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2576s  iter_time=2.420s
2025-08-05 14:41:29 Train INFO: [Train]: [002][00003/00051] (7.7%)  Loss=1.1381  cls_loss=0.8030  reg_loss=0.3351  lr_det=4.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1921s  iter_time=2.354s
2025-08-05 14:43:40 Train INFO: [Train]: [002][00004/00051] (9.6%)  Loss=1.1202  cls_loss=0.7925  reg_loss=0.3276  lr_det=4.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2737s  iter_time=131.126s
2025-08-05 14:43:44 Train INFO: [Train]: [002][00005/00051] (11.5%)  Loss=1.1472  cls_loss=0.8090  reg_loss=0.3383  lr_det=4.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2267s  iter_time=4.505s
2025-08-05 14:43:47 Train INFO: [Train]: [002][00006/00051] (13.5%)  Loss=1.1773  cls_loss=0.8303  reg_loss=0.3470  lr_det=4.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1916s  iter_time=2.363s
2025-08-05 14:43:49 Train INFO: [Train]: [002][00007/00051] (15.4%)  Loss=1.1935  cls_loss=0.8440  reg_loss=0.3495  lr_det=4.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1652s  iter_time=2.314s
2025-08-05 14:45:59 Train INFO: [Train]: [002][00008/00051] (17.3%)  Loss=1.1849  cls_loss=0.8366  reg_loss=0.3484  lr_det=3.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=2057s  iter_time=130.037s
2025-08-05 14:46:08 Train INFO: [Train]: [002][00009/00051] (19.2%)  Loss=1.1807  cls_loss=0.8353  reg_loss=0.3454  lr_det=3.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1844s  iter_time=8.700s
2025-08-05 14:46:11 Train INFO: [Train]: [002][00010/00051] (21.2%)  Loss=1.1948  cls_loss=0.8421  reg_loss=0.3527  lr_det=3.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1651s  iter_time=3.765s
2025-08-05 14:46:14 Train INFO: [Train]: [002][00011/00051] (23.1%)  Loss=1.2026  cls_loss=0.8457  reg_loss=0.3569  lr_det=3.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1484s  iter_time=2.360s
2025-08-05 14:48:18 Train INFO: [Train]: [002][00012/00051] (25.0%)  Loss=1.1892  cls_loss=0.8366  reg_loss=0.3526  lr_det=3.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1709s  iter_time=124.367s
2025-08-05 14:48:28 Train INFO: [Train]: [002][00013/00051] (26.9%)  Loss=1.1837  cls_loss=0.8323  reg_loss=0.3514  lr_det=3.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1573s  iter_time=9.936s
2025-08-05 14:48:32 Train INFO: [Train]: [002][00014/00051] (28.8%)  Loss=1.1786  cls_loss=0.8285  reg_loss=0.3501  lr_det=2.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1439s  iter_time=3.677s
2025-08-05 14:48:34 Train INFO: [Train]: [002][00015/00051] (30.8%)  Loss=1.1731  cls_loss=0.8240  reg_loss=0.3491  lr_det=2.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1317s  iter_time=2.275s
2025-08-05 14:50:50 Train INFO: [Train]: [002][00016/00051] (32.7%)  Loss=1.1629  cls_loss=0.8155  reg_loss=0.3474  lr_det=2.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1485s  iter_time=135.844s
2025-08-05 14:50:52 Train INFO: [Train]: [002][00017/00051] (34.6%)  Loss=1.1655  cls_loss=0.8158  reg_loss=0.3497  lr_det=2.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1367s  iter_time=2.453s
2025-08-05 14:50:56 Train INFO: [Train]: [002][00018/00051] (36.5%)  Loss=1.1602  cls_loss=0.8117  reg_loss=0.3485  lr_det=2.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1263s  iter_time=3.356s
2025-08-05 14:50:58 Train INFO: [Train]: [002][00019/00051] (38.5%)  Loss=1.1674  cls_loss=0.8163  reg_loss=0.3511  lr_det=2.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1167s  iter_time=2.454s
2025-08-05 14:53:32 Train INFO: [Train]: [002][00020/00051] (40.4%)  Loss=1.1602  cls_loss=0.8106  reg_loss=0.3496  lr_det=2.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1304s  iter_time=153.728s
2025-08-05 14:53:34 Train INFO: [Train]: [002][00021/00051] (42.3%)  Loss=1.1604  cls_loss=0.8089  reg_loss=0.3514  lr_det=2.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1208s  iter_time=2.420s
2025-08-05 14:53:38 Train INFO: [Train]: [002][00022/00051] (44.2%)  Loss=1.1732  cls_loss=0.8160  reg_loss=0.3572  lr_det=1.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1121s  iter_time=3.584s
2025-08-05 14:53:40 Train INFO: [Train]: [002][00023/00051] (46.2%)  Loss=1.1623  cls_loss=0.8084  reg_loss=0.3539  lr_det=1.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1040s  iter_time=2.379s
2025-08-05 14:56:19 Train INFO: [Train]: [002][00024/00051] (48.1%)  Loss=1.1655  cls_loss=0.8108  reg_loss=0.3546  lr_det=1.7e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1135s  iter_time=158.996s
2025-08-05 14:56:22 Train INFO: [Train]: [002][00025/00051] (50.0%)  Loss=1.1667  cls_loss=0.8113  reg_loss=0.3554  lr_det=1.6e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1053s  iter_time=2.483s
2025-08-05 14:56:24 Train INFO: [Train]: [002][00026/00051] (51.9%)  Loss=1.1695  cls_loss=0.8126  reg_loss=0.3569  lr_det=1.5e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=978s  iter_time=2.485s
2025-08-05 14:56:27 Train INFO: [Train]: [002][00027/00051] (53.8%)  Loss=1.1692  cls_loss=0.8120  reg_loss=0.3572  lr_det=1.4e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=907s  iter_time=2.420s
2025-08-05 14:58:46 Train INFO: [Train]: [002][00028/00051] (55.8%)  Loss=1.1595  cls_loss=0.8047  reg_loss=0.3548  lr_det=1.3e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=949s  iter_time=139.003s
2025-08-05 14:58:52 Train INFO: [Train]: [002][00029/00051] (57.7%)  Loss=1.1601  cls_loss=0.8045  reg_loss=0.3556  lr_det=1.2e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=883s  iter_time=6.337s
2025-08-05 14:58:54 Train INFO: [Train]: [002][00030/00051] (59.6%)  Loss=1.1591  cls_loss=0.8027  reg_loss=0.3564  lr_det=1.1e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=817s  iter_time=2.300s
2025-08-05 14:58:57 Train INFO: [Train]: [002][00031/00051] (61.5%)  Loss=1.1501  cls_loss=0.7963  reg_loss=0.3538  lr_det=9.7e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=755s  iter_time=2.372s
2025-08-05 15:01:03 Train INFO: [Train]: [002][00032/00051] (63.5%)  Loss=1.1461  cls_loss=0.7938  reg_loss=0.3523  lr_det=8.9e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=768s  iter_time=126.568s
2025-08-05 15:01:09 Train INFO: [Train]: [002][00033/00051] (65.4%)  Loss=1.1448  cls_loss=0.7923  reg_loss=0.3525  lr_det=8.0e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=710s  iter_time=6.254s
2025-08-05 15:01:12 Train INFO: [Train]: [002][00034/00051] (67.3%)  Loss=1.1497  cls_loss=0.7950  reg_loss=0.3548  lr_det=7.2e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=653s  iter_time=2.443s
2025-08-05 15:01:14 Train INFO: [Train]: [002][00035/00051] (69.2%)  Loss=1.1440  cls_loss=0.7909  reg_loss=0.3531  lr_det=6.5e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=598s  iter_time=2.402s
2025-08-05 15:03:25 Train INFO: [Train]: [002][00036/00051] (71.2%)  Loss=1.1420  cls_loss=0.7890  reg_loss=0.3530  lr_det=5.7e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=598s  iter_time=130.424s
2025-08-05 15:03:27 Train INFO: [Train]: [002][00037/00051] (73.1%)  Loss=1.1347  cls_loss=0.7836  reg_loss=0.3511  lr_det=5.1e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=545s  iter_time=2.261s
2025-08-05 15:03:32 Train INFO: [Train]: [002][00038/00051] (75.0%)  Loss=1.1366  cls_loss=0.7842  reg_loss=0.3523  lr_det=4.4e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=495s  iter_time=5.176s
2025-08-05 15:03:34 Train INFO: [Train]: [002][00039/00051] (76.9%)  Loss=1.1353  cls_loss=0.7829  reg_loss=0.3524  lr_det=3.8e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=446s  iter_time=2.285s
2025-08-05 15:06:01 Train INFO: [Train]: [002][00040/00051] (78.8%)  Loss=1.1324  cls_loss=0.7810  reg_loss=0.3514  lr_det=3.3e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=438s  iter_time=146.700s
2025-08-05 15:06:04 Train INFO: [Train]: [002][00041/00051] (80.8%)  Loss=1.1331  cls_loss=0.7814  reg_loss=0.3517  lr_det=2.7e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=389s  iter_time=2.462s
2025-08-05 15:06:06 Train INFO: [Train]: [002][00042/00051] (82.7%)  Loss=1.1345  cls_loss=0.7824  reg_loss=0.3521  lr_det=2.3e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=343s  iter_time=2.433s
2025-08-05 15:06:08 Train INFO: [Train]: [002][00043/00051] (84.6%)  Loss=1.1371  cls_loss=0.7837  reg_loss=0.3535  lr_det=1.8e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=298s  iter_time=2.425s
2025-08-05 15:08:39 Train INFO: [Train]: [002][00044/00051] (86.5%)  Loss=1.1366  cls_loss=0.7827  reg_loss=0.3539  lr_det=1.5e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=279s  iter_time=150.539s
2025-08-05 15:08:41 Train INFO: [Train]: [002][00045/00051] (88.5%)  Loss=1.1323  cls_loss=0.7795  reg_loss=0.3528  lr_det=1.1e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=234s  iter_time=2.322s
2025-08-05 15:08:44 Train INFO: [Train]: [002][00046/00051] (90.4%)  Loss=1.1332  cls_loss=0.7789  reg_loss=0.3543  lr_det=8.3e-07  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=191s  iter_time=2.560s
2025-08-05 15:08:46 Train INFO: [Train]: [002][00047/00051] (92.3%)  Loss=1.1316  cls_loss=0.7777  reg_loss=0.3539  lr_det=5.8e-07  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=150s  iter_time=2.380s
2025-08-05 15:11:00 Train INFO: [Train]: [002][00048/00051] (94.2%)  Loss=1.1313  cls_loss=0.7774  reg_loss=0.3539  lr_det=3.7e-07  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=118s  iter_time=133.710s
2025-08-05 15:11:02 Train INFO: [Train]: [002][00049/00051] (96.2%)  Loss=1.1297  cls_loss=0.7765  reg_loss=0.3532  lr_det=2.2e-07  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=77s  iter_time=2.186s
2025-08-05 15:11:04 Train INFO: [Train]: [002][00050/00051] (98.1%)  Loss=1.1292  cls_loss=0.7757  reg_loss=0.3535  lr_det=1.0e-07  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=38s  iter_time=2.196s
2025-08-05 15:11:06 Train INFO: [Train]: [002][00051/00051] (100.0%)  Loss=1.1291  cls_loss=0.7754  reg_loss=0.3537  lr_det=3.3e-08  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.059s
2025-08-05 15:11:07 Train INFO: [Train]: Epoch 2 completed in 1938.7s (avg 37.283s/iter)
2025-08-05 15:11:07 Train INFO: [Train]: Final Loss=1.1291
2025-08-05 15:11:07 Train INFO: [Val]: Epoch 2 Loss
2025-08-05 15:30:08 Train INFO: [Val]: [002]  Loss=1.6613  cls_loss=1.1043  reg_loss=0.5570  Average-mAP=0.02%
2025-08-05 15:30:10 Train INFO: Checkpoint saved at epoch 2
2025-08-05 15:30:10 Train INFO: Training Over...

2025-08-05 15:37:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 15:37:25 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=3, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=1,
    end_epoch=3,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=1,
    val_loss_interval=1,
    val_start_epoch=1)

2025-08-05 15:37:25 Train INFO: training subset: 831 videos
2025-08-05 15:37:25 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 15:37:25 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 15:37:26 Train INFO: Using single GPU training...
2025-08-05 15:37:26 Train INFO: Using Model EMA...
2025-08-05 15:37:26 Train INFO: Using Automatic Mixed Precision...
2025-08-05 15:37:26 Train INFO: GPU Memory: 24.0 GB
2025-08-05 15:37:26 Train INFO: Freeze the backbone...
2025-08-05 15:37:26 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_2.pth
2025-08-05 15:37:26 Train INFO: Resume epoch is 2
2025-08-05 15:37:26 Train INFO: Training Starts...

2025-08-05 15:37:26 Train INFO: Training Over...

2025-08-05 15:38:12 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-05 15:38:13 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-05 15:38:13 Train INFO: training subset: 831 videos
2025-08-05 15:38:13 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-05 15:38:13 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-05 15:38:14 Train INFO: Using single GPU training...
2025-08-05 15:38:14 Train INFO: Using Model EMA...
2025-08-05 15:38:14 Train INFO: Using Automatic Mixed Precision...
2025-08-05 15:38:14 Train INFO: GPU Memory: 24.0 GB
2025-08-05 15:38:14 Train INFO: Freeze the backbone...
2025-08-05 15:38:14 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_2.pth
2025-08-05 15:38:14 Train INFO: Resume epoch is 2
2025-08-05 15:38:14 Train INFO: Training Starts...

2025-08-05 15:38:14 Train INFO: [Train]: Epoch 3 started (Total iterations: 52)
2025-08-05 15:41:01 Train INFO: [Train]: [003][00001/00051] (3.8%)  Loss=0.9716  cls_loss=0.6569  reg_loss=0.3147  lr_det=3.3e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=4158s  iter_time=166.320s  fwd=2.347s/bwd=0.055s/opt=0.013s
2025-08-05 15:41:03 Train INFO: [Train]: [003][00002/00051] (5.8%)  Loss=1.0755  cls_loss=0.7210  reg_loss=0.3545  lr_det=1.0e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2755s  iter_time=2.372s
2025-08-05 15:41:05 Train INFO: [Train]: [003][00003/00051] (7.7%)  Loss=1.0932  cls_loss=0.7386  reg_loss=0.3547  lr_det=2.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2053s  iter_time=2.356s
2025-08-05 15:43:17 Train INFO: [Train]: [003][00004/00051] (9.6%)  Loss=1.0605  cls_loss=0.7174  reg_loss=0.3430  lr_det=3.7e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2846s  iter_time=131.707s
2025-08-05 15:43:25 Train INFO: [Train]: [003][00005/00051] (11.5%)  Loss=1.1108  cls_loss=0.7481  reg_loss=0.3627  lr_det=5.8e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2380s  iter_time=7.658s
2025-08-05 15:43:27 Train INFO: [Train]: [003][00006/00051] (13.5%)  Loss=1.1296  cls_loss=0.7636  reg_loss=0.3660  lr_det=8.3e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2012s  iter_time=2.490s
2025-08-05 15:43:30 Train INFO: [Train]: [003][00007/00051] (15.4%)  Loss=1.1414  cls_loss=0.7732  reg_loss=0.3682  lr_det=1.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1735s  iter_time=2.524s
2025-08-05 15:45:35 Train INFO: [Train]: [003][00008/00051] (17.3%)  Loss=1.1287  cls_loss=0.7634  reg_loss=0.3653  lr_det=1.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2107s  iter_time=125.558s
2025-08-05 15:45:52 Train INFO: [Train]: [003][00009/00051] (19.2%)  Loss=1.1268  cls_loss=0.7594  reg_loss=0.3674  lr_det=1.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1921s  iter_time=16.413s
2025-08-05 15:45:54 Train INFO: [Train]: [003][00010/00051] (21.2%)  Loss=1.1268  cls_loss=0.7614  reg_loss=0.3654  lr_det=2.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1713s  iter_time=2.309s
2025-08-05 15:45:56 Train INFO: [Train]: [003][00011/00051] (23.1%)  Loss=1.1439  cls_loss=0.7746  reg_loss=0.3692  lr_det=2.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1540s  iter_time=2.304s
2025-08-05 15:47:55 Train INFO: [Train]: [003][00012/00051] (25.0%)  Loss=1.1407  cls_loss=0.7745  reg_loss=0.3663  lr_det=3.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1742s  iter_time=118.658s
2025-08-05 15:48:14 Train INFO: [Train]: [003][00013/00051] (26.9%)  Loss=1.1442  cls_loss=0.7779  reg_loss=0.3663  lr_det=3.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1629s  iter_time=19.473s
2025-08-05 15:48:17 Train INFO: [Train]: [003][00014/00051] (28.8%)  Loss=1.1499  cls_loss=0.7809  reg_loss=0.3690  lr_det=4.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1486s  iter_time=2.373s
2025-08-05 15:48:19 Train INFO: [Train]: [003][00015/00051] (30.8%)  Loss=1.1369  cls_loss=0.7721  reg_loss=0.3648  lr_det=5.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1361s  iter_time=2.296s
2025-08-05 15:50:20 Train INFO: [Train]: [003][00016/00051] (32.7%)  Loss=1.1356  cls_loss=0.7706  reg_loss=0.3650  lr_det=5.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1495s  iter_time=121.087s
2025-08-05 15:50:33 Train INFO: [Train]: [003][00017/00051] (34.6%)  Loss=1.1284  cls_loss=0.7647  reg_loss=0.3637  lr_det=6.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1395s  iter_time=12.745s
2025-08-05 15:50:35 Train INFO: [Train]: [003][00018/00051] (36.5%)  Loss=1.1264  cls_loss=0.7647  reg_loss=0.3616  lr_det=7.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1287s  iter_time=2.475s
2025-08-05 15:50:38 Train INFO: [Train]: [003][00019/00051] (38.5%)  Loss=1.1322  cls_loss=0.7679  reg_loss=0.3643  lr_det=8.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1190s  iter_time=2.332s
2025-08-05 15:52:41 Train INFO: [Train]: [003][00020/00051] (40.4%)  Loss=1.1236  cls_loss=0.7616  reg_loss=0.3620  lr_det=8.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1280s  iter_time=123.506s
2025-08-05 15:52:50 Train INFO: [Train]: [003][00021/00051] (42.3%)  Loss=1.1220  cls_loss=0.7609  reg_loss=0.3611  lr_det=9.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1195s  iter_time=9.181s
2025-08-05 15:52:58 Train INFO: [Train]: [003][00022/00051] (44.2%)  Loss=1.1353  cls_loss=0.7694  reg_loss=0.3659  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1115s  iter_time=8.126s
2025-08-05 15:53:01 Train INFO: [Train]: [003][00023/00051] (46.2%)  Loss=1.1226  cls_loss=0.7605  reg_loss=0.3621  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1035s  iter_time=2.485s
2025-08-05 15:55:07 Train INFO: [Train]: [003][00024/00051] (48.1%)  Loss=1.1204  cls_loss=0.7597  reg_loss=0.3607  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1094s  iter_time=125.854s
2025-08-05 15:55:18 Train INFO: [Train]: [003][00025/00051] (50.0%)  Loss=1.1171  cls_loss=0.7578  reg_loss=0.3593  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1024s  iter_time=11.213s
2025-08-05 15:55:25 Train INFO: [Train]: [003][00026/00051] (51.9%)  Loss=1.1223  cls_loss=0.7611  reg_loss=0.3612  lr_det=1.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=955s  iter_time=7.381s
2025-08-05 15:55:28 Train INFO: [Train]: [003][00027/00051] (53.8%)  Loss=1.1247  cls_loss=0.7632  reg_loss=0.3614  lr_det=1.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=886s  iter_time=2.446s
2025-08-05 15:57:27 Train INFO: [Train]: [003][00028/00051] (55.8%)  Loss=1.1230  cls_loss=0.7620  reg_loss=0.3610  lr_det=1.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=914s  iter_time=118.706s
2025-08-05 15:57:40 Train INFO: [Train]: [003][00029/00051] (57.7%)  Loss=1.1221  cls_loss=0.7611  reg_loss=0.3609  lr_det=1.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=855s  iter_time=13.567s
2025-08-05 15:57:47 Train INFO: [Train]: [003][00030/00051] (59.6%)  Loss=1.1175  cls_loss=0.7582  reg_loss=0.3592  lr_det=1.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=795s  iter_time=7.251s
2025-08-05 15:57:50 Train INFO: [Train]: [003][00031/00051] (61.5%)  Loss=1.1121  cls_loss=0.7543  reg_loss=0.3578  lr_det=2.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=735s  iter_time=2.434s
2025-08-05 15:59:49 Train INFO: [Train]: [003][00032/00051] (63.5%)  Loss=1.1183  cls_loss=0.7589  reg_loss=0.3594  lr_det=2.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=746s  iter_time=119.539s
2025-08-05 16:00:02 Train INFO: [Train]: [003][00033/00051] (65.4%)  Loss=1.1138  cls_loss=0.7556  reg_loss=0.3582  lr_det=2.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=693s  iter_time=12.880s
2025-08-05 16:00:15 Train INFO: [Train]: [003][00034/00051] (67.3%)  Loss=1.1170  cls_loss=0.7581  reg_loss=0.3588  lr_det=2.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=642s  iter_time=13.158s
2025-08-05 16:00:18 Train INFO: [Train]: [003][00035/00051] (69.2%)  Loss=1.1124  cls_loss=0.7550  reg_loss=0.3574  lr_det=2.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=588s  iter_time=2.414s
2025-08-05 16:02:14 Train INFO: [Train]: [003][00036/00051] (71.2%)  Loss=1.1182  cls_loss=0.7586  reg_loss=0.3596  lr_det=2.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=584s  iter_time=116.057s
2025-08-05 16:02:25 Train INFO: [Train]: [003][00037/00051] (73.1%)  Loss=1.1146  cls_loss=0.7557  reg_loss=0.3589  lr_det=2.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=534s  iter_time=10.631s
2025-08-05 16:02:40 Train INFO: [Train]: [003][00038/00051] (75.0%)  Loss=1.1115  cls_loss=0.7536  reg_loss=0.3579  lr_det=2.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=489s  iter_time=15.265s
2025-08-05 16:02:42 Train INFO: [Train]: [003][00039/00051] (76.9%)  Loss=1.1056  cls_loss=0.7490  reg_loss=0.3565  lr_det=3.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=440s  iter_time=2.322s
2025-08-05 16:04:44 Train INFO: [Train]: [003][00040/00051] (78.8%)  Loss=1.1057  cls_loss=0.7486  reg_loss=0.3571  lr_det=3.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=427s  iter_time=122.229s
2025-08-05 16:04:58 Train INFO: [Train]: [003][00041/00051] (80.8%)  Loss=1.1043  cls_loss=0.7474  reg_loss=0.3569  lr_det=3.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=382s  iter_time=14.024s
2025-08-05 16:05:14 Train INFO: [Train]: [003][00042/00051] (82.7%)  Loss=1.1062  cls_loss=0.7483  reg_loss=0.3579  lr_det=3.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=339s  iter_time=15.377s
2025-08-05 16:05:16 Train INFO: [Train]: [003][00043/00051] (84.6%)  Loss=1.1027  cls_loss=0.7456  reg_loss=0.3571  lr_det=3.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=295s  iter_time=2.386s
2025-08-05 16:07:13 Train INFO: [Train]: [003][00044/00051] (86.5%)  Loss=1.0995  cls_loss=0.7438  reg_loss=0.3557  lr_det=3.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=271s  iter_time=117.296s
2025-08-05 16:07:21 Train INFO: [Train]: [003][00045/00051] (88.5%)  Loss=1.0945  cls_loss=0.7405  reg_loss=0.3540  lr_det=4.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=7.089s
2025-08-05 16:07:39 Train INFO: [Train]: [003][00046/00051] (90.4%)  Loss=1.0919  cls_loss=0.7386  reg_loss=0.3534  lr_det=4.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=188s  iter_time=18.830s
2025-08-05 16:07:42 Train INFO: [Train]: [003][00047/00051] (92.3%)  Loss=1.0899  cls_loss=0.7371  reg_loss=0.3527  lr_det=4.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=147s  iter_time=2.359s
2025-08-05 16:09:35 Train INFO: [Train]: [003][00048/00051] (94.2%)  Loss=1.0884  cls_loss=0.7364  reg_loss=0.3520  lr_det=4.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=115s  iter_time=113.050s
2025-08-05 16:09:39 Train INFO: [Train]: [003][00049/00051] (96.2%)  Loss=1.0854  cls_loss=0.7347  reg_loss=0.3507  lr_det=4.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=75s  iter_time=4.522s
2025-08-05 16:09:50 Train INFO: [Train]: [003][00050/00051] (98.1%)  Loss=1.0880  cls_loss=0.7365  reg_loss=0.3514  lr_det=4.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=37s  iter_time=10.678s
2025-08-05 16:09:54 Train INFO: [Train]: [003][00051/00051] (100.0%)  Loss=1.0884  cls_loss=0.7363  reg_loss=0.3521  lr_det=4.8e-05  GPU=1392MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=4.459s
2025-08-05 16:09:55 Train INFO: [Train]: Epoch 3 completed in 1901.0s (avg 36.557s/iter)
2025-08-05 16:09:55 Train INFO: [Train]: Final Loss=1.0884
2025-08-05 16:09:55 Train INFO: [Train]: Epoch 4 started (Total iterations: 52)
2025-08-05 16:12:38 Train INFO: [Train]: [004][00001/00051] (3.8%)  Loss=1.0882  cls_loss=0.7249  reg_loss=0.3633  lr_det=5.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=4075s  iter_time=163.020s  fwd=2.183s/bwd=0.064s/opt=0.019s
2025-08-05 16:12:41 Train INFO: [Train]: [004][00002/00051] (5.8%)  Loss=1.1827  cls_loss=0.7869  reg_loss=0.3958  lr_det=5.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2702s  iter_time=2.406s
2025-08-05 16:12:43 Train INFO: [Train]: [004][00003/00051] (7.7%)  Loss=1.1740  cls_loss=0.7821  reg_loss=0.3918  lr_det=5.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2013s  iter_time=2.279s
2025-08-05 16:15:03 Train INFO: [Train]: [004][00004/00051] (9.6%)  Loss=1.1476  cls_loss=0.7602  reg_loss=0.3875  lr_det=5.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2897s  iter_time=140.533s
2025-08-05 16:15:06 Train INFO: [Train]: [004][00005/00051] (11.5%)  Loss=1.1316  cls_loss=0.7472  reg_loss=0.3844  lr_det=5.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2381s  iter_time=2.372s
2025-08-05 16:15:08 Train INFO: [Train]: [004][00006/00051] (13.5%)  Loss=1.1374  cls_loss=0.7519  reg_loss=0.3855  lr_det=5.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2012s  iter_time=2.328s
2025-08-05 16:15:10 Train INFO: [Train]: [004][00007/00051] (15.4%)  Loss=1.1611  cls_loss=0.7695  reg_loss=0.3916  lr_det=6.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1734s  iter_time=2.266s
2025-08-05 16:17:20 Train INFO: [Train]: [004][00008/00051] (17.3%)  Loss=1.1446  cls_loss=0.7580  reg_loss=0.3867  lr_det=6.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2127s  iter_time=129.884s
2025-08-05 16:17:27 Train INFO: [Train]: [004][00009/00051] (19.2%)  Loss=1.1361  cls_loss=0.7527  reg_loss=0.3834  lr_det=6.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1898s  iter_time=6.793s
2025-08-05 16:17:29 Train INFO: [Train]: [004][00010/00051] (21.2%)  Loss=1.1350  cls_loss=0.7519  reg_loss=0.3831  lr_det=6.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1693s  iter_time=2.393s
2025-08-05 16:17:32 Train INFO: [Train]: [004][00011/00051] (23.1%)  Loss=1.1410  cls_loss=0.7561  reg_loss=0.3850  lr_det=6.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1522s  iter_time=2.397s
2025-08-05 16:19:47 Train INFO: [Train]: [004][00012/00051] (25.0%)  Loss=1.1336  cls_loss=0.7520  reg_loss=0.3816  lr_det=6.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1775s  iter_time=134.858s
2025-08-05 16:19:52 Train INFO: [Train]: [004][00013/00051] (26.9%)  Loss=1.1317  cls_loss=0.7541  reg_loss=0.3776  lr_det=6.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1621s  iter_time=5.562s
2025-08-05 16:19:55 Train INFO: [Train]: [004][00014/00051] (28.8%)  Loss=1.1461  cls_loss=0.7645  reg_loss=0.3815  lr_det=7.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1479s  iter_time=2.370s
2025-08-05 16:19:57 Train INFO: [Train]: [004][00015/00051] (30.8%)  Loss=1.1427  cls_loss=0.7636  reg_loss=0.3791  lr_det=7.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1354s  iter_time=2.411s
2025-08-05 16:22:10 Train INFO: [Train]: [004][00016/00051] (32.7%)  Loss=1.1366  cls_loss=0.7597  reg_loss=0.3769  lr_det=7.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1513s  iter_time=132.954s
2025-08-05 16:22:12 Train INFO: [Train]: [004][00017/00051] (34.6%)  Loss=1.1287  cls_loss=0.7535  reg_loss=0.3753  lr_det=7.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1393s  iter_time=2.366s
2025-08-05 16:22:15 Train INFO: [Train]: [004][00018/00051] (36.5%)  Loss=1.1242  cls_loss=0.7513  reg_loss=0.3728  lr_det=7.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1284s  iter_time=2.343s
2025-08-05 16:22:17 Train INFO: [Train]: [004][00019/00051] (38.5%)  Loss=1.1223  cls_loss=0.7502  reg_loss=0.3721  lr_det=7.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1187s  iter_time=2.302s
2025-08-05 16:24:32 Train INFO: [Train]: [004][00020/00051] (40.4%)  Loss=1.1069  cls_loss=0.7400  reg_loss=0.3670  lr_det=7.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1295s  iter_time=135.182s
2025-08-05 16:24:35 Train INFO: [Train]: [004][00021/00051] (42.3%)  Loss=1.1096  cls_loss=0.7413  reg_loss=0.3683  lr_det=8.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1199s  iter_time=2.452s
2025-08-05 16:24:37 Train INFO: [Train]: [004][00022/00051] (44.2%)  Loss=1.1134  cls_loss=0.7440  reg_loss=0.3695  lr_det=8.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1112s  iter_time=2.395s
2025-08-05 16:24:39 Train INFO: [Train]: [004][00023/00051] (46.2%)  Loss=1.0997  cls_loss=0.7346  reg_loss=0.3651  lr_det=8.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1032s  iter_time=2.308s
2025-08-05 16:27:04 Train INFO: [Train]: [004][00024/00051] (48.1%)  Loss=1.1002  cls_loss=0.7360  reg_loss=0.3642  lr_det=8.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1111s  iter_time=144.176s
2025-08-05 16:27:06 Train INFO: [Train]: [004][00025/00051] (50.0%)  Loss=1.0988  cls_loss=0.7349  reg_loss=0.3639  lr_det=8.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1031s  iter_time=2.422s
2025-08-05 16:27:08 Train INFO: [Train]: [004][00026/00051] (51.9%)  Loss=1.1004  cls_loss=0.7366  reg_loss=0.3638  lr_det=8.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=957s  iter_time=2.453s
2025-08-05 16:27:11 Train INFO: [Train]: [004][00027/00051] (53.8%)  Loss=1.1058  cls_loss=0.7409  reg_loss=0.3649  lr_det=8.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=888s  iter_time=2.420s
2025-08-05 16:29:25 Train INFO: [Train]: [004][00028/00051] (55.8%)  Loss=1.0996  cls_loss=0.7371  reg_loss=0.3625  lr_det=8.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=928s  iter_time=133.843s
2025-08-05 16:29:30 Train INFO: [Train]: [004][00029/00051] (57.7%)  Loss=1.1046  cls_loss=0.7404  reg_loss=0.3641  lr_det=8.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=862s  iter_time=5.596s
2025-08-05 16:29:33 Train INFO: [Train]: [004][00030/00051] (59.6%)  Loss=1.1081  cls_loss=0.7435  reg_loss=0.3646  lr_det=8.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=798s  iter_time=2.460s
2025-08-05 16:29:35 Train INFO: [Train]: [004][00031/00051] (61.5%)  Loss=1.1033  cls_loss=0.7405  reg_loss=0.3628  lr_det=9.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=737s  iter_time=2.387s
2025-08-05 16:31:45 Train INFO: [Train]: [004][00032/00051] (63.5%)  Loss=1.1020  cls_loss=0.7397  reg_loss=0.3622  lr_det=9.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=754s  iter_time=129.982s
2025-08-05 16:31:49 Train INFO: [Train]: [004][00033/00051] (65.4%)  Loss=1.0958  cls_loss=0.7360  reg_loss=0.3598  lr_det=9.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=695s  iter_time=3.759s
2025-08-05 16:31:51 Train INFO: [Train]: [004][00034/00051] (67.3%)  Loss=1.1032  cls_loss=0.7407  reg_loss=0.3625  lr_det=9.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=639s  iter_time=2.335s
2025-08-05 16:31:54 Train INFO: [Train]: [004][00035/00051] (69.2%)  Loss=1.0984  cls_loss=0.7376  reg_loss=0.3608  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=586s  iter_time=2.373s
2025-08-05 16:34:10 Train INFO: [Train]: [004][00036/00051] (71.2%)  Loss=1.0999  cls_loss=0.7385  reg_loss=0.3614  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=590s  iter_time=136.497s
2025-08-05 16:34:13 Train INFO: [Train]: [004][00037/00051] (73.1%)  Loss=1.1009  cls_loss=0.7384  reg_loss=0.3625  lr_det=9.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=537s  iter_time=3.019s
2025-08-05 16:34:16 Train INFO: [Train]: [004][00038/00051] (75.0%)  Loss=1.1011  cls_loss=0.7382  reg_loss=0.3629  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=487s  iter_time=2.444s
2025-08-05 16:34:18 Train INFO: [Train]: [004][00039/00051] (76.9%)  Loss=1.1022  cls_loss=0.7393  reg_loss=0.3630  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=439s  iter_time=2.390s
2025-08-05 16:36:32 Train INFO: [Train]: [004][00040/00051] (78.8%)  Loss=1.1012  cls_loss=0.7391  reg_loss=0.3621  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=428s  iter_time=133.802s
2025-08-05 16:36:40 Train INFO: [Train]: [004][00041/00051] (80.8%)  Loss=1.0977  cls_loss=0.7367  reg_loss=0.3609  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=382s  iter_time=7.842s
2025-08-05 16:36:42 Train INFO: [Train]: [004][00042/00051] (82.7%)  Loss=1.0974  cls_loss=0.7368  reg_loss=0.3606  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=336s  iter_time=2.397s
2025-08-05 16:36:44 Train INFO: [Train]: [004][00043/00051] (84.6%)  Loss=1.0963  cls_loss=0.7359  reg_loss=0.3604  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=293s  iter_time=2.278s
2025-08-05 16:38:58 Train INFO: [Train]: [004][00044/00051] (86.5%)  Loss=1.0972  cls_loss=0.7360  reg_loss=0.3613  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=271s  iter_time=133.759s
2025-08-05 16:39:04 Train INFO: [Train]: [004][00045/00051] (88.5%)  Loss=1.0925  cls_loss=0.7323  reg_loss=0.3602  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=5.643s
2025-08-05 16:39:06 Train INFO: [Train]: [004][00046/00051] (90.4%)  Loss=1.0887  cls_loss=0.7296  reg_loss=0.3591  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=186s  iter_time=2.410s
2025-08-05 16:39:08 Train INFO: [Train]: [004][00047/00051] (92.3%)  Loss=1.0852  cls_loss=0.7269  reg_loss=0.3583  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=146s  iter_time=2.305s
2025-08-05 16:41:17 Train INFO: [Train]: [004][00048/00051] (94.2%)  Loss=1.0815  cls_loss=0.7247  reg_loss=0.3568  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=115s  iter_time=128.743s
2025-08-05 16:41:22 Train INFO: [Train]: [004][00049/00051] (96.2%)  Loss=1.0823  cls_loss=0.7258  reg_loss=0.3565  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=75s  iter_time=4.878s
2025-08-05 16:41:24 Train INFO: [Train]: [004][00050/00051] (98.1%)  Loss=1.0832  cls_loss=0.7265  reg_loss=0.3567  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=37s  iter_time=2.205s
2025-08-05 16:41:26 Train INFO: [Train]: [004][00051/00051] (100.0%)  Loss=1.0819  cls_loss=0.7259  reg_loss=0.3560  lr_det=1.0e-04  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.081s
2025-08-05 16:41:27 Train INFO: [Train]: Epoch 4 completed in 1891.9s (avg 36.382s/iter)
2025-08-05 16:41:27 Train INFO: [Train]: Final Loss=1.0819
2025-08-05 16:41:29 Train INFO: Checkpoint saved at epoch 4
2025-08-05 16:41:29 Train INFO: [Train]: Epoch 5 started (Total iterations: 52)
2025-08-05 16:44:07 Train INFO: [Train]: [005][00001/00051] (3.8%)  Loss=1.0328  cls_loss=0.6941  reg_loss=0.3387  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=3974s  iter_time=158.957s  fwd=2.244s/bwd=0.053s/opt=0.019s
2025-08-05 16:44:10 Train INFO: [Train]: [005][00002/00051] (5.8%)  Loss=1.0872  cls_loss=0.7204  reg_loss=0.3669  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2634s  iter_time=2.313s
2025-08-05 16:44:12 Train INFO: [Train]: [005][00003/00051] (7.7%)  Loss=1.0722  cls_loss=0.7155  reg_loss=0.3568  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1962s  iter_time=2.269s
2025-08-05 16:46:19 Train INFO: [Train]: [005][00004/00051] (9.6%)  Loss=1.0652  cls_loss=0.7084  reg_loss=0.3569  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2729s  iter_time=126.771s
2025-08-05 16:46:24 Train INFO: [Train]: [005][00005/00051] (11.5%)  Loss=1.0927  cls_loss=0.7290  reg_loss=0.3637  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2265s  iter_time=5.130s
2025-08-05 16:46:26 Train INFO: [Train]: [005][00006/00051] (13.5%)  Loss=1.1044  cls_loss=0.7384  reg_loss=0.3660  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1914s  iter_time=2.349s
2025-08-05 16:46:29 Train INFO: [Train]: [005][00007/00051] (15.4%)  Loss=1.1169  cls_loss=0.7429  reg_loss=0.3740  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1650s  iter_time=2.230s
2025-08-05 16:48:35 Train INFO: [Train]: [005][00008/00051] (17.3%)  Loss=1.1123  cls_loss=0.7408  reg_loss=0.3714  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2038s  iter_time=126.480s
2025-08-05 16:48:46 Train INFO: [Train]: [005][00009/00051] (19.2%)  Loss=1.1052  cls_loss=0.7401  reg_loss=0.3651  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1836s  iter_time=10.544s
2025-08-05 16:48:49 Train INFO: [Train]: [005][00010/00051] (21.2%)  Loss=1.0903  cls_loss=0.7319  reg_loss=0.3584  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1643s  iter_time=3.753s
2025-08-05 16:48:52 Train INFO: [Train]: [005][00011/00051] (23.1%)  Loss=1.1007  cls_loss=0.7388  reg_loss=0.3619  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1477s  iter_time=2.350s
2025-08-05 16:50:55 Train INFO: [Train]: [005][00012/00051] (25.0%)  Loss=1.1043  cls_loss=0.7423  reg_loss=0.3620  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1699s  iter_time=123.152s
2025-08-05 16:51:06 Train INFO: [Train]: [005][00013/00051] (26.9%)  Loss=1.1000  cls_loss=0.7395  reg_loss=0.3605  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1567s  iter_time=10.864s
2025-08-05 16:51:09 Train INFO: [Train]: [005][00014/00051] (28.8%)  Loss=1.1058  cls_loss=0.7417  reg_loss=0.3640  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1431s  iter_time=3.138s
2025-08-05 16:51:11 Train INFO: [Train]: [005][00015/00051] (30.8%)  Loss=1.1035  cls_loss=0.7400  reg_loss=0.3635  lr_det=9.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1311s  iter_time=2.372s
2025-08-05 16:53:14 Train INFO: [Train]: [005][00016/00051] (32.7%)  Loss=1.0961  cls_loss=0.7356  reg_loss=0.3604  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1453s  iter_time=122.891s
2025-08-05 16:53:20 Train INFO: [Train]: [005][00017/00051] (34.6%)  Loss=1.0900  cls_loss=0.7310  reg_loss=0.3591  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1344s  iter_time=5.985s
2025-08-05 16:53:26 Train INFO: [Train]: [005][00018/00051] (36.5%)  Loss=1.0862  cls_loss=0.7288  reg_loss=0.3574  lr_det=9.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1247s  iter_time=6.186s
2025-08-05 16:53:29 Train INFO: [Train]: [005][00019/00051] (38.5%)  Loss=1.0867  cls_loss=0.7289  reg_loss=0.3578  lr_det=9.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1152s  iter_time=2.288s
2025-08-05 16:55:35 Train INFO: [Train]: [005][00020/00051] (40.4%)  Loss=1.0801  cls_loss=0.7243  reg_loss=0.3557  lr_det=9.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1249s  iter_time=126.137s
2025-08-05 16:55:37 Train INFO: [Train]: [005][00021/00051] (42.3%)  Loss=1.0788  cls_loss=0.7232  reg_loss=0.3555  lr_det=9.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1157s  iter_time=2.262s
2025-08-05 16:55:50 Train INFO: [Train]: [005][00022/00051] (44.2%)  Loss=1.0907  cls_loss=0.7312  reg_loss=0.3595  lr_det=8.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1086s  iter_time=13.076s
2025-08-05 16:55:52 Train INFO: [Train]: [005][00023/00051] (46.2%)  Loss=1.0840  cls_loss=0.7262  reg_loss=0.3577  lr_det=8.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1008s  iter_time=2.337s
2025-08-05 16:57:59 Train INFO: [Train]: [005][00024/00051] (48.1%)  Loss=1.0884  cls_loss=0.7296  reg_loss=0.3588  lr_det=8.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1070s  iter_time=127.093s
2025-08-05 16:58:02 Train INFO: [Train]: [005][00025/00051] (50.0%)  Loss=1.0885  cls_loss=0.7288  reg_loss=0.3598  lr_det=8.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=993s  iter_time=2.301s
2025-08-05 16:58:16 Train INFO: [Train]: [005][00026/00051] (51.9%)  Loss=1.0976  cls_loss=0.7348  reg_loss=0.3628  lr_det=8.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=933s  iter_time=14.221s
2025-08-05 16:58:18 Train INFO: [Train]: [005][00027/00051] (53.8%)  Loss=1.0965  cls_loss=0.7343  reg_loss=0.3622  lr_det=8.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=866s  iter_time=2.342s
2025-08-05 17:00:17 Train INFO: [Train]: [005][00028/00051] (55.8%)  Loss=1.0908  cls_loss=0.7310  reg_loss=0.3598  lr_det=8.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=895s  iter_time=118.473s
2025-08-05 17:00:23 Train INFO: [Train]: [005][00029/00051] (57.7%)  Loss=1.0896  cls_loss=0.7301  reg_loss=0.3596  lr_det=8.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=832s  iter_time=6.443s
2025-08-05 17:00:33 Train INFO: [Train]: [005][00030/00051] (59.6%)  Loss=1.0841  cls_loss=0.7269  reg_loss=0.3572  lr_det=8.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=776s  iter_time=10.104s
2025-08-05 17:00:36 Train INFO: [Train]: [005][00031/00051] (61.5%)  Loss=1.0790  cls_loss=0.7237  reg_loss=0.3554  lr_det=8.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=717s  iter_time=2.326s
2025-08-05 17:02:39 Train INFO: [Train]: [005][00032/00051] (63.5%)  Loss=1.0833  cls_loss=0.7261  reg_loss=0.3572  lr_det=7.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=731s  iter_time=123.070s
2025-08-05 17:02:41 Train INFO: [Train]: [005][00033/00051] (65.4%)  Loss=1.0789  cls_loss=0.7230  reg_loss=0.3559  lr_det=7.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=674s  iter_time=2.327s
2025-08-05 17:02:56 Train INFO: [Train]: [005][00034/00051] (67.3%)  Loss=1.0841  cls_loss=0.7271  reg_loss=0.3571  lr_det=7.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=625s  iter_time=14.633s
2025-08-05 17:02:58 Train INFO: [Train]: [005][00035/00051] (69.2%)  Loss=1.0858  cls_loss=0.7284  reg_loss=0.3574  lr_det=7.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=573s  iter_time=2.301s
2025-08-05 17:05:03 Train INFO: [Train]: [005][00036/00051] (71.2%)  Loss=1.0884  cls_loss=0.7293  reg_loss=0.3590  lr_det=7.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=574s  iter_time=125.157s
2025-08-05 17:05:06 Train INFO: [Train]: [005][00037/00051] (73.1%)  Loss=1.0861  cls_loss=0.7274  reg_loss=0.3586  lr_det=7.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=522s  iter_time=2.445s
2025-08-05 17:05:22 Train INFO: [Train]: [005][00038/00051] (75.0%)  Loss=1.0882  cls_loss=0.7280  reg_loss=0.3602  lr_det=7.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=478s  iter_time=16.273s
2025-08-05 17:05:24 Train INFO: [Train]: [005][00039/00051] (76.9%)  Loss=1.0868  cls_loss=0.7264  reg_loss=0.3604  lr_det=6.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=431s  iter_time=2.364s
2025-08-05 17:07:24 Train INFO: [Train]: [005][00040/00051] (78.8%)  Loss=1.0835  cls_loss=0.7247  reg_loss=0.3589  lr_det=6.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=417s  iter_time=120.007s
2025-08-05 17:07:27 Train INFO: [Train]: [005][00041/00051] (80.8%)  Loss=1.0829  cls_loss=0.7247  reg_loss=0.3582  lr_det=6.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=371s  iter_time=2.309s
2025-08-05 17:07:40 Train INFO: [Train]: [005][00042/00051] (82.7%)  Loss=1.0841  cls_loss=0.7261  reg_loss=0.3580  lr_det=6.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=329s  iter_time=13.663s
2025-08-05 17:07:43 Train INFO: [Train]: [005][00043/00051] (84.6%)  Loss=1.0795  cls_loss=0.7228  reg_loss=0.3567  lr_det=6.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.394s
2025-08-05 17:09:52 Train INFO: [Train]: [005][00044/00051] (86.5%)  Loss=1.0744  cls_loss=0.7192  reg_loss=0.3552  lr_det=6.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=265s  iter_time=128.943s
2025-08-05 17:09:54 Train INFO: [Train]: [005][00045/00051] (88.5%)  Loss=1.0742  cls_loss=0.7187  reg_loss=0.3555  lr_det=6.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=222s  iter_time=2.311s
2025-08-05 17:10:07 Train INFO: [Train]: [005][00046/00051] (90.4%)  Loss=1.0749  cls_loss=0.7189  reg_loss=0.3559  lr_det=5.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=183s  iter_time=13.274s
2025-08-05 17:10:10 Train INFO: [Train]: [005][00047/00051] (92.3%)  Loss=1.0735  cls_loss=0.7182  reg_loss=0.3553  lr_det=5.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.312s
2025-08-05 17:12:20 Train INFO: [Train]: [005][00048/00051] (94.2%)  Loss=1.0727  cls_loss=0.7180  reg_loss=0.3547  lr_det=5.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=113s  iter_time=130.940s
2025-08-05 17:12:23 Train INFO: [Train]: [005][00049/00051] (96.2%)  Loss=1.0722  cls_loss=0.7180  reg_loss=0.3541  lr_det=5.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.371s
2025-08-05 17:12:33 Train INFO: [Train]: [005][00050/00051] (98.1%)  Loss=1.0711  cls_loss=0.7173  reg_loss=0.3538  lr_det=5.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=37s  iter_time=9.983s
2025-08-05 17:12:35 Train INFO: [Train]: [005][00051/00051] (100.0%)  Loss=1.0693  cls_loss=0.7158  reg_loss=0.3535  lr_det=5.2e-05  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.252s
2025-08-05 17:12:36 Train INFO: [Train]: Epoch 5 completed in 1867.7s (avg 35.916s/iter)
2025-08-05 17:12:36 Train INFO: [Train]: Final Loss=1.0693
2025-08-05 17:12:36 Train INFO: [Train]: Epoch 6 started (Total iterations: 52)
2025-08-05 17:15:42 Train INFO: [Train]: [006][00001/00051] (3.8%)  Loss=0.9904  cls_loss=0.6549  reg_loss=0.3356  lr_det=4.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=4641s  iter_time=185.646s  fwd=2.327s/bwd=0.089s/opt=0.021s
2025-08-05 17:15:44 Train INFO: [Train]: [006][00002/00051] (5.8%)  Loss=1.0637  cls_loss=0.7055  reg_loss=0.3582  lr_det=4.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=3071s  iter_time=2.362s
2025-08-05 17:15:47 Train INFO: [Train]: [006][00003/00051] (7.7%)  Loss=1.0644  cls_loss=0.7058  reg_loss=0.3586  lr_det=4.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2285s  iter_time=2.398s
2025-08-05 17:18:00 Train INFO: [Train]: [006][00004/00051] (9.6%)  Loss=1.0466  cls_loss=0.6966  reg_loss=0.3499  lr_det=4.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=3042s  iter_time=133.156s
2025-08-05 17:18:08 Train INFO: [Train]: [006][00005/00051] (11.5%)  Loss=1.0862  cls_loss=0.7230  reg_loss=0.3632  lr_det=4.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2543s  iter_time=8.122s
2025-08-05 17:18:10 Train INFO: [Train]: [006][00006/00051] (13.5%)  Loss=1.0796  cls_loss=0.7176  reg_loss=0.3620  lr_det=4.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2147s  iter_time=2.359s
2025-08-05 17:18:13 Train INFO: [Train]: [006][00007/00051] (15.4%)  Loss=1.0873  cls_loss=0.7264  reg_loss=0.3609  lr_det=4.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1850s  iter_time=2.327s
2025-08-05 17:20:20 Train INFO: [Train]: [006][00008/00051] (17.3%)  Loss=1.0826  cls_loss=0.7199  reg_loss=0.3627  lr_det=3.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2218s  iter_time=127.813s
2025-08-05 17:20:31 Train INFO: [Train]: [006][00009/00051] (19.2%)  Loss=1.0679  cls_loss=0.7140  reg_loss=0.3539  lr_det=3.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1995s  iter_time=10.803s
2025-08-05 17:20:34 Train INFO: [Train]: [006][00010/00051] (21.2%)  Loss=1.0706  cls_loss=0.7148  reg_loss=0.3558  lr_det=3.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1781s  iter_time=2.794s
2025-08-05 17:20:37 Train INFO: [Train]: [006][00011/00051] (23.1%)  Loss=1.0922  cls_loss=0.7270  reg_loss=0.3651  lr_det=3.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1601s  iter_time=2.542s
2025-08-05 17:22:49 Train INFO: [Train]: [006][00012/00051] (25.0%)  Loss=1.0738  cls_loss=0.7164  reg_loss=0.3574  lr_det=3.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1839s  iter_time=132.685s
2025-08-05 17:23:00 Train INFO: [Train]: [006][00013/00051] (26.9%)  Loss=1.0809  cls_loss=0.7227  reg_loss=0.3582  lr_det=3.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1693s  iter_time=10.617s
2025-08-05 17:23:06 Train INFO: [Train]: [006][00014/00051] (28.8%)  Loss=1.0847  cls_loss=0.7252  reg_loss=0.3594  lr_det=2.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1554s  iter_time=6.236s
2025-08-05 17:23:08 Train INFO: [Train]: [006][00015/00051] (30.8%)  Loss=1.0806  cls_loss=0.7229  reg_loss=0.3576  lr_det=2.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1422s  iter_time=2.332s
2025-08-05 17:25:17 Train INFO: [Train]: [006][00016/00051] (32.7%)  Loss=1.0749  cls_loss=0.7183  reg_loss=0.3566  lr_det=2.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1566s  iter_time=128.212s
2025-08-05 17:25:19 Train INFO: [Train]: [006][00017/00051] (34.6%)  Loss=1.0647  cls_loss=0.7109  reg_loss=0.3538  lr_det=2.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1441s  iter_time=2.371s
2025-08-05 17:25:25 Train INFO: [Train]: [006][00018/00051] (36.5%)  Loss=1.0588  cls_loss=0.7065  reg_loss=0.3523  lr_det=2.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1335s  iter_time=5.912s
2025-08-05 17:25:27 Train INFO: [Train]: [006][00019/00051] (38.5%)  Loss=1.0586  cls_loss=0.7053  reg_loss=0.3533  lr_det=2.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1234s  iter_time=2.404s
2025-08-05 17:27:40 Train INFO: [Train]: [006][00020/00051] (40.4%)  Loss=1.0561  cls_loss=0.7034  reg_loss=0.3527  lr_det=2.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1334s  iter_time=132.853s
2025-08-05 17:27:42 Train INFO: [Train]: [006][00021/00051] (42.3%)  Loss=1.0652  cls_loss=0.7102  reg_loss=0.3550  lr_det=2.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1236s  iter_time=2.323s
2025-08-05 17:27:49 Train INFO: [Train]: [006][00022/00051] (44.2%)  Loss=1.0692  cls_loss=0.7131  reg_loss=0.3561  lr_det=1.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1151s  iter_time=6.846s
2025-08-05 17:27:52 Train INFO: [Train]: [006][00023/00051] (46.2%)  Loss=1.0617  cls_loss=0.7083  reg_loss=0.3534  lr_det=1.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1068s  iter_time=2.349s
2025-08-05 17:30:04 Train INFO: [Train]: [006][00024/00051] (48.1%)  Loss=1.0686  cls_loss=0.7120  reg_loss=0.3566  lr_det=1.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1131s  iter_time=132.084s
2025-08-05 17:30:06 Train INFO: [Train]: [006][00025/00051] (50.0%)  Loss=1.0684  cls_loss=0.7114  reg_loss=0.3570  lr_det=1.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1050s  iter_time=2.305s
2025-08-05 17:30:12 Train INFO: [Train]: [006][00026/00051] (51.9%)  Loss=1.0688  cls_loss=0.7115  reg_loss=0.3574  lr_det=1.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=978s  iter_time=6.026s
2025-08-05 17:30:14 Train INFO: [Train]: [006][00027/00051] (53.8%)  Loss=1.0693  cls_loss=0.7119  reg_loss=0.3574  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=907s  iter_time=2.336s
2025-08-05 17:32:28 Train INFO: [Train]: [006][00028/00051] (55.8%)  Loss=1.0667  cls_loss=0.7100  reg_loss=0.3567  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=945s  iter_time=133.472s
2025-08-05 17:32:34 Train INFO: [Train]: [006][00029/00051] (57.7%)  Loss=1.0690  cls_loss=0.7121  reg_loss=0.3569  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=878s  iter_time=5.740s
2025-08-05 17:32:40 Train INFO: [Train]: [006][00030/00051] (59.6%)  Loss=1.0681  cls_loss=0.7109  reg_loss=0.3572  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=816s  iter_time=6.610s
2025-08-05 17:32:43 Train INFO: [Train]: [006][00031/00051] (61.5%)  Loss=1.0652  cls_loss=0.7087  reg_loss=0.3565  lr_det=9.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=754s  iter_time=2.405s
2025-08-05 17:34:49 Train INFO: [Train]: [006][00032/00051] (63.5%)  Loss=1.0598  cls_loss=0.7053  reg_loss=0.3545  lr_det=8.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=767s  iter_time=125.844s
2025-08-05 17:34:52 Train INFO: [Train]: [006][00033/00051] (65.4%)  Loss=1.0559  cls_loss=0.7028  reg_loss=0.3531  lr_det=8.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=707s  iter_time=3.140s
2025-08-05 17:34:59 Train INFO: [Train]: [006][00034/00051] (67.3%)  Loss=1.0609  cls_loss=0.7061  reg_loss=0.3548  lr_det=7.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=652s  iter_time=6.977s
2025-08-05 17:35:01 Train INFO: [Train]: [006][00035/00051] (69.2%)  Loss=1.0576  cls_loss=0.7042  reg_loss=0.3534  lr_det=6.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=598s  iter_time=2.384s
2025-08-05 17:37:12 Train INFO: [Train]: [006][00036/00051] (71.2%)  Loss=1.0609  cls_loss=0.7067  reg_loss=0.3542  lr_det=5.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=598s  iter_time=130.615s
2025-08-05 17:37:16 Train INFO: [Train]: [006][00037/00051] (73.1%)  Loss=1.0623  cls_loss=0.7070  reg_loss=0.3554  lr_det=5.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=545s  iter_time=3.932s
2025-08-05 17:37:22 Train INFO: [Train]: [006][00038/00051] (75.0%)  Loss=1.0621  cls_loss=0.7068  reg_loss=0.3553  lr_det=4.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=495s  iter_time=6.834s
2025-08-05 17:37:25 Train INFO: [Train]: [006][00039/00051] (76.9%)  Loss=1.0599  cls_loss=0.7057  reg_loss=0.3542  lr_det=3.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=447s  iter_time=2.339s
2025-08-05 17:39:27 Train INFO: [Train]: [006][00040/00051] (78.8%)  Loss=1.0620  cls_loss=0.7070  reg_loss=0.3550  lr_det=3.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=432s  iter_time=121.774s
2025-08-05 17:39:33 Train INFO: [Train]: [006][00041/00051] (80.8%)  Loss=1.0617  cls_loss=0.7065  reg_loss=0.3552  lr_det=2.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=385s  iter_time=6.198s
2025-08-05 17:39:39 Train INFO: [Train]: [006][00042/00051] (82.7%)  Loss=1.0622  cls_loss=0.7073  reg_loss=0.3549  lr_det=2.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=340s  iter_time=6.149s
2025-08-05 17:39:41 Train INFO: [Train]: [006][00043/00051] (84.6%)  Loss=1.0599  cls_loss=0.7058  reg_loss=0.3541  lr_det=1.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=295s  iter_time=2.362s
2025-08-05 17:41:45 Train INFO: [Train]: [006][00044/00051] (86.5%)  Loss=1.0564  cls_loss=0.7032  reg_loss=0.3532  lr_det=1.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=272s  iter_time=124.039s
2025-08-05 17:41:48 Train INFO: [Train]: [006][00045/00051] (88.5%)  Loss=1.0552  cls_loss=0.7023  reg_loss=0.3529  lr_det=1.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=2.302s
2025-08-05 17:41:58 Train INFO: [Train]: [006][00046/00051] (90.4%)  Loss=1.0542  cls_loss=0.7014  reg_loss=0.3528  lr_det=8.3e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=187s  iter_time=10.736s
2025-08-05 17:42:01 Train INFO: [Train]: [006][00047/00051] (92.3%)  Loss=1.0532  cls_loss=0.7008  reg_loss=0.3524  lr_det=5.8e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=147s  iter_time=2.315s
2025-08-05 17:44:04 Train INFO: [Train]: [006][00048/00051] (94.2%)  Loss=1.0546  cls_loss=0.7018  reg_loss=0.3528  lr_det=3.7e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=116s  iter_time=123.515s
2025-08-05 17:44:06 Train INFO: [Train]: [006][00049/00051] (96.2%)  Loss=1.0535  cls_loss=0.7011  reg_loss=0.3524  lr_det=2.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=76s  iter_time=2.195s
2025-08-05 17:44:10 Train INFO: [Train]: [006][00050/00051] (98.1%)  Loss=1.0548  cls_loss=0.7018  reg_loss=0.3529  lr_det=1.0e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=37s  iter_time=3.511s
2025-08-05 17:44:12 Train INFO: [Train]: [006][00051/00051] (100.0%)  Loss=1.0491  cls_loss=0.6979  reg_loss=0.3512  lr_det=3.3e-08  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.039s
2025-08-05 17:44:13 Train INFO: [Train]: Epoch 6 completed in 1896.4s (avg 36.468s/iter)
2025-08-05 17:44:13 Train INFO: [Train]: Final Loss=1.0491
2025-08-05 17:44:13 Train INFO: [Train]: Epoch 7 started (Total iterations: 52)
2025-08-05 17:46:57 Train INFO: [Train]: [007][00001/00051] (3.8%)  Loss=0.9117  cls_loss=0.6138  reg_loss=0.2978  lr_det=3.3e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=4113s  iter_time=164.527s  fwd=2.230s/bwd=0.087s/opt=0.027s
2025-08-05 17:46:59 Train INFO: [Train]: [007][00002/00051] (5.8%)  Loss=1.0185  cls_loss=0.6762  reg_loss=0.3423  lr_det=1.0e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2726s  iter_time=2.378s
2025-08-05 17:47:02 Train INFO: [Train]: [007][00003/00051] (7.7%)  Loss=1.0092  cls_loss=0.6748  reg_loss=0.3344  lr_det=2.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2031s  iter_time=2.343s
2025-08-05 17:49:09 Train INFO: [Train]: [007][00004/00051] (9.6%)  Loss=0.9939  cls_loss=0.6674  reg_loss=0.3265  lr_det=3.7e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2788s  iter_time=127.297s
2025-08-05 17:49:21 Train INFO: [Train]: [007][00005/00051] (11.5%)  Loss=1.0181  cls_loss=0.6832  reg_loss=0.3349  lr_det=5.8e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2361s  iter_time=11.455s
2025-08-05 17:49:23 Train INFO: [Train]: [007][00006/00051] (13.5%)  Loss=1.0469  cls_loss=0.7035  reg_loss=0.3434  lr_det=8.3e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1995s  iter_time=2.264s
2025-08-05 17:49:25 Train INFO: [Train]: [007][00007/00051] (15.4%)  Loss=1.0647  cls_loss=0.7190  reg_loss=0.3457  lr_det=1.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1719s  iter_time=2.231s
2025-08-05 17:51:25 Train INFO: [Train]: [007][00008/00051] (17.3%)  Loss=1.0580  cls_loss=0.7135  reg_loss=0.3445  lr_det=1.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2068s  iter_time=120.247s
2025-08-05 17:51:43 Train INFO: [Train]: [007][00009/00051] (19.2%)  Loss=1.0549  cls_loss=0.7126  reg_loss=0.3422  lr_det=1.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1894s  iter_time=18.165s
2025-08-05 17:51:46 Train INFO: [Train]: [007][00010/00051] (21.2%)  Loss=1.0674  cls_loss=0.7194  reg_loss=0.3480  lr_det=2.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1689s  iter_time=2.294s
2025-08-05 17:51:48 Train INFO: [Train]: [007][00011/00051] (23.1%)  Loss=1.0751  cls_loss=0.7229  reg_loss=0.3521  lr_det=2.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1518s  iter_time=2.235s
2025-08-05 17:53:46 Train INFO: [Train]: [007][00012/00051] (25.0%)  Loss=1.0652  cls_loss=0.7175  reg_loss=0.3478  lr_det=3.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1721s  iter_time=118.206s
2025-08-05 17:54:04 Train INFO: [Train]: [007][00013/00051] (26.9%)  Loss=1.0610  cls_loss=0.7146  reg_loss=0.3464  lr_det=3.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1605s  iter_time=17.842s
2025-08-05 17:54:06 Train INFO: [Train]: [007][00014/00051] (28.8%)  Loss=1.0561  cls_loss=0.7109  reg_loss=0.3452  lr_det=4.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1465s  iter_time=2.309s
2025-08-05 17:54:09 Train INFO: [Train]: [007][00015/00051] (30.8%)  Loss=1.0545  cls_loss=0.7102  reg_loss=0.3443  lr_det=5.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1341s  iter_time=2.292s
2025-08-05 17:56:09 Train INFO: [Train]: [007][00016/00051] (32.7%)  Loss=1.0452  cls_loss=0.7026  reg_loss=0.3426  lr_det=5.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1475s  iter_time=120.345s
2025-08-05 17:56:17 Train INFO: [Train]: [007][00017/00051] (34.6%)  Loss=1.0479  cls_loss=0.7036  reg_loss=0.3443  lr_det=6.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1368s  iter_time=7.737s
2025-08-05 17:56:21 Train INFO: [Train]: [007][00018/00051] (36.5%)  Loss=1.0430  cls_loss=0.6999  reg_loss=0.3431  lr_det=7.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1265s  iter_time=3.898s
2025-08-05 17:56:23 Train INFO: [Train]: [007][00019/00051] (38.5%)  Loss=1.0513  cls_loss=0.7054  reg_loss=0.3459  lr_det=8.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1169s  iter_time=2.241s
2025-08-05 17:58:26 Train INFO: [Train]: [007][00020/00051] (40.4%)  Loss=1.0460  cls_loss=0.7018  reg_loss=0.3442  lr_det=8.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1260s  iter_time=123.457s
2025-08-05 17:58:32 Train INFO: [Train]: [007][00021/00051] (42.3%)  Loss=1.0494  cls_loss=0.7032  reg_loss=0.3462  lr_det=9.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1171s  iter_time=5.194s
2025-08-05 17:58:38 Train INFO: [Train]: [007][00022/00051] (44.2%)  Loss=1.0621  cls_loss=0.7104  reg_loss=0.3517  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1091s  iter_time=6.543s
2025-08-05 17:58:40 Train INFO: [Train]: [007][00023/00051] (46.2%)  Loss=1.0536  cls_loss=0.7053  reg_loss=0.3483  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1013s  iter_time=2.335s
2025-08-05 18:00:50 Train INFO: [Train]: [007][00024/00051] (48.1%)  Loss=1.0567  cls_loss=0.7078  reg_loss=0.3489  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1078s  iter_time=129.852s
2025-08-05 18:00:53 Train INFO: [Train]: [007][00025/00051] (50.0%)  Loss=1.0590  cls_loss=0.7093  reg_loss=0.3497  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1000s  iter_time=2.416s
2025-08-05 18:00:59 Train INFO: [Train]: [007][00026/00051] (51.9%)  Loss=1.0626  cls_loss=0.7114  reg_loss=0.3511  lr_det=1.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=932s  iter_time=5.959s
2025-08-05 18:01:01 Train INFO: [Train]: [007][00027/00051] (53.8%)  Loss=1.0635  cls_loss=0.7120  reg_loss=0.3514  lr_det=1.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=864s  iter_time=2.313s
2025-08-05 18:03:08 Train INFO: [Train]: [007][00028/00051] (55.8%)  Loss=1.0562  cls_loss=0.7073  reg_loss=0.3489  lr_det=1.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=901s  iter_time=127.193s
2025-08-05 18:03:12 Train INFO: [Train]: [007][00029/00051] (57.7%)  Loss=1.0583  cls_loss=0.7085  reg_loss=0.3499  lr_det=1.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=836s  iter_time=4.256s
2025-08-05 18:03:19 Train INFO: [Train]: [007][00030/00051] (59.6%)  Loss=1.0585  cls_loss=0.7078  reg_loss=0.3507  lr_det=1.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=777s  iter_time=7.033s
2025-08-05 18:03:22 Train INFO: [Train]: [007][00031/00051] (61.5%)  Loss=1.0510  cls_loss=0.7027  reg_loss=0.3483  lr_det=2.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=718s  iter_time=2.330s
2025-08-05 18:05:24 Train INFO: [Train]: [007][00032/00051] (63.5%)  Loss=1.0482  cls_loss=0.7012  reg_loss=0.3470  lr_det=2.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=732s  iter_time=121.850s
2025-08-05 18:05:28 Train INFO: [Train]: [007][00033/00051] (65.4%)  Loss=1.0479  cls_loss=0.7006  reg_loss=0.3473  lr_det=2.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=675s  iter_time=4.728s
2025-08-05 18:05:38 Train INFO: [Train]: [007][00034/00051] (67.3%)  Loss=1.0533  cls_loss=0.7037  reg_loss=0.3496  lr_det=2.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=624s  iter_time=9.246s
2025-08-05 18:05:40 Train INFO: [Train]: [007][00035/00051] (69.2%)  Loss=1.0486  cls_loss=0.7007  reg_loss=0.3480  lr_det=2.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=572s  iter_time=2.334s
2025-08-05 18:07:47 Train INFO: [Train]: [007][00036/00051] (71.2%)  Loss=1.0479  cls_loss=0.7001  reg_loss=0.3478  lr_det=2.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=574s  iter_time=127.406s
2025-08-05 18:07:50 Train INFO: [Train]: [007][00037/00051] (73.1%)  Loss=1.0416  cls_loss=0.6957  reg_loss=0.3459  lr_det=2.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=522s  iter_time=2.293s
2025-08-05 18:07:57 Train INFO: [Train]: [007][00038/00051] (75.0%)  Loss=1.0442  cls_loss=0.6969  reg_loss=0.3473  lr_det=2.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=475s  iter_time=7.296s
2025-08-05 18:07:59 Train INFO: [Train]: [007][00039/00051] (76.9%)  Loss=1.0440  cls_loss=0.6966  reg_loss=0.3474  lr_det=3.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=428s  iter_time=2.272s
2025-08-05 18:10:09 Train INFO: [Train]: [007][00040/00051] (78.8%)  Loss=1.0420  cls_loss=0.6956  reg_loss=0.3463  lr_det=3.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=418s  iter_time=129.724s
2025-08-05 18:10:11 Train INFO: [Train]: [007][00041/00051] (80.8%)  Loss=1.0436  cls_loss=0.6970  reg_loss=0.3466  lr_det=3.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=371s  iter_time=2.258s
2025-08-05 18:10:16 Train INFO: [Train]: [007][00042/00051] (82.7%)  Loss=1.0465  cls_loss=0.6993  reg_loss=0.3472  lr_det=3.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=327s  iter_time=4.425s
2025-08-05 18:10:18 Train INFO: [Train]: [007][00043/00051] (84.6%)  Loss=1.0493  cls_loss=0.7008  reg_loss=0.3486  lr_det=3.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=285s  iter_time=2.295s
2025-08-05 18:12:30 Train INFO: [Train]: [007][00044/00051] (86.5%)  Loss=1.0494  cls_loss=0.7005  reg_loss=0.3490  lr_det=3.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=264s  iter_time=131.961s
2025-08-05 18:12:32 Train INFO: [Train]: [007][00045/00051] (88.5%)  Loss=1.0460  cls_loss=0.6982  reg_loss=0.3478  lr_det=4.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=222s  iter_time=2.242s
2025-08-05 18:12:36 Train INFO: [Train]: [007][00046/00051] (90.4%)  Loss=1.0474  cls_loss=0.6982  reg_loss=0.3492  lr_det=4.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=181s  iter_time=4.361s
2025-08-05 18:12:39 Train INFO: [Train]: [007][00047/00051] (92.3%)  Loss=1.0463  cls_loss=0.6974  reg_loss=0.3489  lr_det=4.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=142s  iter_time=2.321s
2025-08-05 18:14:45 Train INFO: [Train]: [007][00048/00051] (94.2%)  Loss=1.0466  cls_loss=0.6976  reg_loss=0.3490  lr_det=4.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=112s  iter_time=126.071s
2025-08-05 18:14:47 Train INFO: [Train]: [007][00049/00051] (96.2%)  Loss=1.0458  cls_loss=0.6972  reg_loss=0.3485  lr_det=4.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=73s  iter_time=2.163s
2025-08-05 18:14:49 Train INFO: [Train]: [007][00050/00051] (98.1%)  Loss=1.0460  cls_loss=0.6970  reg_loss=0.3490  lr_det=4.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.167s
2025-08-05 18:14:51 Train INFO: [Train]: [007][00051/00051] (100.0%)  Loss=1.0460  cls_loss=0.6966  reg_loss=0.3493  lr_det=4.8e-05  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.034s
2025-08-05 18:14:52 Train INFO: [Train]: Epoch 7 completed in 1839.4s (avg 35.373s/iter)
2025-08-05 18:14:52 Train INFO: [Train]: Final Loss=1.0460
2025-08-05 18:14:52 Train INFO: [Train]: Epoch 8 started (Total iterations: 52)
2025-08-05 18:17:38 Train INFO: [Train]: [008][00001/00051] (3.8%)  Loss=0.9320  cls_loss=0.6201  reg_loss=0.3119  lr_det=5.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=4148s  iter_time=165.925s  fwd=2.127s/bwd=0.071s/opt=0.022s
2025-08-05 18:17:40 Train INFO: [Train]: [008][00002/00051] (5.8%)  Loss=1.0121  cls_loss=0.6720  reg_loss=0.3401  lr_det=5.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2748s  iter_time=2.329s
2025-08-05 18:17:43 Train INFO: [Train]: [008][00003/00051] (7.7%)  Loss=1.0308  cls_loss=0.6824  reg_loss=0.3484  lr_det=5.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2047s  iter_time=2.332s
2025-08-05 18:20:01 Train INFO: [Train]: [008][00004/00051] (9.6%)  Loss=1.0352  cls_loss=0.6869  reg_loss=0.3482  lr_det=5.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2908s  iter_time=138.793s
2025-08-05 18:20:07 Train INFO: [Train]: [008][00005/00051] (11.5%)  Loss=1.0837  cls_loss=0.7136  reg_loss=0.3702  lr_det=5.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2412s  iter_time=5.286s
2025-08-05 18:20:09 Train INFO: [Train]: [008][00006/00051] (13.5%)  Loss=1.0732  cls_loss=0.7081  reg_loss=0.3652  lr_det=5.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2038s  iter_time=2.293s
2025-08-05 18:20:11 Train INFO: [Train]: [008][00007/00051] (15.4%)  Loss=1.0971  cls_loss=0.7226  reg_loss=0.3745  lr_det=6.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1756s  iter_time=2.338s
2025-08-05 18:22:18 Train INFO: [Train]: [008][00008/00051] (17.3%)  Loss=1.0652  cls_loss=0.7029  reg_loss=0.3622  lr_det=6.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2130s  iter_time=126.416s
2025-08-05 18:22:27 Train INFO: [Train]: [008][00009/00051] (19.2%)  Loss=1.0654  cls_loss=0.7051  reg_loss=0.3603  lr_det=6.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1910s  iter_time=8.984s
2025-08-05 18:22:29 Train INFO: [Train]: [008][00010/00051] (21.2%)  Loss=1.0490  cls_loss=0.6971  reg_loss=0.3519  lr_det=6.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1703s  iter_time=2.291s
2025-08-05 18:22:31 Train INFO: [Train]: [008][00011/00051] (23.1%)  Loss=1.0646  cls_loss=0.7062  reg_loss=0.3584  lr_det=6.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1531s  iter_time=2.280s
2025-08-05 18:24:40 Train INFO: [Train]: [008][00012/00051] (25.0%)  Loss=1.0668  cls_loss=0.7083  reg_loss=0.3585  lr_det=6.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1765s  iter_time=129.194s
2025-08-05 18:24:50 Train INFO: [Train]: [008][00013/00051] (26.9%)  Loss=1.0616  cls_loss=0.7059  reg_loss=0.3557  lr_det=6.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1623s  iter_time=9.582s
2025-08-05 18:24:52 Train INFO: [Train]: [008][00014/00051] (28.8%)  Loss=1.0551  cls_loss=0.7010  reg_loss=0.3541  lr_det=7.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1481s  iter_time=2.311s
2025-08-05 18:24:55 Train INFO: [Train]: [008][00015/00051] (30.8%)  Loss=1.0485  cls_loss=0.6963  reg_loss=0.3522  lr_det=7.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1356s  iter_time=2.369s
2025-08-05 18:27:04 Train INFO: [Train]: [008][00016/00051] (32.7%)  Loss=1.0369  cls_loss=0.6894  reg_loss=0.3476  lr_det=7.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1507s  iter_time=129.438s
2025-08-05 18:27:06 Train INFO: [Train]: [008][00017/00051] (34.6%)  Loss=1.0441  cls_loss=0.6936  reg_loss=0.3505  lr_det=7.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1387s  iter_time=2.322s
2025-08-05 18:27:09 Train INFO: [Train]: [008][00018/00051] (36.5%)  Loss=1.0374  cls_loss=0.6901  reg_loss=0.3473  lr_det=7.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1280s  iter_time=2.228s
2025-08-05 18:27:11 Train INFO: [Train]: [008][00019/00051] (38.5%)  Loss=1.0395  cls_loss=0.6913  reg_loss=0.3481  lr_det=7.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1182s  iter_time=2.293s
2025-08-05 18:29:20 Train INFO: [Train]: [008][00020/00051] (40.4%)  Loss=1.0312  cls_loss=0.6868  reg_loss=0.3444  lr_det=7.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1282s  iter_time=129.463s
2025-08-05 18:29:23 Train INFO: [Train]: [008][00021/00051] (42.3%)  Loss=1.0317  cls_loss=0.6874  reg_loss=0.3443  lr_det=8.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1187s  iter_time=2.287s
2025-08-05 18:29:25 Train INFO: [Train]: [008][00022/00051] (44.2%)  Loss=1.0396  cls_loss=0.6920  reg_loss=0.3476  lr_det=8.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1101s  iter_time=2.286s
2025-08-05 18:29:27 Train INFO: [Train]: [008][00023/00051] (46.2%)  Loss=1.0376  cls_loss=0.6908  reg_loss=0.3468  lr_det=8.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1021s  iter_time=2.220s
2025-08-05 18:31:38 Train INFO: [Train]: [008][00024/00051] (48.1%)  Loss=1.0399  cls_loss=0.6930  reg_loss=0.3469  lr_det=8.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1087s  iter_time=130.842s
2025-08-05 18:31:46 Train INFO: [Train]: [008][00025/00051] (50.0%)  Loss=1.0389  cls_loss=0.6920  reg_loss=0.3468  lr_det=8.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1014s  iter_time=7.598s
2025-08-05 18:31:48 Train INFO: [Train]: [008][00026/00051] (51.9%)  Loss=1.0455  cls_loss=0.6969  reg_loss=0.3486  lr_det=8.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=941s  iter_time=2.338s
2025-08-05 18:31:50 Train INFO: [Train]: [008][00027/00051] (53.8%)  Loss=1.0472  cls_loss=0.6983  reg_loss=0.3489  lr_det=8.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=873s  iter_time=2.298s
2025-08-05 18:33:53 Train INFO: [Train]: [008][00028/00051] (55.8%)  Loss=1.0459  cls_loss=0.6972  reg_loss=0.3487  lr_det=8.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=905s  iter_time=122.667s
2025-08-05 18:34:10 Train INFO: [Train]: [008][00029/00051] (57.7%)  Loss=1.0449  cls_loss=0.6965  reg_loss=0.3483  lr_det=8.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=849s  iter_time=16.846s
2025-08-05 18:34:12 Train INFO: [Train]: [008][00030/00051] (59.6%)  Loss=1.0412  cls_loss=0.6948  reg_loss=0.3465  lr_det=8.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=786s  iter_time=2.320s
2025-08-05 18:34:14 Train INFO: [Train]: [008][00031/00051] (61.5%)  Loss=1.0473  cls_loss=0.6986  reg_loss=0.3487  lr_det=9.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=727s  iter_time=2.283s
2025-08-05 18:36:13 Train INFO: [Train]: [008][00032/00051] (63.5%)  Loss=1.0510  cls_loss=0.7003  reg_loss=0.3507  lr_det=9.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=737s  iter_time=118.141s
2025-08-05 18:36:24 Train INFO: [Train]: [008][00033/00051] (65.4%)  Loss=1.0508  cls_loss=0.7002  reg_loss=0.3507  lr_det=9.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=684s  iter_time=11.797s
2025-08-05 18:36:27 Train INFO: [Train]: [008][00034/00051] (67.3%)  Loss=1.0517  cls_loss=0.7006  reg_loss=0.3511  lr_det=9.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=629s  iter_time=2.259s
2025-08-05 18:36:29 Train INFO: [Train]: [008][00035/00051] (69.2%)  Loss=1.0496  cls_loss=0.6989  reg_loss=0.3507  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=576s  iter_time=2.284s
2025-08-05 18:38:36 Train INFO: [Train]: [008][00036/00051] (71.2%)  Loss=1.0492  cls_loss=0.6987  reg_loss=0.3505  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=577s  iter_time=127.341s
2025-08-05 18:38:46 Train INFO: [Train]: [008][00037/00051] (73.1%)  Loss=1.0498  cls_loss=0.6981  reg_loss=0.3518  lr_det=9.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=528s  iter_time=9.426s
2025-08-05 18:38:48 Train INFO: [Train]: [008][00038/00051] (75.0%)  Loss=1.0489  cls_loss=0.6972  reg_loss=0.3518  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=479s  iter_time=2.297s
2025-08-05 18:38:50 Train INFO: [Train]: [008][00039/00051] (76.9%)  Loss=1.0512  cls_loss=0.6984  reg_loss=0.3528  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=431s  iter_time=2.277s
2025-08-05 18:40:54 Train INFO: [Train]: [008][00040/00051] (78.8%)  Loss=1.0530  cls_loss=0.6993  reg_loss=0.3537  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=419s  iter_time=123.246s
2025-08-05 18:41:08 Train INFO: [Train]: [008][00041/00051] (80.8%)  Loss=1.0536  cls_loss=0.6991  reg_loss=0.3545  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=375s  iter_time=14.969s
2025-08-05 18:41:11 Train INFO: [Train]: [008][00042/00051] (82.7%)  Loss=1.0552  cls_loss=0.7003  reg_loss=0.3549  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=330s  iter_time=2.292s
2025-08-05 18:41:13 Train INFO: [Train]: [008][00043/00051] (84.6%)  Loss=1.0512  cls_loss=0.6978  reg_loss=0.3534  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=287s  iter_time=2.281s
2025-08-05 18:43:13 Train INFO: [Train]: [008][00044/00051] (86.5%)  Loss=1.0488  cls_loss=0.6961  reg_loss=0.3528  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=265s  iter_time=120.220s
2025-08-05 18:43:24 Train INFO: [Train]: [008][00045/00051] (88.5%)  Loss=1.0475  cls_loss=0.6946  reg_loss=0.3529  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=223s  iter_time=10.338s
2025-08-05 18:43:26 Train INFO: [Train]: [008][00046/00051] (90.4%)  Loss=1.0506  cls_loss=0.6959  reg_loss=0.3546  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=182s  iter_time=2.282s
2025-08-05 18:43:28 Train INFO: [Train]: [008][00047/00051] (92.3%)  Loss=1.0534  cls_loss=0.6980  reg_loss=0.3554  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.276s
2025-08-05 18:45:28 Train INFO: [Train]: [008][00048/00051] (94.2%)  Loss=1.0505  cls_loss=0.6965  reg_loss=0.3540  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=112s  iter_time=119.386s
2025-08-05 18:45:35 Train INFO: [Train]: [008][00049/00051] (96.2%)  Loss=1.0544  cls_loss=0.6992  reg_loss=0.3552  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=74s  iter_time=7.473s
2025-08-05 18:45:37 Train INFO: [Train]: [008][00050/00051] (98.1%)  Loss=1.0572  cls_loss=0.7008  reg_loss=0.3564  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.255s
2025-08-05 18:45:39 Train INFO: [Train]: [008][00051/00051] (100.0%)  Loss=1.0538  cls_loss=0.6987  reg_loss=0.3551  lr_det=1.0e-04  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.032s
2025-08-05 18:45:40 Train INFO: [Train]: Epoch 8 completed in 1848.1s (avg 35.540s/iter)
2025-08-05 18:45:40 Train INFO: [Train]: Final Loss=1.0538
2025-08-05 18:45:40 Train INFO: [Train]: Epoch 9 started (Total iterations: 52)
2025-08-05 18:48:21 Train INFO: [Train]: [009][00001/00051] (3.8%)  Loss=1.0404  cls_loss=0.6973  reg_loss=0.3431  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=4028s  iter_time=161.118s  fwd=2.116s/bwd=0.086s/opt=0.013s
2025-08-05 18:48:23 Train INFO: [Train]: [009][00002/00051] (5.8%)  Loss=1.0858  cls_loss=0.7307  reg_loss=0.3551  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2669s  iter_time=2.291s
2025-08-05 18:48:26 Train INFO: [Train]: [009][00003/00051] (7.7%)  Loss=1.0788  cls_loss=0.7267  reg_loss=0.3521  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1989s  iter_time=2.375s
2025-08-05 18:50:37 Train INFO: [Train]: [009][00004/00051] (9.6%)  Loss=1.0251  cls_loss=0.6912  reg_loss=0.3340  lr_det=1.0e-04  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2792s  iter_time=131.259s
2025-08-05 18:50:45 Train INFO: [Train]: [009][00005/00051] (11.5%)  Loss=1.0395  cls_loss=0.6983  reg_loss=0.3412  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2339s  iter_time=7.985s
2025-08-05 18:50:47 Train INFO: [Train]: [009][00006/00051] (13.5%)  Loss=1.0582  cls_loss=0.7108  reg_loss=0.3475  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1976s  iter_time=2.310s
2025-08-05 18:50:50 Train INFO: [Train]: [009][00007/00051] (15.4%)  Loss=1.0518  cls_loss=0.7056  reg_loss=0.3462  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1703s  iter_time=2.233s
2025-08-05 18:52:56 Train INFO: [Train]: [009][00008/00051] (17.3%)  Loss=1.0364  cls_loss=0.6929  reg_loss=0.3435  lr_det=9.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=2084s  iter_time=126.616s
2025-08-05 18:53:06 Train INFO: [Train]: [009][00009/00051] (19.2%)  Loss=1.0258  cls_loss=0.6865  reg_loss=0.3392  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1871s  iter_time=9.395s
2025-08-05 18:53:09 Train INFO: [Train]: [009][00010/00051] (21.2%)  Loss=1.0208  cls_loss=0.6839  reg_loss=0.3370  lr_det=9.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1673s  iter_time=3.307s
2025-08-05 18:53:11 Train INFO: [Train]: [009][00011/00051] (23.1%)  Loss=1.0396  cls_loss=0.6961  reg_loss=0.3434  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1504s  iter_time=2.268s
2025-08-05 18:55:14 Train INFO: [Train]: [009][00012/00051] (25.0%)  Loss=1.0246  cls_loss=0.6868  reg_loss=0.3379  lr_det=9.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1723s  iter_time=123.154s
2025-08-05 18:55:25 Train INFO: [Train]: [009][00013/00051] (26.9%)  Loss=1.0356  cls_loss=0.6949  reg_loss=0.3407  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1589s  iter_time=10.987s
2025-08-05 18:55:29 Train INFO: [Train]: [009][00014/00051] (28.8%)  Loss=1.0480  cls_loss=0.7018  reg_loss=0.3462  lr_det=9.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1454s  iter_time=4.146s
2025-08-05 18:55:32 Train INFO: [Train]: [009][00015/00051] (30.8%)  Loss=1.0457  cls_loss=0.7022  reg_loss=0.3435  lr_det=9.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1331s  iter_time=2.254s
2025-08-05 18:57:37 Train INFO: [Train]: [009][00016/00051] (32.7%)  Loss=1.0432  cls_loss=0.7003  reg_loss=0.3429  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1475s  iter_time=124.917s
2025-08-05 18:57:39 Train INFO: [Train]: [009][00017/00051] (34.6%)  Loss=1.0327  cls_loss=0.6931  reg_loss=0.3396  lr_det=9.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1358s  iter_time=2.326s
2025-08-05 18:57:45 Train INFO: [Train]: [009][00018/00051] (36.5%)  Loss=1.0296  cls_loss=0.6920  reg_loss=0.3376  lr_det=9.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1259s  iter_time=5.995s
2025-08-05 18:57:47 Train INFO: [Train]: [009][00019/00051] (38.5%)  Loss=1.0344  cls_loss=0.6947  reg_loss=0.3397  lr_det=9.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1164s  iter_time=2.247s
2025-08-05 18:59:54 Train INFO: [Train]: [009][00020/00051] (40.4%)  Loss=1.0277  cls_loss=0.6896  reg_loss=0.3381  lr_det=9.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1261s  iter_time=127.044s
2025-08-05 18:59:57 Train INFO: [Train]: [009][00021/00051] (42.3%)  Loss=1.0346  cls_loss=0.6945  reg_loss=0.3402  lr_det=9.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1168s  iter_time=2.282s
2025-08-05 19:00:03 Train INFO: [Train]: [009][00022/00051] (44.2%)  Loss=1.0479  cls_loss=0.7022  reg_loss=0.3456  lr_det=8.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1088s  iter_time=6.219s
2025-08-05 19:00:05 Train INFO: [Train]: [009][00023/00051] (46.2%)  Loss=1.0425  cls_loss=0.6994  reg_loss=0.3431  lr_det=8.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1009s  iter_time=2.373s
2025-08-05 19:02:16 Train INFO: [Train]: [009][00024/00051] (48.1%)  Loss=1.0453  cls_loss=0.7010  reg_loss=0.3443  lr_det=8.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1075s  iter_time=130.449s
2025-08-05 19:02:18 Train INFO: [Train]: [009][00025/00051] (50.0%)  Loss=1.0427  cls_loss=0.6992  reg_loss=0.3435  lr_det=8.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=998s  iter_time=2.320s
2025-08-05 19:02:23 Train INFO: [Train]: [009][00026/00051] (51.9%)  Loss=1.0429  cls_loss=0.7009  reg_loss=0.3420  lr_det=8.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=928s  iter_time=4.806s
2025-08-05 19:02:25 Train INFO: [Train]: [009][00027/00051] (53.8%)  Loss=1.0458  cls_loss=0.7016  reg_loss=0.3441  lr_det=8.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=861s  iter_time=2.284s
2025-08-05 19:04:30 Train INFO: [Train]: [009][00028/00051] (55.8%)  Loss=1.0433  cls_loss=0.6994  reg_loss=0.3439  lr_det=8.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=896s  iter_time=125.141s
2025-08-05 19:04:39 Train INFO: [Train]: [009][00029/00051] (57.7%)  Loss=1.0454  cls_loss=0.7008  reg_loss=0.3446  lr_det=8.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=835s  iter_time=9.026s
2025-08-05 19:04:42 Train INFO: [Train]: [009][00030/00051] (59.6%)  Loss=1.0446  cls_loss=0.7009  reg_loss=0.3438  lr_det=8.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=773s  iter_time=2.351s
2025-08-05 19:04:44 Train INFO: [Train]: [009][00031/00051] (61.5%)  Loss=1.0463  cls_loss=0.7019  reg_loss=0.3444  lr_det=8.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=715s  iter_time=2.243s
2025-08-05 19:06:52 Train INFO: [Train]: [009][00032/00051] (63.5%)  Loss=1.0547  cls_loss=0.7073  reg_loss=0.3474  lr_det=7.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=732s  iter_time=128.283s
2025-08-05 19:06:58 Train INFO: [Train]: [009][00033/00051] (65.4%)  Loss=1.0504  cls_loss=0.7049  reg_loss=0.3455  lr_det=7.7e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=676s  iter_time=5.487s
2025-08-05 19:07:00 Train INFO: [Train]: [009][00034/00051] (67.3%)  Loss=1.0537  cls_loss=0.7065  reg_loss=0.3472  lr_det=7.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=622s  iter_time=2.564s
2025-08-05 19:07:02 Train INFO: [Train]: [009][00035/00051] (69.2%)  Loss=1.0507  cls_loss=0.7044  reg_loss=0.3464  lr_det=7.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=570s  iter_time=2.316s
2025-08-05 19:09:13 Train INFO: [Train]: [009][00036/00051] (71.2%)  Loss=1.0533  cls_loss=0.7062  reg_loss=0.3471  lr_det=7.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=573s  iter_time=130.226s
2025-08-05 19:09:15 Train INFO: [Train]: [009][00037/00051] (73.1%)  Loss=1.0496  cls_loss=0.7040  reg_loss=0.3457  lr_det=7.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=521s  iter_time=2.455s
2025-08-05 19:09:21 Train INFO: [Train]: [009][00038/00051] (75.0%)  Loss=1.0527  cls_loss=0.7057  reg_loss=0.3470  lr_det=7.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=474s  iter_time=5.492s
2025-08-05 19:09:23 Train INFO: [Train]: [009][00039/00051] (76.9%)  Loss=1.0532  cls_loss=0.7055  reg_loss=0.3478  lr_det=6.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=427s  iter_time=2.307s
2025-08-05 19:11:34 Train INFO: [Train]: [009][00040/00051] (78.8%)  Loss=1.0546  cls_loss=0.7062  reg_loss=0.3484  lr_det=6.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=417s  iter_time=131.553s
2025-08-05 19:11:37 Train INFO: [Train]: [009][00041/00051] (80.8%)  Loss=1.0545  cls_loss=0.7056  reg_loss=0.3489  lr_det=6.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=371s  iter_time=2.297s
2025-08-05 19:11:40 Train INFO: [Train]: [009][00042/00051] (82.7%)  Loss=1.0550  cls_loss=0.7060  reg_loss=0.3489  lr_det=6.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=326s  iter_time=3.198s
2025-08-05 19:11:42 Train INFO: [Train]: [009][00043/00051] (84.6%)  Loss=1.0532  cls_loss=0.7046  reg_loss=0.3486  lr_det=6.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=284s  iter_time=2.316s
2025-08-05 19:13:55 Train INFO: [Train]: [009][00044/00051] (86.5%)  Loss=1.0517  cls_loss=0.7034  reg_loss=0.3483  lr_det=6.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=264s  iter_time=132.853s
2025-08-05 19:13:57 Train INFO: [Train]: [009][00045/00051] (88.5%)  Loss=1.0509  cls_loss=0.7020  reg_loss=0.3489  lr_det=6.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=221s  iter_time=2.288s
2025-08-05 19:14:00 Train INFO: [Train]: [009][00046/00051] (90.4%)  Loss=1.0516  cls_loss=0.7021  reg_loss=0.3495  lr_det=5.9e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=181s  iter_time=2.342s
2025-08-05 19:14:02 Train INFO: [Train]: [009][00047/00051] (92.3%)  Loss=1.0470  cls_loss=0.6990  reg_loss=0.3481  lr_det=5.8e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=142s  iter_time=2.326s
2025-08-05 19:16:10 Train INFO: [Train]: [009][00048/00051] (94.2%)  Loss=1.0490  cls_loss=0.7000  reg_loss=0.3491  lr_det=5.6e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=112s  iter_time=128.294s
2025-08-05 19:16:13 Train INFO: [Train]: [009][00049/00051] (96.2%)  Loss=1.0501  cls_loss=0.7004  reg_loss=0.3497  lr_det=5.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=73s  iter_time=2.161s
2025-08-05 19:16:15 Train INFO: [Train]: [009][00050/00051] (98.1%)  Loss=1.0542  cls_loss=0.7028  reg_loss=0.3514  lr_det=5.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.180s
2025-08-05 19:16:17 Train INFO: [Train]: [009][00051/00051] (100.0%)  Loss=1.0506  cls_loss=0.7005  reg_loss=0.3501  lr_det=5.2e-05  GPU=1392MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.032s
2025-08-05 19:16:17 Train INFO: [Train]: Epoch 9 completed in 1837.4s (avg 35.335s/iter)
2025-08-05 19:16:17 Train INFO: [Train]: Final Loss=1.0506
2025-08-05 19:16:18 Train INFO: [Val]: Epoch 9 Loss
2025-08-05 19:35:34 Train INFO: [Val]: [009]  Loss=1.3998  cls_loss=1.0253  reg_loss=0.3745  Average-mAP=0.15%
2025-08-05 19:35:35 Train INFO: Checkpoint saved at epoch 9
2025-08-05 19:35:35 Train INFO: [Train]: Epoch 10 started (Total iterations: 52)
2025-08-05 19:38:12 Train INFO: [Train]: [010][00001/00051] (3.8%)  Loss=0.9418  cls_loss=0.6330  reg_loss=0.3088  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3912s  iter_time=156.495s  fwd=2.143s/bwd=0.082s/opt=0.023s
2025-08-05 19:38:14 Train INFO: [Train]: [010][00002/00051] (5.8%)  Loss=0.9985  cls_loss=0.6712  reg_loss=0.3273  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2594s  iter_time=2.302s
2025-08-05 19:38:16 Train INFO: [Train]: [010][00003/00051] (7.7%)  Loss=0.9948  cls_loss=0.6699  reg_loss=0.3249  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1933s  iter_time=2.287s
2025-08-05 19:40:33 Train INFO: [Train]: [010][00004/00051] (9.6%)  Loss=0.9929  cls_loss=0.6672  reg_loss=0.3257  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2797s  iter_time=136.477s
2025-08-05 19:40:35 Train INFO: [Train]: [010][00005/00051] (11.5%)  Loss=1.0389  cls_loss=0.6907  reg_loss=0.3482  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2299s  iter_time=2.290s
2025-08-05 19:40:38 Train INFO: [Train]: [010][00006/00051] (13.5%)  Loss=1.0531  cls_loss=0.6998  reg_loss=0.3533  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1942s  iter_time=2.281s
2025-08-05 19:40:40 Train INFO: [Train]: [010][00007/00051] (15.4%)  Loss=1.0641  cls_loss=0.7073  reg_loss=0.3568  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1674s  iter_time=2.243s
2025-08-05 19:42:49 Train INFO: [Train]: [010][00008/00051] (17.3%)  Loss=1.0633  cls_loss=0.7070  reg_loss=0.3564  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2070s  iter_time=128.924s
2025-08-05 19:42:59 Train INFO: [Train]: [010][00009/00051] (19.2%)  Loss=1.0573  cls_loss=0.7028  reg_loss=0.3545  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1864s  iter_time=10.461s
2025-08-05 19:43:01 Train INFO: [Train]: [010][00010/00051] (21.2%)  Loss=1.0636  cls_loss=0.7065  reg_loss=0.3572  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1662s  iter_time=2.253s
2025-08-05 19:43:04 Train INFO: [Train]: [010][00011/00051] (23.1%)  Loss=1.0675  cls_loss=0.7097  reg_loss=0.3578  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1494s  iter_time=2.323s
2025-08-05 19:45:08 Train INFO: [Train]: [010][00012/00051] (25.0%)  Loss=1.0618  cls_loss=0.7048  reg_loss=0.3569  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1719s  iter_time=124.526s
2025-08-05 19:45:20 Train INFO: [Train]: [010][00013/00051] (26.9%)  Loss=1.0545  cls_loss=0.7012  reg_loss=0.3533  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1587s  iter_time=11.989s
2025-08-05 19:45:23 Train INFO: [Train]: [010][00014/00051] (28.8%)  Loss=1.0510  cls_loss=0.6989  reg_loss=0.3520  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1448s  iter_time=2.355s
2025-08-05 19:45:25 Train INFO: [Train]: [010][00015/00051] (30.8%)  Loss=1.0443  cls_loss=0.6945  reg_loss=0.3498  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1326s  iter_time=2.276s
2025-08-05 19:47:34 Train INFO: [Train]: [010][00016/00051] (32.7%)  Loss=1.0409  cls_loss=0.6929  reg_loss=0.3479  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1480s  iter_time=129.322s
2025-08-05 19:47:37 Train INFO: [Train]: [010][00017/00051] (34.6%)  Loss=1.0354  cls_loss=0.6900  reg_loss=0.3454  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1362s  iter_time=2.489s
2025-08-05 19:47:39 Train INFO: [Train]: [010][00018/00051] (36.5%)  Loss=1.0326  cls_loss=0.6888  reg_loss=0.3438  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1257s  iter_time=2.577s
2025-08-05 19:47:42 Train INFO: [Train]: [010][00019/00051] (38.5%)  Loss=1.0354  cls_loss=0.6899  reg_loss=0.3455  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1162s  iter_time=2.346s
2025-08-05 19:49:53 Train INFO: [Train]: [010][00020/00051] (40.4%)  Loss=1.0223  cls_loss=0.6807  reg_loss=0.3417  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1265s  iter_time=130.945s
2025-08-05 19:49:55 Train INFO: [Train]: [010][00021/00051] (42.3%)  Loss=1.0198  cls_loss=0.6798  reg_loss=0.3399  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1172s  iter_time=2.378s
2025-08-05 19:50:01 Train INFO: [Train]: [010][00022/00051] (44.2%)  Loss=1.0318  cls_loss=0.6886  reg_loss=0.3432  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1092s  iter_time=6.115s
2025-08-05 19:50:03 Train INFO: [Train]: [010][00023/00051] (46.2%)  Loss=1.0267  cls_loss=0.6852  reg_loss=0.3415  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1013s  iter_time=2.309s
2025-08-05 19:52:17 Train INFO: [Train]: [010][00024/00051] (48.1%)  Loss=1.0276  cls_loss=0.6857  reg_loss=0.3419  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1081s  iter_time=133.160s
2025-08-05 19:52:19 Train INFO: [Train]: [010][00025/00051] (50.0%)  Loss=1.0197  cls_loss=0.6803  reg_loss=0.3394  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1004s  iter_time=2.349s
2025-08-05 19:52:25 Train INFO: [Train]: [010][00026/00051] (51.9%)  Loss=1.0304  cls_loss=0.6871  reg_loss=0.3433  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=935s  iter_time=5.756s
2025-08-05 19:52:27 Train INFO: [Train]: [010][00027/00051] (53.8%)  Loss=1.0440  cls_loss=0.6957  reg_loss=0.3483  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=867s  iter_time=2.326s
2025-08-05 19:54:34 Train INFO: [Train]: [010][00028/00051] (55.8%)  Loss=1.0423  cls_loss=0.6950  reg_loss=0.3473  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=903s  iter_time=126.531s
2025-08-05 19:54:36 Train INFO: [Train]: [010][00029/00051] (57.7%)  Loss=1.0460  cls_loss=0.6974  reg_loss=0.3487  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=836s  iter_time=2.296s
2025-08-05 19:54:44 Train INFO: [Train]: [010][00030/00051] (59.6%)  Loss=1.0450  cls_loss=0.6970  reg_loss=0.3479  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=778s  iter_time=8.167s
2025-08-05 19:54:46 Train INFO: [Train]: [010][00031/00051] (61.5%)  Loss=1.0397  cls_loss=0.6935  reg_loss=0.3462  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=719s  iter_time=2.362s
2025-08-05 19:56:52 Train INFO: [Train]: [010][00032/00051] (63.5%)  Loss=1.0394  cls_loss=0.6934  reg_loss=0.3460  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=735s  iter_time=125.887s
2025-08-05 19:56:55 Train INFO: [Train]: [010][00033/00051] (65.4%)  Loss=1.0334  cls_loss=0.6895  reg_loss=0.3438  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=677s  iter_time=2.331s
2025-08-05 19:57:03 Train INFO: [Train]: [010][00034/00051] (67.3%)  Loss=1.0401  cls_loss=0.6943  reg_loss=0.3458  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=626s  iter_time=8.904s
2025-08-05 19:57:06 Train INFO: [Train]: [010][00035/00051] (69.2%)  Loss=1.0373  cls_loss=0.6926  reg_loss=0.3447  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=573s  iter_time=2.298s
2025-08-05 19:59:13 Train INFO: [Train]: [010][00036/00051] (71.2%)  Loss=1.0411  cls_loss=0.6950  reg_loss=0.3461  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=575s  iter_time=127.398s
2025-08-05 19:59:16 Train INFO: [Train]: [010][00037/00051] (73.1%)  Loss=1.0405  cls_loss=0.6946  reg_loss=0.3459  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=523s  iter_time=2.405s
2025-08-05 19:59:25 Train INFO: [Train]: [010][00038/00051] (75.0%)  Loss=1.0426  cls_loss=0.6951  reg_loss=0.3475  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=477s  iter_time=9.835s
2025-08-05 19:59:28 Train INFO: [Train]: [010][00039/00051] (76.9%)  Loss=1.0419  cls_loss=0.6937  reg_loss=0.3482  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.262s
2025-08-05 20:01:31 Train INFO: [Train]: [010][00040/00051] (78.8%)  Loss=1.0389  cls_loss=0.6915  reg_loss=0.3475  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=417s  iter_time=123.447s
2025-08-05 20:01:36 Train INFO: [Train]: [010][00041/00051] (80.8%)  Loss=1.0368  cls_loss=0.6902  reg_loss=0.3466  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=371s  iter_time=4.412s
2025-08-05 20:01:43 Train INFO: [Train]: [010][00042/00051] (82.7%)  Loss=1.0396  cls_loss=0.6928  reg_loss=0.3467  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=7.563s
2025-08-05 20:01:45 Train INFO: [Train]: [010][00043/00051] (84.6%)  Loss=1.0394  cls_loss=0.6926  reg_loss=0.3469  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=285s  iter_time=2.312s
2025-08-05 20:03:52 Train INFO: [Train]: [010][00044/00051] (86.5%)  Loss=1.0387  cls_loss=0.6919  reg_loss=0.3468  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=264s  iter_time=126.098s
2025-08-05 20:03:55 Train INFO: [Train]: [010][00045/00051] (88.5%)  Loss=1.0338  cls_loss=0.6886  reg_loss=0.3452  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=222s  iter_time=3.066s
2025-08-05 20:04:05 Train INFO: [Train]: [010][00046/00051] (90.4%)  Loss=1.0343  cls_loss=0.6893  reg_loss=0.3450  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=10.141s
2025-08-05 20:04:07 Train INFO: [Train]: [010][00047/00051] (92.3%)  Loss=1.0338  cls_loss=0.6890  reg_loss=0.3448  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.339s
2025-08-05 20:06:06 Train INFO: [Train]: [010][00048/00051] (94.2%)  Loss=1.0304  cls_loss=0.6868  reg_loss=0.3436  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=112s  iter_time=119.084s
2025-08-05 20:06:09 Train INFO: [Train]: [010][00049/00051] (96.2%)  Loss=1.0343  cls_loss=0.6897  reg_loss=0.3446  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=73s  iter_time=2.825s
2025-08-05 20:06:14 Train INFO: [Train]: [010][00050/00051] (98.1%)  Loss=1.0357  cls_loss=0.6908  reg_loss=0.3448  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=4.863s
2025-08-05 20:06:16 Train INFO: [Train]: [010][00051/00051] (100.0%)  Loss=1.0366  cls_loss=0.6911  reg_loss=0.3454  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.044s
2025-08-05 20:06:17 Train INFO: [Train]: Epoch 10 completed in 1841.2s (avg 35.408s/iter)
2025-08-05 20:06:17 Train INFO: [Train]: Final Loss=1.0366
2025-08-05 20:06:17 Train INFO: [Train]: Epoch 11 started (Total iterations: 52)
2025-08-05 20:08:56 Train INFO: [Train]: [011][00001/00051] (3.8%)  Loss=0.8532  cls_loss=0.5821  reg_loss=0.2711  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3994s  iter_time=159.741s  fwd=2.124s/bwd=0.076s/opt=0.019s
2025-08-05 20:08:59 Train INFO: [Train]: [011][00002/00051] (5.8%)  Loss=1.0033  cls_loss=0.6706  reg_loss=0.3328  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2647s  iter_time=2.317s
2025-08-05 20:09:01 Train INFO: [Train]: [011][00003/00051] (7.7%)  Loss=1.0757  cls_loss=0.7123  reg_loss=0.3634  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1972s  iter_time=2.294s
2025-08-05 20:11:15 Train INFO: [Train]: [011][00004/00051] (9.6%)  Loss=1.0753  cls_loss=0.7114  reg_loss=0.3638  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2804s  iter_time=133.971s
2025-08-05 20:11:24 Train INFO: [Train]: [011][00005/00051] (11.5%)  Loss=1.0623  cls_loss=0.7012  reg_loss=0.3611  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2358s  iter_time=9.258s
2025-08-05 20:11:27 Train INFO: [Train]: [011][00006/00051] (13.5%)  Loss=1.0570  cls_loss=0.6987  reg_loss=0.3583  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1992s  iter_time=2.330s
2025-08-05 20:11:29 Train INFO: [Train]: [011][00007/00051] (15.4%)  Loss=1.0601  cls_loss=0.6992  reg_loss=0.3609  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1718s  iter_time=2.366s
2025-08-05 20:13:34 Train INFO: [Train]: [011][00008/00051] (17.3%)  Loss=1.0413  cls_loss=0.6885  reg_loss=0.3529  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2091s  iter_time=125.395s
2025-08-05 20:13:45 Train INFO: [Train]: [011][00009/00051] (19.2%)  Loss=1.0320  cls_loss=0.6832  reg_loss=0.3488  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1882s  iter_time=10.403s
2025-08-05 20:13:47 Train INFO: [Train]: [011][00010/00051] (21.2%)  Loss=1.0330  cls_loss=0.6822  reg_loss=0.3508  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1679s  iter_time=2.288s
2025-08-05 20:13:49 Train INFO: [Train]: [011][00011/00051] (23.1%)  Loss=1.0361  cls_loss=0.6857  reg_loss=0.3504  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1509s  iter_time=2.320s
2025-08-05 20:15:56 Train INFO: [Train]: [011][00012/00051] (25.0%)  Loss=1.0259  cls_loss=0.6802  reg_loss=0.3457  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1739s  iter_time=126.959s
2025-08-05 20:16:05 Train INFO: [Train]: [011][00013/00051] (26.9%)  Loss=1.0337  cls_loss=0.6851  reg_loss=0.3486  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1598s  iter_time=9.180s
2025-08-05 20:16:13 Train INFO: [Train]: [011][00014/00051] (28.8%)  Loss=1.0330  cls_loss=0.6866  reg_loss=0.3464  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1470s  iter_time=7.253s
2025-08-05 20:16:15 Train INFO: [Train]: [011][00015/00051] (30.8%)  Loss=1.0369  cls_loss=0.6907  reg_loss=0.3463  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1346s  iter_time=2.344s
2025-08-05 20:18:17 Train INFO: [Train]: [011][00016/00051] (32.7%)  Loss=1.0330  cls_loss=0.6882  reg_loss=0.3448  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1483s  iter_time=121.929s
2025-08-05 20:18:22 Train INFO: [Train]: [011][00017/00051] (34.6%)  Loss=1.0235  cls_loss=0.6831  reg_loss=0.3403  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1370s  iter_time=4.741s
2025-08-05 20:18:30 Train INFO: [Train]: [011][00018/00051] (36.5%)  Loss=1.0171  cls_loss=0.6800  reg_loss=0.3371  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1274s  iter_time=8.657s
2025-08-05 20:18:33 Train INFO: [Train]: [011][00019/00051] (38.5%)  Loss=1.0242  cls_loss=0.6843  reg_loss=0.3399  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1178s  iter_time=2.337s
2025-08-05 20:20:37 Train INFO: [Train]: [011][00020/00051] (40.4%)  Loss=1.0141  cls_loss=0.6786  reg_loss=0.3356  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1269s  iter_time=123.844s
2025-08-05 20:20:40 Train INFO: [Train]: [011][00021/00051] (42.3%)  Loss=1.0198  cls_loss=0.6823  reg_loss=0.3375  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1178s  iter_time=3.753s
2025-08-05 20:20:52 Train INFO: [Train]: [011][00022/00051] (44.2%)  Loss=1.0247  cls_loss=0.6858  reg_loss=0.3389  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1103s  iter_time=11.438s
2025-08-05 20:20:54 Train INFO: [Train]: [011][00023/00051] (46.2%)  Loss=1.0178  cls_loss=0.6807  reg_loss=0.3371  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1024s  iter_time=2.274s
2025-08-05 20:22:56 Train INFO: [Train]: [011][00024/00051] (48.1%)  Loss=1.0221  cls_loss=0.6835  reg_loss=0.3387  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1080s  iter_time=122.418s
2025-08-05 20:23:00 Train INFO: [Train]: [011][00025/00051] (50.0%)  Loss=1.0187  cls_loss=0.6805  reg_loss=0.3382  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1003s  iter_time=3.310s
2025-08-05 20:23:13 Train INFO: [Train]: [011][00026/00051] (51.9%)  Loss=1.0240  cls_loss=0.6838  reg_loss=0.3402  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=941s  iter_time=12.778s
2025-08-05 20:23:15 Train INFO: [Train]: [011][00027/00051] (53.8%)  Loss=1.0256  cls_loss=0.6846  reg_loss=0.3409  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=873s  iter_time=2.289s
2025-08-05 20:25:16 Train INFO: [Train]: [011][00028/00051] (55.8%)  Loss=1.0237  cls_loss=0.6837  reg_loss=0.3400  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=904s  iter_time=120.994s
2025-08-05 20:25:20 Train INFO: [Train]: [011][00029/00051] (57.7%)  Loss=1.0248  cls_loss=0.6846  reg_loss=0.3402  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=838s  iter_time=4.003s
2025-08-05 20:25:31 Train INFO: [Train]: [011][00030/00051] (59.6%)  Loss=1.0215  cls_loss=0.6829  reg_loss=0.3386  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=782s  iter_time=11.651s
2025-08-05 20:25:34 Train INFO: [Train]: [011][00031/00051] (61.5%)  Loss=1.0159  cls_loss=0.6788  reg_loss=0.3371  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=723s  iter_time=2.293s
2025-08-05 20:27:34 Train INFO: [Train]: [011][00032/00051] (63.5%)  Loss=1.0192  cls_loss=0.6811  reg_loss=0.3381  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=736s  iter_time=120.635s
2025-08-05 20:27:37 Train INFO: [Train]: [011][00033/00051] (65.4%)  Loss=1.0162  cls_loss=0.6788  reg_loss=0.3373  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=678s  iter_time=2.297s
2025-08-05 20:27:54 Train INFO: [Train]: [011][00034/00051] (67.3%)  Loss=1.0210  cls_loss=0.6822  reg_loss=0.3388  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=630s  iter_time=16.919s
2025-08-05 20:27:56 Train INFO: [Train]: [011][00035/00051] (69.2%)  Loss=1.0251  cls_loss=0.6840  reg_loss=0.3411  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=578s  iter_time=2.395s
2025-08-05 20:30:02 Train INFO: [Train]: [011][00036/00051] (71.2%)  Loss=1.0272  cls_loss=0.6845  reg_loss=0.3427  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=578s  iter_time=125.959s
2025-08-05 20:30:04 Train INFO: [Train]: [011][00037/00051] (73.1%)  Loss=1.0277  cls_loss=0.6842  reg_loss=0.3435  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=526s  iter_time=2.328s
2025-08-05 20:30:20 Train INFO: [Train]: [011][00038/00051] (75.0%)  Loss=1.0274  cls_loss=0.6840  reg_loss=0.3434  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=481s  iter_time=15.213s
2025-08-05 20:30:22 Train INFO: [Train]: [011][00039/00051] (76.9%)  Loss=1.0253  cls_loss=0.6823  reg_loss=0.3430  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=434s  iter_time=2.313s
2025-08-05 20:32:24 Train INFO: [Train]: [011][00040/00051] (78.8%)  Loss=1.0282  cls_loss=0.6838  reg_loss=0.3443  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=121.924s
2025-08-05 20:32:26 Train INFO: [Train]: [011][00041/00051] (80.8%)  Loss=1.0247  cls_loss=0.6818  reg_loss=0.3429  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=374s  iter_time=2.337s
2025-08-05 20:32:40 Train INFO: [Train]: [011][00042/00051] (82.7%)  Loss=1.0279  cls_loss=0.6835  reg_loss=0.3444  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=331s  iter_time=13.417s
2025-08-05 20:32:42 Train INFO: [Train]: [011][00043/00051] (84.6%)  Loss=1.0268  cls_loss=0.6826  reg_loss=0.3442  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=288s  iter_time=2.348s
2025-08-05 20:34:46 Train INFO: [Train]: [011][00044/00051] (86.5%)  Loss=1.0264  cls_loss=0.6823  reg_loss=0.3442  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=123.770s
2025-08-05 20:34:48 Train INFO: [Train]: [011][00045/00051] (88.5%)  Loss=1.0236  cls_loss=0.6801  reg_loss=0.3435  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.289s
2025-08-05 20:35:03 Train INFO: [Train]: [011][00046/00051] (90.4%)  Loss=1.0261  cls_loss=0.6814  reg_loss=0.3447  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=184s  iter_time=15.307s
2025-08-05 20:35:06 Train INFO: [Train]: [011][00047/00051] (92.3%)  Loss=1.0270  cls_loss=0.6817  reg_loss=0.3453  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.296s
2025-08-05 20:37:01 Train INFO: [Train]: [011][00048/00051] (94.2%)  Loss=1.0248  cls_loss=0.6806  reg_loss=0.3442  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=115.953s
2025-08-05 20:37:04 Train INFO: [Train]: [011][00049/00051] (96.2%)  Loss=1.0242  cls_loss=0.6807  reg_loss=0.3435  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.180s
2025-08-05 20:37:12 Train INFO: [Train]: [011][00050/00051] (98.1%)  Loss=1.0249  cls_loss=0.6817  reg_loss=0.3432  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=8.201s
2025-08-05 20:37:14 Train INFO: [Train]: [011][00051/00051] (100.0%)  Loss=1.0238  cls_loss=0.6808  reg_loss=0.3430  lr_det=4.8e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.028s
2025-08-05 20:37:15 Train INFO: [Train]: Epoch 11 completed in 1858.0s (avg 35.731s/iter)
2025-08-05 20:37:15 Train INFO: [Train]: Final Loss=1.0238
2025-08-05 20:37:15 Train INFO: [Train]: Epoch 12 started (Total iterations: 52)
2025-08-05 20:40:03 Train INFO: [Train]: [012][00001/00051] (3.8%)  Loss=1.0383  cls_loss=0.6835  reg_loss=0.3548  lr_det=5.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4202s  iter_time=168.070s  fwd=2.142s/bwd=0.077s/opt=0.023s
2025-08-05 20:40:05 Train INFO: [Train]: [012][00002/00051] (5.8%)  Loss=1.1083  cls_loss=0.7237  reg_loss=0.3847  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2782s  iter_time=2.282s
2025-08-05 20:40:07 Train INFO: [Train]: [012][00003/00051] (7.7%)  Loss=1.1022  cls_loss=0.7295  reg_loss=0.3727  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2071s  iter_time=2.252s
2025-08-05 20:42:22 Train INFO: [Train]: [012][00004/00051] (9.6%)  Loss=1.0576  cls_loss=0.6997  reg_loss=0.3578  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2892s  iter_time=135.041s
2025-08-05 20:42:25 Train INFO: [Train]: [012][00005/00051] (11.5%)  Loss=1.0706  cls_loss=0.7082  reg_loss=0.3624  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2376s  iter_time=2.323s
2025-08-05 20:42:27 Train INFO: [Train]: [012][00006/00051] (13.5%)  Loss=1.0875  cls_loss=0.7206  reg_loss=0.3669  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2007s  iter_time=2.298s
2025-08-05 20:42:29 Train INFO: [Train]: [012][00007/00051] (15.4%)  Loss=1.0742  cls_loss=0.7129  reg_loss=0.3613  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1730s  iter_time=2.338s
2025-08-05 20:44:39 Train INFO: [Train]: [012][00008/00051] (17.3%)  Loss=1.0598  cls_loss=0.7023  reg_loss=0.3574  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2123s  iter_time=129.641s
2025-08-05 20:44:48 Train INFO: [Train]: [012][00009/00051] (19.2%)  Loss=1.0535  cls_loss=0.7009  reg_loss=0.3526  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1904s  iter_time=8.978s
2025-08-05 20:44:50 Train INFO: [Train]: [012][00010/00051] (21.2%)  Loss=1.0458  cls_loss=0.6940  reg_loss=0.3517  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1698s  iter_time=2.321s
2025-08-05 20:44:53 Train INFO: [Train]: [012][00011/00051] (23.1%)  Loss=1.0681  cls_loss=0.7060  reg_loss=0.3621  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1526s  iter_time=2.336s
2025-08-05 20:46:59 Train INFO: [Train]: [012][00012/00051] (25.0%)  Loss=1.0468  cls_loss=0.6937  reg_loss=0.3532  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1754s  iter_time=126.894s
2025-08-05 20:47:10 Train INFO: [Train]: [012][00013/00051] (26.9%)  Loss=1.0570  cls_loss=0.7034  reg_loss=0.3536  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1617s  iter_time=10.784s
2025-08-05 20:47:13 Train INFO: [Train]: [012][00014/00051] (28.8%)  Loss=1.0611  cls_loss=0.7063  reg_loss=0.3548  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1475s  iter_time=2.336s
2025-08-05 20:47:15 Train INFO: [Train]: [012][00015/00051] (30.8%)  Loss=1.0501  cls_loss=0.7005  reg_loss=0.3496  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1351s  iter_time=2.330s
2025-08-05 20:49:24 Train INFO: [Train]: [012][00016/00051] (32.7%)  Loss=1.0425  cls_loss=0.6952  reg_loss=0.3473  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1502s  iter_time=129.453s
2025-08-05 20:49:27 Train INFO: [Train]: [012][00017/00051] (34.6%)  Loss=1.0351  cls_loss=0.6908  reg_loss=0.3443  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1383s  iter_time=2.506s
2025-08-05 20:49:29 Train INFO: [Train]: [012][00018/00051] (36.5%)  Loss=1.0303  cls_loss=0.6881  reg_loss=0.3422  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1276s  iter_time=2.290s
2025-08-05 20:49:31 Train INFO: [Train]: [012][00019/00051] (38.5%)  Loss=1.0367  cls_loss=0.6923  reg_loss=0.3445  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1179s  iter_time=2.260s
2025-08-05 20:51:43 Train INFO: [Train]: [012][00020/00051] (40.4%)  Loss=1.0269  cls_loss=0.6860  reg_loss=0.3409  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1282s  iter_time=131.650s
2025-08-05 20:51:46 Train INFO: [Train]: [012][00021/00051] (42.3%)  Loss=1.0338  cls_loss=0.6898  reg_loss=0.3440  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1188s  iter_time=2.692s
2025-08-05 20:51:48 Train INFO: [Train]: [012][00022/00051] (44.2%)  Loss=1.0443  cls_loss=0.6964  reg_loss=0.3479  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1101s  iter_time=2.357s
2025-08-05 20:51:50 Train INFO: [Train]: [012][00023/00051] (46.2%)  Loss=1.0395  cls_loss=0.6931  reg_loss=0.3464  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1022s  iter_time=2.342s
2025-08-05 20:54:02 Train INFO: [Train]: [012][00024/00051] (48.1%)  Loss=1.0432  cls_loss=0.6951  reg_loss=0.3481  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1088s  iter_time=132.039s
2025-08-05 20:54:10 Train INFO: [Train]: [012][00025/00051] (50.0%)  Loss=1.0457  cls_loss=0.6964  reg_loss=0.3493  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1015s  iter_time=7.367s
2025-08-05 20:54:12 Train INFO: [Train]: [012][00026/00051] (51.9%)  Loss=1.0457  cls_loss=0.6963  reg_loss=0.3494  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=942s  iter_time=2.308s
2025-08-05 20:54:14 Train INFO: [Train]: [012][00027/00051] (53.8%)  Loss=1.0487  cls_loss=0.6981  reg_loss=0.3507  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=874s  iter_time=2.344s
2025-08-05 20:56:21 Train INFO: [Train]: [012][00028/00051] (55.8%)  Loss=1.0427  cls_loss=0.6945  reg_loss=0.3482  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=910s  iter_time=126.906s
2025-08-05 20:56:33 Train INFO: [Train]: [012][00029/00051] (57.7%)  Loss=1.0424  cls_loss=0.6939  reg_loss=0.3485  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=849s  iter_time=11.218s
2025-08-05 20:56:35 Train INFO: [Train]: [012][00030/00051] (59.6%)  Loss=1.0407  cls_loss=0.6931  reg_loss=0.3476  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=786s  iter_time=2.239s
2025-08-05 20:56:37 Train INFO: [Train]: [012][00031/00051] (61.5%)  Loss=1.0421  cls_loss=0.6944  reg_loss=0.3477  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=727s  iter_time=2.277s
2025-08-05 20:58:40 Train INFO: [Train]: [012][00032/00051] (63.5%)  Loss=1.0439  cls_loss=0.6951  reg_loss=0.3488  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=740s  iter_time=122.635s
2025-08-05 20:58:50 Train INFO: [Train]: [012][00033/00051] (65.4%)  Loss=1.0461  cls_loss=0.6962  reg_loss=0.3499  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=686s  iter_time=9.785s
2025-08-05 20:58:52 Train INFO: [Train]: [012][00034/00051] (67.3%)  Loss=1.0548  cls_loss=0.7021  reg_loss=0.3526  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=630s  iter_time=2.340s
2025-08-05 20:58:54 Train INFO: [Train]: [012][00035/00051] (69.2%)  Loss=1.0536  cls_loss=0.7007  reg_loss=0.3529  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=578s  iter_time=2.286s
2025-08-05 21:01:01 Train INFO: [Train]: [012][00036/00051] (71.2%)  Loss=1.0551  cls_loss=0.7016  reg_loss=0.3535  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=578s  iter_time=126.515s
2025-08-05 21:01:10 Train INFO: [Train]: [012][00037/00051] (73.1%)  Loss=1.0540  cls_loss=0.7006  reg_loss=0.3534  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=529s  iter_time=9.597s
2025-08-05 21:01:13 Train INFO: [Train]: [012][00038/00051] (75.0%)  Loss=1.0520  cls_loss=0.6988  reg_loss=0.3532  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=479s  iter_time=2.325s
2025-08-05 21:01:15 Train INFO: [Train]: [012][00039/00051] (76.9%)  Loss=1.0489  cls_loss=0.6969  reg_loss=0.3520  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=432s  iter_time=2.251s
2025-08-05 21:03:23 Train INFO: [Train]: [012][00040/00051] (78.8%)  Loss=1.0491  cls_loss=0.6973  reg_loss=0.3518  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=421s  iter_time=127.960s
2025-08-05 21:03:33 Train INFO: [Train]: [012][00041/00051] (80.8%)  Loss=1.0485  cls_loss=0.6966  reg_loss=0.3519  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=376s  iter_time=10.408s
2025-08-05 21:03:36 Train INFO: [Train]: [012][00042/00051] (82.7%)  Loss=1.0495  cls_loss=0.6975  reg_loss=0.3520  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=331s  iter_time=2.336s
2025-08-05 21:03:38 Train INFO: [Train]: [012][00043/00051] (84.6%)  Loss=1.0453  cls_loss=0.6947  reg_loss=0.3506  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=288s  iter_time=2.230s
2025-08-05 21:05:47 Train INFO: [Train]: [012][00044/00051] (86.5%)  Loss=1.0459  cls_loss=0.6947  reg_loss=0.3512  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=129.598s
2025-08-05 21:05:52 Train INFO: [Train]: [012][00045/00051] (88.5%)  Loss=1.0420  cls_loss=0.6923  reg_loss=0.3497  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=4.398s
2025-08-05 21:05:55 Train INFO: [Train]: [012][00046/00051] (90.4%)  Loss=1.0430  cls_loss=0.6924  reg_loss=0.3506  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=3.338s
2025-08-05 21:05:57 Train INFO: [Train]: [012][00047/00051] (92.3%)  Loss=1.0430  cls_loss=0.6927  reg_loss=0.3503  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.261s
2025-08-05 21:08:03 Train INFO: [Train]: [012][00048/00051] (94.2%)  Loss=1.0450  cls_loss=0.6938  reg_loss=0.3512  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=125.620s
2025-08-05 21:08:05 Train INFO: [Train]: [012][00049/00051] (96.2%)  Loss=1.0450  cls_loss=0.6937  reg_loss=0.3513  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.168s
2025-08-05 21:08:07 Train INFO: [Train]: [012][00050/00051] (98.1%)  Loss=1.0433  cls_loss=0.6930  reg_loss=0.3503  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.172s
2025-08-05 21:08:09 Train INFO: [Train]: [012][00051/00051] (100.0%)  Loss=1.0407  cls_loss=0.6911  reg_loss=0.3496  lr_det=1.0e-04  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.044s
2025-08-05 21:08:10 Train INFO: [Train]: Epoch 12 completed in 1855.5s (avg 35.683s/iter)
2025-08-05 21:08:10 Train INFO: [Train]: Final Loss=1.0407
2025-08-05 21:08:10 Train INFO: [Train]: Epoch 13 started (Total iterations: 52)
2025-08-05 21:10:53 Train INFO: [Train]: [013][00001/00051] (3.8%)  Loss=0.9610  cls_loss=0.6439  reg_loss=0.3171  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4062s  iter_time=162.475s  fwd=2.125s/bwd=0.084s/opt=0.020s
2025-08-05 21:10:55 Train INFO: [Train]: [013][00002/00051] (5.8%)  Loss=1.0391  cls_loss=0.6872  reg_loss=0.3519  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2693s  iter_time=2.410s
2025-08-05 21:10:57 Train INFO: [Train]: [013][00003/00051] (7.7%)  Loss=1.0426  cls_loss=0.6903  reg_loss=0.3523  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2006s  iter_time=2.270s
2025-08-05 21:13:16 Train INFO: [Train]: [013][00004/00051] (9.6%)  Loss=1.0605  cls_loss=0.7004  reg_loss=0.3601  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2877s  iter_time=138.954s
2025-08-05 21:13:19 Train INFO: [Train]: [013][00005/00051] (11.5%)  Loss=1.0424  cls_loss=0.6882  reg_loss=0.3541  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2365s  iter_time=2.303s
2025-08-05 21:13:21 Train INFO: [Train]: [013][00006/00051] (13.5%)  Loss=1.0632  cls_loss=0.7027  reg_loss=0.3605  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1997s  iter_time=2.254s
2025-08-05 21:13:23 Train INFO: [Train]: [013][00007/00051] (15.4%)  Loss=1.0710  cls_loss=0.7044  reg_loss=0.3666  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1721s  iter_time=2.264s
2025-08-05 21:15:31 Train INFO: [Train]: [013][00008/00051] (17.3%)  Loss=1.0520  cls_loss=0.6916  reg_loss=0.3604  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2105s  iter_time=127.698s
2025-08-05 21:15:38 Train INFO: [Train]: [013][00009/00051] (19.2%)  Loss=1.0352  cls_loss=0.6834  reg_loss=0.3518  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1882s  iter_time=7.519s
2025-08-05 21:15:41 Train INFO: [Train]: [013][00010/00051] (21.2%)  Loss=1.0313  cls_loss=0.6813  reg_loss=0.3500  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1680s  iter_time=2.477s
2025-08-05 21:15:43 Train INFO: [Train]: [013][00011/00051] (23.1%)  Loss=1.0322  cls_loss=0.6815  reg_loss=0.3506  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1510s  iter_time=2.346s
2025-08-05 21:17:53 Train INFO: [Train]: [013][00012/00051] (25.0%)  Loss=1.0421  cls_loss=0.6891  reg_loss=0.3530  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1748s  iter_time=129.607s
2025-08-05 21:18:04 Train INFO: [Train]: [013][00013/00051] (26.9%)  Loss=1.0547  cls_loss=0.6972  reg_loss=0.3575  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1612s  iter_time=11.385s
2025-08-05 21:18:06 Train INFO: [Train]: [013][00014/00051] (28.8%)  Loss=1.0610  cls_loss=0.7021  reg_loss=0.3589  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1471s  iter_time=2.374s
2025-08-05 21:18:09 Train INFO: [Train]: [013][00015/00051] (30.8%)  Loss=1.0516  cls_loss=0.6967  reg_loss=0.3549  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1347s  iter_time=2.241s
2025-08-05 21:20:17 Train INFO: [Train]: [013][00016/00051] (32.7%)  Loss=1.0562  cls_loss=0.6984  reg_loss=0.3578  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1495s  iter_time=127.769s
2025-08-05 21:20:23 Train INFO: [Train]: [013][00017/00051] (34.6%)  Loss=1.0496  cls_loss=0.6941  reg_loss=0.3555  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1384s  iter_time=6.092s
2025-08-05 21:20:25 Train INFO: [Train]: [013][00018/00051] (36.5%)  Loss=1.0447  cls_loss=0.6907  reg_loss=0.3540  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1276s  iter_time=2.309s
2025-08-05 21:20:27 Train INFO: [Train]: [013][00019/00051] (38.5%)  Loss=1.0502  cls_loss=0.6944  reg_loss=0.3558  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1179s  iter_time=2.284s
2025-08-05 21:22:34 Train INFO: [Train]: [013][00020/00051] (40.4%)  Loss=1.0422  cls_loss=0.6889  reg_loss=0.3533  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1276s  iter_time=127.266s
2025-08-05 21:22:41 Train INFO: [Train]: [013][00021/00051] (42.3%)  Loss=1.0412  cls_loss=0.6877  reg_loss=0.3535  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1187s  iter_time=6.297s
2025-08-05 21:22:43 Train INFO: [Train]: [013][00022/00051] (44.2%)  Loss=1.0590  cls_loss=0.6992  reg_loss=0.3597  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1101s  iter_time=2.320s
2025-08-05 21:22:45 Train INFO: [Train]: [013][00023/00051] (46.2%)  Loss=1.0563  cls_loss=0.6968  reg_loss=0.3596  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1021s  iter_time=2.344s
2025-08-05 21:24:55 Train INFO: [Train]: [013][00024/00051] (48.1%)  Loss=1.0601  cls_loss=0.6998  reg_loss=0.3603  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1085s  iter_time=129.351s
2025-08-05 21:25:05 Train INFO: [Train]: [013][00025/00051] (50.0%)  Loss=1.0628  cls_loss=0.7015  reg_loss=0.3613  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1015s  iter_time=10.189s
2025-08-05 21:25:07 Train INFO: [Train]: [013][00026/00051] (51.9%)  Loss=1.0641  cls_loss=0.7030  reg_loss=0.3611  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=942s  iter_time=2.332s
2025-08-05 21:25:10 Train INFO: [Train]: [013][00027/00051] (53.8%)  Loss=1.0627  cls_loss=0.7017  reg_loss=0.3610  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=874s  iter_time=2.327s
2025-08-05 21:27:13 Train INFO: [Train]: [013][00028/00051] (55.8%)  Loss=1.0677  cls_loss=0.7041  reg_loss=0.3637  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=906s  iter_time=123.078s
2025-08-05 21:27:29 Train INFO: [Train]: [013][00029/00051] (57.7%)  Loss=1.0674  cls_loss=0.7046  reg_loss=0.3627  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=850s  iter_time=16.632s
2025-08-05 21:27:32 Train INFO: [Train]: [013][00030/00051] (59.6%)  Loss=1.0663  cls_loss=0.7037  reg_loss=0.3627  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=787s  iter_time=2.404s
2025-08-05 21:27:34 Train INFO: [Train]: [013][00031/00051] (61.5%)  Loss=1.0656  cls_loss=0.7031  reg_loss=0.3625  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=727s  iter_time=2.322s
2025-08-05 21:29:37 Train INFO: [Train]: [013][00032/00051] (63.5%)  Loss=1.0654  cls_loss=0.7032  reg_loss=0.3621  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=741s  iter_time=122.426s
2025-08-05 21:29:49 Train INFO: [Train]: [013][00033/00051] (65.4%)  Loss=1.0581  cls_loss=0.6984  reg_loss=0.3597  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=688s  iter_time=12.970s
2025-08-05 21:29:52 Train INFO: [Train]: [013][00034/00051] (67.3%)  Loss=1.0593  cls_loss=0.6995  reg_loss=0.3598  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=632s  iter_time=2.277s
2025-08-05 21:29:54 Train INFO: [Train]: [013][00035/00051] (69.2%)  Loss=1.0632  cls_loss=0.7019  reg_loss=0.3613  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=580s  iter_time=2.327s
2025-08-05 21:32:00 Train INFO: [Train]: [013][00036/00051] (71.2%)  Loss=1.0630  cls_loss=0.7019  reg_loss=0.3610  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=580s  iter_time=126.136s
2025-08-05 21:32:11 Train INFO: [Train]: [013][00037/00051] (73.1%)  Loss=1.0576  cls_loss=0.6983  reg_loss=0.3593  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=531s  iter_time=10.973s
2025-08-05 21:32:14 Train INFO: [Train]: [013][00038/00051] (75.0%)  Loss=1.0585  cls_loss=0.6985  reg_loss=0.3600  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=481s  iter_time=2.346s
2025-08-05 21:32:18 Train INFO: [Train]: [013][00039/00051] (76.9%)  Loss=1.0588  cls_loss=0.6986  reg_loss=0.3603  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=434s  iter_time=4.339s
2025-08-05 21:34:21 Train INFO: [Train]: [013][00040/00051] (78.8%)  Loss=1.0564  cls_loss=0.6974  reg_loss=0.3589  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=421s  iter_time=123.302s
2025-08-05 21:34:31 Train INFO: [Train]: [013][00041/00051] (80.8%)  Loss=1.0527  cls_loss=0.6948  reg_loss=0.3579  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=376s  iter_time=10.182s
2025-08-05 21:34:34 Train INFO: [Train]: [013][00042/00051] (82.7%)  Loss=1.0528  cls_loss=0.6947  reg_loss=0.3581  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=331s  iter_time=2.339s
2025-08-05 21:34:39 Train INFO: [Train]: [013][00043/00051] (84.6%)  Loss=1.0513  cls_loss=0.6943  reg_loss=0.3570  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=289s  iter_time=5.167s
2025-08-05 21:36:49 Train INFO: [Train]: [013][00044/00051] (86.5%)  Loss=1.0483  cls_loss=0.6920  reg_loss=0.3563  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=267s  iter_time=129.864s
2025-08-05 21:36:53 Train INFO: [Train]: [013][00045/00051] (88.5%)  Loss=1.0425  cls_loss=0.6885  reg_loss=0.3541  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=225s  iter_time=4.042s
2025-08-05 21:36:56 Train INFO: [Train]: [013][00046/00051] (90.4%)  Loss=1.0442  cls_loss=0.6899  reg_loss=0.3543  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=184s  iter_time=3.589s
2025-08-05 21:36:59 Train INFO: [Train]: [013][00047/00051] (92.3%)  Loss=1.0426  cls_loss=0.6890  reg_loss=0.3537  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.267s
2025-08-05 21:39:08 Train INFO: [Train]: [013][00048/00051] (94.2%)  Loss=1.0439  cls_loss=0.6901  reg_loss=0.3538  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=114s  iter_time=129.716s
2025-08-05 21:39:11 Train INFO: [Train]: [013][00049/00051] (96.2%)  Loss=1.0451  cls_loss=0.6911  reg_loss=0.3539  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.165s
2025-08-05 21:39:13 Train INFO: [Train]: [013][00050/00051] (98.1%)  Loss=1.0468  cls_loss=0.6916  reg_loss=0.3552  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=37s  iter_time=2.163s
2025-08-05 21:39:15 Train INFO: [Train]: [013][00051/00051] (100.0%)  Loss=1.0464  cls_loss=0.6911  reg_loss=0.3553  lr_det=5.2e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.050s
2025-08-05 21:39:15 Train INFO: [Train]: Epoch 13 completed in 1865.3s (avg 35.871s/iter)
2025-08-05 21:39:15 Train INFO: [Train]: Final Loss=1.0464
2025-08-05 21:39:15 Train INFO: [Train]: Epoch 14 started (Total iterations: 52)
2025-08-05 21:41:54 Train INFO: [Train]: [014][00001/00051] (3.8%)  Loss=0.9342  cls_loss=0.6176  reg_loss=0.3166  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3970s  iter_time=158.788s  fwd=2.125s/bwd=0.069s/opt=0.020s
2025-08-05 21:41:56 Train INFO: [Train]: [014][00002/00051] (5.8%)  Loss=1.0426  cls_loss=0.6887  reg_loss=0.3539  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2630s  iter_time=2.256s
2025-08-05 21:41:59 Train INFO: [Train]: [014][00003/00051] (7.7%)  Loss=1.0508  cls_loss=0.6945  reg_loss=0.3563  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1961s  iter_time=2.336s
2025-08-05 21:44:12 Train INFO: [Train]: [014][00004/00051] (9.6%)  Loss=1.0473  cls_loss=0.6944  reg_loss=0.3529  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2787s  iter_time=133.152s
2025-08-05 21:44:20 Train INFO: [Train]: [014][00005/00051] (11.5%)  Loss=1.0333  cls_loss=0.6840  reg_loss=0.3494  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2338s  iter_time=8.371s
2025-08-05 21:44:23 Train INFO: [Train]: [014][00006/00051] (13.5%)  Loss=1.0615  cls_loss=0.7028  reg_loss=0.3588  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1975s  iter_time=2.351s
2025-08-05 21:44:25 Train INFO: [Train]: [014][00007/00051] (15.4%)  Loss=1.0678  cls_loss=0.7093  reg_loss=0.3585  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1702s  iter_time=2.274s
2025-08-05 21:46:30 Train INFO: [Train]: [014][00008/00051] (17.3%)  Loss=1.0627  cls_loss=0.7065  reg_loss=0.3562  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2077s  iter_time=125.281s
2025-08-05 21:46:41 Train INFO: [Train]: [014][00009/00051] (19.2%)  Loss=1.0508  cls_loss=0.6976  reg_loss=0.3532  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1873s  iter_time=11.108s
2025-08-05 21:46:44 Train INFO: [Train]: [014][00010/00051] (21.2%)  Loss=1.0438  cls_loss=0.6927  reg_loss=0.3511  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1671s  iter_time=2.297s
2025-08-05 21:46:46 Train INFO: [Train]: [014][00011/00051] (23.1%)  Loss=1.0613  cls_loss=0.7024  reg_loss=0.3589  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1502s  iter_time=2.237s
2025-08-05 21:48:53 Train INFO: [Train]: [014][00012/00051] (25.0%)  Loss=1.0571  cls_loss=0.7004  reg_loss=0.3567  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1734s  iter_time=127.549s
2025-08-05 21:49:05 Train INFO: [Train]: [014][00013/00051] (26.9%)  Loss=1.0669  cls_loss=0.7060  reg_loss=0.3609  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1600s  iter_time=11.430s
2025-08-05 21:49:07 Train INFO: [Train]: [014][00014/00051] (28.8%)  Loss=1.0828  cls_loss=0.7196  reg_loss=0.3632  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1460s  iter_time=2.332s
2025-08-05 21:49:10 Train INFO: [Train]: [014][00015/00051] (30.8%)  Loss=1.0856  cls_loss=0.7206  reg_loss=0.3650  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1337s  iter_time=2.315s
2025-08-05 21:51:19 Train INFO: [Train]: [014][00016/00051] (32.7%)  Loss=1.0804  cls_loss=0.7166  reg_loss=0.3638  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1489s  iter_time=129.326s
2025-08-05 21:51:24 Train INFO: [Train]: [014][00017/00051] (34.6%)  Loss=1.0742  cls_loss=0.7123  reg_loss=0.3619  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1375s  iter_time=4.630s
2025-08-05 21:51:26 Train INFO: [Train]: [014][00018/00051] (36.5%)  Loss=1.0671  cls_loss=0.7093  reg_loss=0.3577  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1269s  iter_time=2.317s
2025-08-05 21:51:28 Train INFO: [Train]: [014][00019/00051] (38.5%)  Loss=1.0640  cls_loss=0.7076  reg_loss=0.3564  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1172s  iter_time=2.319s
2025-08-05 21:53:37 Train INFO: [Train]: [014][00020/00051] (40.4%)  Loss=1.0485  cls_loss=0.6968  reg_loss=0.3516  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1272s  iter_time=128.866s
2025-08-05 21:53:40 Train INFO: [Train]: [014][00021/00051] (42.3%)  Loss=1.0537  cls_loss=0.7003  reg_loss=0.3535  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1178s  iter_time=2.588s
2025-08-05 21:53:45 Train INFO: [Train]: [014][00022/00051] (44.2%)  Loss=1.0549  cls_loss=0.7007  reg_loss=0.3542  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1096s  iter_time=5.285s
2025-08-05 21:53:47 Train INFO: [Train]: [014][00023/00051] (46.2%)  Loss=1.0475  cls_loss=0.6954  reg_loss=0.3521  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1017s  iter_time=2.337s
2025-08-05 21:56:03 Train INFO: [Train]: [014][00024/00051] (48.1%)  Loss=1.0496  cls_loss=0.6965  reg_loss=0.3531  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1088s  iter_time=135.429s
2025-08-05 21:56:05 Train INFO: [Train]: [014][00025/00051] (50.0%)  Loss=1.0480  cls_loss=0.6951  reg_loss=0.3529  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1010s  iter_time=2.305s
2025-08-05 21:56:07 Train INFO: [Train]: [014][00026/00051] (51.9%)  Loss=1.0531  cls_loss=0.6983  reg_loss=0.3548  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=937s  iter_time=2.312s
2025-08-05 21:56:10 Train INFO: [Train]: [014][00027/00051] (53.8%)  Loss=1.0474  cls_loss=0.6948  reg_loss=0.3526  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=869s  iter_time=2.279s
2025-08-05 21:58:20 Train INFO: [Train]: [014][00028/00051] (55.8%)  Loss=1.0460  cls_loss=0.6934  reg_loss=0.3526  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=908s  iter_time=130.682s
2025-08-05 21:58:25 Train INFO: [Train]: [014][00029/00051] (57.7%)  Loss=1.0478  cls_loss=0.6939  reg_loss=0.3539  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=843s  iter_time=4.579s
2025-08-05 21:58:27 Train INFO: [Train]: [014][00030/00051] (59.6%)  Loss=1.0488  cls_loss=0.6942  reg_loss=0.3546  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=780s  iter_time=2.222s
2025-08-05 21:58:29 Train INFO: [Train]: [014][00031/00051] (61.5%)  Loss=1.0452  cls_loss=0.6917  reg_loss=0.3535  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=721s  iter_time=2.300s
2025-08-05 22:00:42 Train INFO: [Train]: [014][00032/00051] (63.5%)  Loss=1.0433  cls_loss=0.6907  reg_loss=0.3526  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=741s  iter_time=132.768s
2025-08-05 22:00:44 Train INFO: [Train]: [014][00033/00051] (65.4%)  Loss=1.0390  cls_loss=0.6877  reg_loss=0.3513  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=682s  iter_time=2.280s
2025-08-05 22:00:47 Train INFO: [Train]: [014][00034/00051] (67.3%)  Loss=1.0435  cls_loss=0.6903  reg_loss=0.3533  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=627s  iter_time=2.339s
2025-08-05 22:00:49 Train INFO: [Train]: [014][00035/00051] (69.2%)  Loss=1.0414  cls_loss=0.6891  reg_loss=0.3523  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=575s  iter_time=2.256s
2025-08-05 22:03:07 Train INFO: [Train]: [014][00036/00051] (71.2%)  Loss=1.0407  cls_loss=0.6888  reg_loss=0.3519  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=580s  iter_time=137.594s
2025-08-05 22:03:09 Train INFO: [Train]: [014][00037/00051] (73.1%)  Loss=1.0402  cls_loss=0.6886  reg_loss=0.3517  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=528s  iter_time=2.337s
2025-08-05 22:03:11 Train INFO: [Train]: [014][00038/00051] (75.0%)  Loss=1.0436  cls_loss=0.6897  reg_loss=0.3539  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=479s  iter_time=2.307s
2025-08-05 22:03:14 Train INFO: [Train]: [014][00039/00051] (76.9%)  Loss=1.0415  cls_loss=0.6882  reg_loss=0.3534  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=431s  iter_time=2.296s
2025-08-05 22:05:27 Train INFO: [Train]: [014][00040/00051] (78.8%)  Loss=1.0427  cls_loss=0.6885  reg_loss=0.3542  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=422s  iter_time=133.753s
2025-08-05 22:05:30 Train INFO: [Train]: [014][00041/00051] (80.8%)  Loss=1.0450  cls_loss=0.6899  reg_loss=0.3551  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=375s  iter_time=2.313s
2025-08-05 22:05:32 Train INFO: [Train]: [014][00042/00051] (82.7%)  Loss=1.0481  cls_loss=0.6916  reg_loss=0.3566  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=330s  iter_time=2.289s
2025-08-05 22:05:34 Train INFO: [Train]: [014][00043/00051] (84.6%)  Loss=1.0467  cls_loss=0.6903  reg_loss=0.3563  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=287s  iter_time=2.306s
2025-08-05 22:07:53 Train INFO: [Train]: [014][00044/00051] (86.5%)  Loss=1.0424  cls_loss=0.6875  reg_loss=0.3550  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=267s  iter_time=138.433s
2025-08-05 22:07:55 Train INFO: [Train]: [014][00045/00051] (88.5%)  Loss=1.0366  cls_loss=0.6839  reg_loss=0.3528  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=2.384s
2025-08-05 22:07:57 Train INFO: [Train]: [014][00046/00051] (90.4%)  Loss=1.0361  cls_loss=0.6832  reg_loss=0.3528  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=2.339s
2025-08-05 22:08:00 Train INFO: [Train]: [014][00047/00051] (92.3%)  Loss=1.0348  cls_loss=0.6827  reg_loss=0.3521  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.299s
2025-08-05 22:10:07 Train INFO: [Train]: [014][00048/00051] (94.2%)  Loss=1.0330  cls_loss=0.6817  reg_loss=0.3513  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=127.819s
2025-08-05 22:10:10 Train INFO: [Train]: [014][00049/00051] (96.2%)  Loss=1.0337  cls_loss=0.6824  reg_loss=0.3513  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.164s
2025-08-05 22:10:12 Train INFO: [Train]: [014][00050/00051] (98.1%)  Loss=1.0340  cls_loss=0.6825  reg_loss=0.3515  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.199s
2025-08-05 22:10:14 Train INFO: [Train]: [014][00051/00051] (100.0%)  Loss=1.0315  cls_loss=0.6809  reg_loss=0.3506  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.037s
2025-08-05 22:10:15 Train INFO: [Train]: Epoch 14 completed in 1859.1s (avg 35.752s/iter)
2025-08-05 22:10:15 Train INFO: [Train]: Final Loss=1.0315
2025-08-05 22:10:15 Train INFO: [Val]: Epoch 14 Loss
2025-08-05 22:29:18 Train INFO: [Val]: [014]  Loss=1.4103  cls_loss=1.0400  reg_loss=0.3703  Average-mAP=0.17%
2025-08-05 22:29:19 Train INFO: Checkpoint saved at epoch 14
2025-08-05 22:29:19 Train INFO: [Train]: Epoch 15 started (Total iterations: 52)
2025-08-05 22:31:58 Train INFO: [Train]: [015][00001/00051] (3.8%)  Loss=1.0340  cls_loss=0.6780  reg_loss=0.3561  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3952s  iter_time=158.067s  fwd=2.140s/bwd=0.073s/opt=0.022s
2025-08-05 22:32:00 Train INFO: [Train]: [015][00002/00051] (5.8%)  Loss=1.0866  cls_loss=0.7068  reg_loss=0.3798  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2620s  iter_time=2.327s
2025-08-05 22:32:02 Train INFO: [Train]: [015][00003/00051] (7.7%)  Loss=1.0795  cls_loss=0.7022  reg_loss=0.3773  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1952s  iter_time=2.276s
2025-08-05 22:34:17 Train INFO: [Train]: [015][00004/00051] (9.6%)  Loss=1.0853  cls_loss=0.7058  reg_loss=0.3795  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2800s  iter_time=135.180s
2025-08-05 22:34:20 Train INFO: [Train]: [015][00005/00051] (11.5%)  Loss=1.0988  cls_loss=0.7128  reg_loss=0.3859  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2301s  iter_time=2.316s
2025-08-05 22:34:22 Train INFO: [Train]: [015][00006/00051] (13.5%)  Loss=1.1074  cls_loss=0.7213  reg_loss=0.3860  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1945s  iter_time=2.387s
2025-08-05 22:34:24 Train INFO: [Train]: [015][00007/00051] (15.4%)  Loss=1.1022  cls_loss=0.7180  reg_loss=0.3842  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1676s  iter_time=2.237s
2025-08-05 22:36:33 Train INFO: [Train]: [015][00008/00051] (17.3%)  Loss=1.0742  cls_loss=0.7007  reg_loss=0.3735  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2072s  iter_time=128.784s
2025-08-05 22:36:37 Train INFO: [Train]: [015][00009/00051] (19.2%)  Loss=1.0698  cls_loss=0.7009  reg_loss=0.3689  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1838s  iter_time=3.996s
2025-08-05 22:36:40 Train INFO: [Train]: [015][00010/00051] (21.2%)  Loss=1.0630  cls_loss=0.6955  reg_loss=0.3674  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1644s  iter_time=3.442s
2025-08-05 22:36:43 Train INFO: [Train]: [015][00011/00051] (23.1%)  Loss=1.0576  cls_loss=0.6920  reg_loss=0.3656  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1478s  iter_time=2.273s
2025-08-05 22:38:57 Train INFO: [Train]: [015][00012/00051] (25.0%)  Loss=1.0455  cls_loss=0.6858  reg_loss=0.3597  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1734s  iter_time=134.610s
2025-08-05 22:39:00 Train INFO: [Train]: [015][00013/00051] (26.9%)  Loss=1.0545  cls_loss=0.6923  reg_loss=0.3622  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1575s  iter_time=2.330s
2025-08-05 22:39:05 Train INFO: [Train]: [015][00014/00051] (28.8%)  Loss=1.0694  cls_loss=0.7039  reg_loss=0.3655  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1444s  iter_time=5.194s
2025-08-05 22:39:07 Train INFO: [Train]: [015][00015/00051] (30.8%)  Loss=1.0740  cls_loss=0.7067  reg_loss=0.3673  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1322s  iter_time=2.337s
2025-08-05 22:41:20 Train INFO: [Train]: [015][00016/00051] (32.7%)  Loss=1.0688  cls_loss=0.7026  reg_loss=0.3662  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1483s  iter_time=132.734s
2025-08-05 22:41:22 Train INFO: [Train]: [015][00017/00051] (34.6%)  Loss=1.0609  cls_loss=0.6969  reg_loss=0.3640  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1365s  iter_time=2.331s
2025-08-05 22:41:25 Train INFO: [Train]: [015][00018/00051] (36.5%)  Loss=1.0509  cls_loss=0.6918  reg_loss=0.3591  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1259s  iter_time=2.303s
2025-08-05 22:41:27 Train INFO: [Train]: [015][00019/00051] (38.5%)  Loss=1.0562  cls_loss=0.6957  reg_loss=0.3606  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1164s  iter_time=2.246s
2025-08-05 22:43:41 Train INFO: [Train]: [015][00020/00051] (40.4%)  Loss=1.0485  cls_loss=0.6904  reg_loss=0.3581  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1271s  iter_time=133.935s
2025-08-05 22:43:43 Train INFO: [Train]: [015][00021/00051] (42.3%)  Loss=1.0449  cls_loss=0.6886  reg_loss=0.3563  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1178s  iter_time=2.331s
2025-08-05 22:43:45 Train INFO: [Train]: [015][00022/00051] (44.2%)  Loss=1.0444  cls_loss=0.6882  reg_loss=0.3562  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1092s  iter_time=2.361s
2025-08-05 22:43:48 Train INFO: [Train]: [015][00023/00051] (46.2%)  Loss=1.0382  cls_loss=0.6844  reg_loss=0.3538  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1013s  iter_time=2.262s
2025-08-05 22:46:04 Train INFO: [Train]: [015][00024/00051] (48.1%)  Loss=1.0426  cls_loss=0.6872  reg_loss=0.3554  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1085s  iter_time=136.724s
2025-08-05 22:46:07 Train INFO: [Train]: [015][00025/00051] (50.0%)  Loss=1.0406  cls_loss=0.6844  reg_loss=0.3561  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1007s  iter_time=2.315s
2025-08-05 22:46:09 Train INFO: [Train]: [015][00026/00051] (51.9%)  Loss=1.0465  cls_loss=0.6881  reg_loss=0.3584  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=935s  iter_time=2.265s
2025-08-05 22:46:11 Train INFO: [Train]: [015][00027/00051] (53.8%)  Loss=1.0452  cls_loss=0.6876  reg_loss=0.3576  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=867s  iter_time=2.273s
2025-08-05 22:48:20 Train INFO: [Train]: [015][00028/00051] (55.8%)  Loss=1.0414  cls_loss=0.6852  reg_loss=0.3562  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=904s  iter_time=128.188s
2025-08-05 22:48:22 Train INFO: [Train]: [015][00029/00051] (57.7%)  Loss=1.0450  cls_loss=0.6876  reg_loss=0.3574  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=838s  iter_time=2.317s
2025-08-05 22:48:24 Train INFO: [Train]: [015][00030/00051] (59.6%)  Loss=1.0459  cls_loss=0.6878  reg_loss=0.3581  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=775s  iter_time=2.294s
2025-08-05 22:48:26 Train INFO: [Train]: [015][00031/00051] (61.5%)  Loss=1.0387  cls_loss=0.6829  reg_loss=0.3558  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=717s  iter_time=2.280s
2025-08-05 22:50:39 Train INFO: [Train]: [015][00032/00051] (63.5%)  Loss=1.0395  cls_loss=0.6841  reg_loss=0.3554  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=132.563s
2025-08-05 22:50:41 Train INFO: [Train]: [015][00033/00051] (65.4%)  Loss=1.0399  cls_loss=0.6841  reg_loss=0.3557  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=679s  iter_time=2.283s
2025-08-05 22:50:45 Train INFO: [Train]: [015][00034/00051] (67.3%)  Loss=1.0411  cls_loss=0.6857  reg_loss=0.3554  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=624s  iter_time=3.334s
2025-08-05 22:50:47 Train INFO: [Train]: [015][00035/00051] (69.2%)  Loss=1.0401  cls_loss=0.6847  reg_loss=0.3554  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=572s  iter_time=2.380s
2025-08-05 22:53:00 Train INFO: [Train]: [015][00036/00051] (71.2%)  Loss=1.0404  cls_loss=0.6848  reg_loss=0.3556  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=132.933s
2025-08-05 22:53:02 Train INFO: [Train]: [015][00037/00051] (73.1%)  Loss=1.0430  cls_loss=0.6859  reg_loss=0.3571  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=524s  iter_time=2.309s
2025-08-05 22:53:07 Train INFO: [Train]: [015][00038/00051] (75.0%)  Loss=1.0421  cls_loss=0.6854  reg_loss=0.3567  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=476s  iter_time=4.920s
2025-08-05 22:53:09 Train INFO: [Train]: [015][00039/00051] (76.9%)  Loss=1.0428  cls_loss=0.6859  reg_loss=0.3569  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=429s  iter_time=2.343s
2025-08-05 22:55:22 Train INFO: [Train]: [015][00040/00051] (78.8%)  Loss=1.0428  cls_loss=0.6860  reg_loss=0.3567  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=419s  iter_time=132.362s
2025-08-05 22:55:24 Train INFO: [Train]: [015][00041/00051] (80.8%)  Loss=1.0400  cls_loss=0.6840  reg_loss=0.3560  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=373s  iter_time=2.280s
2025-08-05 22:55:29 Train INFO: [Train]: [015][00042/00051] (82.7%)  Loss=1.0393  cls_loss=0.6841  reg_loss=0.3553  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=329s  iter_time=4.861s
2025-08-05 22:55:31 Train INFO: [Train]: [015][00043/00051] (84.6%)  Loss=1.0359  cls_loss=0.6818  reg_loss=0.3541  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.310s
2025-08-05 22:57:47 Train INFO: [Train]: [015][00044/00051] (86.5%)  Loss=1.0397  cls_loss=0.6843  reg_loss=0.3554  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=135.917s
2025-08-05 22:57:49 Train INFO: [Train]: [015][00045/00051] (88.5%)  Loss=1.0372  cls_loss=0.6827  reg_loss=0.3545  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.248s
2025-08-05 22:57:55 Train INFO: [Train]: [015][00046/00051] (90.4%)  Loss=1.0369  cls_loss=0.6823  reg_loss=0.3546  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=5.146s
2025-08-05 22:57:57 Train INFO: [Train]: [015][00047/00051] (92.3%)  Loss=1.0390  cls_loss=0.6840  reg_loss=0.3551  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.285s
2025-08-05 23:00:02 Train INFO: [Train]: [015][00048/00051] (94.2%)  Loss=1.0367  cls_loss=0.6826  reg_loss=0.3541  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=125.524s
2025-08-05 23:00:05 Train INFO: [Train]: [015][00049/00051] (96.2%)  Loss=1.0354  cls_loss=0.6818  reg_loss=0.3535  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.169s
2025-08-05 23:00:07 Train INFO: [Train]: [015][00050/00051] (98.1%)  Loss=1.0373  cls_loss=0.6832  reg_loss=0.3541  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.658s
2025-08-05 23:00:09 Train INFO: [Train]: [015][00051/00051] (100.0%)  Loss=1.0343  cls_loss=0.6817  reg_loss=0.3526  lr_det=4.8e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.039s
2025-08-05 23:00:10 Train INFO: [Train]: Epoch 15 completed in 1850.5s (avg 35.587s/iter)
2025-08-05 23:00:10 Train INFO: [Train]: Final Loss=1.0343
2025-08-05 23:00:10 Train INFO: [Train]: Epoch 16 started (Total iterations: 52)
2025-08-05 23:02:52 Train INFO: [Train]: [016][00001/00051] (3.8%)  Loss=1.0187  cls_loss=0.6684  reg_loss=0.3503  lr_det=5.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4039s  iter_time=161.578s  fwd=2.110s/bwd=0.087s/opt=0.022s
2025-08-05 23:02:54 Train INFO: [Train]: [016][00002/00051] (5.8%)  Loss=1.0396  cls_loss=0.6808  reg_loss=0.3587  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2677s  iter_time=2.303s
2025-08-05 23:02:56 Train INFO: [Train]: [016][00003/00051] (7.7%)  Loss=1.0758  cls_loss=0.7080  reg_loss=0.3679  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1995s  iter_time=2.335s
2025-08-05 23:05:15 Train INFO: [Train]: [016][00004/00051] (9.6%)  Loss=1.0510  cls_loss=0.6903  reg_loss=0.3607  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2869s  iter_time=139.031s
2025-08-05 23:05:18 Train INFO: [Train]: [016][00005/00051] (11.5%)  Loss=1.0817  cls_loss=0.7111  reg_loss=0.3707  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2358s  iter_time=2.318s
2025-08-05 23:05:20 Train INFO: [Train]: [016][00006/00051] (13.5%)  Loss=1.0818  cls_loss=0.7122  reg_loss=0.3696  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1992s  iter_time=2.343s
2025-08-05 23:05:22 Train INFO: [Train]: [016][00007/00051] (15.4%)  Loss=1.0833  cls_loss=0.7126  reg_loss=0.3707  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1717s  iter_time=2.279s
2025-08-05 23:07:29 Train INFO: [Train]: [016][00008/00051] (17.3%)  Loss=1.0628  cls_loss=0.7018  reg_loss=0.3610  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2099s  iter_time=127.076s
2025-08-05 23:07:38 Train INFO: [Train]: [016][00009/00051] (19.2%)  Loss=1.0578  cls_loss=0.7004  reg_loss=0.3575  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1881s  iter_time=8.554s
2025-08-05 23:07:40 Train INFO: [Train]: [016][00010/00051] (21.2%)  Loss=1.0479  cls_loss=0.6935  reg_loss=0.3544  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1678s  iter_time=2.306s
2025-08-05 23:07:42 Train INFO: [Train]: [016][00011/00051] (23.1%)  Loss=1.0616  cls_loss=0.6995  reg_loss=0.3621  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1508s  iter_time=2.281s
2025-08-05 23:09:49 Train INFO: [Train]: [016][00012/00051] (25.0%)  Loss=1.0477  cls_loss=0.6917  reg_loss=0.3560  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1736s  iter_time=126.334s
2025-08-05 23:09:59 Train INFO: [Train]: [016][00013/00051] (26.9%)  Loss=1.0630  cls_loss=0.7023  reg_loss=0.3607  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1599s  iter_time=10.419s
2025-08-05 23:10:01 Train INFO: [Train]: [016][00014/00051] (28.8%)  Loss=1.0691  cls_loss=0.7072  reg_loss=0.3619  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1459s  iter_time=2.311s
2025-08-05 23:10:04 Train INFO: [Train]: [016][00015/00051] (30.8%)  Loss=1.0586  cls_loss=0.7011  reg_loss=0.3575  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1336s  iter_time=2.336s
2025-08-05 23:12:14 Train INFO: [Train]: [016][00016/00051] (32.7%)  Loss=1.0582  cls_loss=0.7005  reg_loss=0.3576  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1491s  iter_time=130.458s
2025-08-05 23:12:17 Train INFO: [Train]: [016][00017/00051] (34.6%)  Loss=1.0544  cls_loss=0.6983  reg_loss=0.3562  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1372s  iter_time=2.326s
2025-08-05 23:12:19 Train INFO: [Train]: [016][00018/00051] (36.5%)  Loss=1.0449  cls_loss=0.6921  reg_loss=0.3528  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1266s  iter_time=2.328s
2025-08-05 23:12:21 Train INFO: [Train]: [016][00019/00051] (38.5%)  Loss=1.0460  cls_loss=0.6931  reg_loss=0.3529  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1170s  iter_time=2.254s
2025-08-05 23:14:32 Train INFO: [Train]: [016][00020/00051] (40.4%)  Loss=1.0394  cls_loss=0.6885  reg_loss=0.3509  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1272s  iter_time=130.642s
2025-08-05 23:14:35 Train INFO: [Train]: [016][00021/00051] (42.3%)  Loss=1.0510  cls_loss=0.6958  reg_loss=0.3552  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1179s  iter_time=3.077s
2025-08-05 23:14:38 Train INFO: [Train]: [016][00022/00051] (44.2%)  Loss=1.0587  cls_loss=0.7004  reg_loss=0.3583  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1095s  iter_time=3.573s
2025-08-05 23:14:41 Train INFO: [Train]: [016][00023/00051] (46.2%)  Loss=1.0571  cls_loss=0.6991  reg_loss=0.3580  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1016s  iter_time=2.261s
2025-08-05 23:16:53 Train INFO: [Train]: [016][00024/00051] (48.1%)  Loss=1.0639  cls_loss=0.7041  reg_loss=0.3598  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1083s  iter_time=132.317s
2025-08-05 23:16:58 Train INFO: [Train]: [016][00025/00051] (50.0%)  Loss=1.0620  cls_loss=0.7022  reg_loss=0.3598  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1008s  iter_time=4.847s
2025-08-05 23:17:01 Train INFO: [Train]: [016][00026/00051] (51.9%)  Loss=1.0612  cls_loss=0.7011  reg_loss=0.3601  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=936s  iter_time=2.591s
2025-08-05 23:17:03 Train INFO: [Train]: [016][00027/00051] (53.8%)  Loss=1.0620  cls_loss=0.7019  reg_loss=0.3601  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=868s  iter_time=2.312s
2025-08-05 23:19:11 Train INFO: [Train]: [016][00028/00051] (55.8%)  Loss=1.0553  cls_loss=0.6979  reg_loss=0.3574  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=905s  iter_time=127.986s
2025-08-05 23:19:25 Train INFO: [Train]: [016][00029/00051] (57.7%)  Loss=1.0499  cls_loss=0.6949  reg_loss=0.3550  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=847s  iter_time=14.557s
2025-08-05 23:19:28 Train INFO: [Train]: [016][00030/00051] (59.6%)  Loss=1.0534  cls_loss=0.6987  reg_loss=0.3547  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=784s  iter_time=2.345s
2025-08-05 23:19:30 Train INFO: [Train]: [016][00031/00051] (61.5%)  Loss=1.0499  cls_loss=0.6967  reg_loss=0.3532  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=725s  iter_time=2.288s
2025-08-05 23:21:29 Train INFO: [Train]: [016][00032/00051] (63.5%)  Loss=1.0513  cls_loss=0.6980  reg_loss=0.3533  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=119.486s
2025-08-05 23:21:42 Train INFO: [Train]: [016][00033/00051] (65.4%)  Loss=1.0437  cls_loss=0.6932  reg_loss=0.3505  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=684s  iter_time=12.998s
2025-08-05 23:21:45 Train INFO: [Train]: [016][00034/00051] (67.3%)  Loss=1.0444  cls_loss=0.6939  reg_loss=0.3505  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=629s  iter_time=2.246s
2025-08-05 23:21:47 Train INFO: [Train]: [016][00035/00051] (69.2%)  Loss=1.0445  cls_loss=0.6940  reg_loss=0.3505  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=2.355s
2025-08-05 23:23:54 Train INFO: [Train]: [016][00036/00051] (71.2%)  Loss=1.0441  cls_loss=0.6932  reg_loss=0.3509  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=577s  iter_time=127.392s
2025-08-05 23:24:06 Train INFO: [Train]: [016][00037/00051] (73.1%)  Loss=1.0460  cls_loss=0.6938  reg_loss=0.3522  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=529s  iter_time=11.235s
2025-08-05 23:24:08 Train INFO: [Train]: [016][00038/00051] (75.0%)  Loss=1.0449  cls_loss=0.6926  reg_loss=0.3522  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=479s  iter_time=2.282s
2025-08-05 23:24:10 Train INFO: [Train]: [016][00039/00051] (76.9%)  Loss=1.0450  cls_loss=0.6929  reg_loss=0.3521  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=432s  iter_time=2.216s
2025-08-05 23:26:14 Train INFO: [Train]: [016][00040/00051] (78.8%)  Loss=1.0458  cls_loss=0.6934  reg_loss=0.3523  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=123.613s
2025-08-05 23:26:25 Train INFO: [Train]: [016][00041/00051] (80.8%)  Loss=1.0424  cls_loss=0.6909  reg_loss=0.3515  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=375s  iter_time=11.044s
2025-08-05 23:26:27 Train INFO: [Train]: [016][00042/00051] (82.7%)  Loss=1.0429  cls_loss=0.6913  reg_loss=0.3516  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=330s  iter_time=2.281s
2025-08-05 23:26:29 Train INFO: [Train]: [016][00043/00051] (84.6%)  Loss=1.0437  cls_loss=0.6913  reg_loss=0.3523  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=287s  iter_time=2.316s
2025-08-05 23:28:39 Train INFO: [Train]: [016][00044/00051] (86.5%)  Loss=1.0436  cls_loss=0.6910  reg_loss=0.3526  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=129.519s
2025-08-05 23:28:41 Train INFO: [Train]: [016][00045/00051] (88.5%)  Loss=1.0418  cls_loss=0.6900  reg_loss=0.3519  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.407s
2025-08-05 23:28:47 Train INFO: [Train]: [016][00046/00051] (90.4%)  Loss=1.0416  cls_loss=0.6893  reg_loss=0.3523  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=5.820s
2025-08-05 23:28:50 Train INFO: [Train]: [016][00047/00051] (92.3%)  Loss=1.0388  cls_loss=0.6876  reg_loss=0.3512  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.317s
2025-08-05 23:30:54 Train INFO: [Train]: [016][00048/00051] (94.2%)  Loss=1.0398  cls_loss=0.6885  reg_loss=0.3513  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=124.683s
2025-08-05 23:30:57 Train INFO: [Train]: [016][00049/00051] (96.2%)  Loss=1.0413  cls_loss=0.6896  reg_loss=0.3517  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.441s
2025-08-05 23:31:00 Train INFO: [Train]: [016][00050/00051] (98.1%)  Loss=1.0443  cls_loss=0.6914  reg_loss=0.3528  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=3.178s
2025-08-05 23:31:02 Train INFO: [Train]: [016][00051/00051] (100.0%)  Loss=1.0406  cls_loss=0.6891  reg_loss=0.3515  lr_det=1.0e-04  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.038s
2025-08-05 23:31:03 Train INFO: [Train]: Epoch 16 completed in 1852.6s (avg 35.627s/iter)
2025-08-05 23:31:03 Train INFO: [Train]: Final Loss=1.0406
2025-08-05 23:31:03 Train INFO: [Train]: Epoch 17 started (Total iterations: 52)
2025-08-05 23:33:49 Train INFO: [Train]: [017][00001/00051] (3.8%)  Loss=0.9437  cls_loss=0.6412  reg_loss=0.3024  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4156s  iter_time=166.221s  fwd=2.141s/bwd=0.080s/opt=0.026s
2025-08-05 23:33:51 Train INFO: [Train]: [017][00002/00051] (5.8%)  Loss=1.0319  cls_loss=0.6890  reg_loss=0.3429  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2753s  iter_time=2.327s
2025-08-05 23:33:53 Train INFO: [Train]: [017][00003/00051] (7.7%)  Loss=1.0433  cls_loss=0.6930  reg_loss=0.3504  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2049s  iter_time=2.232s
2025-08-05 23:36:08 Train INFO: [Train]: [017][00004/00051] (9.6%)  Loss=1.0283  cls_loss=0.6829  reg_loss=0.3454  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2875s  iter_time=135.033s
2025-08-05 23:36:14 Train INFO: [Train]: [017][00005/00051] (11.5%)  Loss=1.0558  cls_loss=0.6968  reg_loss=0.3589  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2387s  iter_time=5.489s
2025-08-05 23:36:16 Train INFO: [Train]: [017][00006/00051] (13.5%)  Loss=1.0759  cls_loss=0.7081  reg_loss=0.3678  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2016s  iter_time=2.338s
2025-08-05 23:36:19 Train INFO: [Train]: [017][00007/00051] (15.4%)  Loss=1.0698  cls_loss=0.7046  reg_loss=0.3652  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1738s  iter_time=2.271s
2025-08-05 23:38:27 Train INFO: [Train]: [017][00008/00051] (17.3%)  Loss=1.0539  cls_loss=0.6950  reg_loss=0.3589  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2122s  iter_time=128.286s
2025-08-05 23:38:37 Train INFO: [Train]: [017][00009/00051] (19.2%)  Loss=1.0529  cls_loss=0.6955  reg_loss=0.3574  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1908s  iter_time=10.012s
2025-08-05 23:38:39 Train INFO: [Train]: [017][00010/00051] (21.2%)  Loss=1.0432  cls_loss=0.6903  reg_loss=0.3529  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1702s  iter_time=2.355s
2025-08-05 23:38:41 Train INFO: [Train]: [017][00011/00051] (23.1%)  Loss=1.0535  cls_loss=0.6980  reg_loss=0.3555  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1529s  iter_time=2.260s
2025-08-05 23:40:53 Train INFO: [Train]: [017][00012/00051] (25.0%)  Loss=1.0541  cls_loss=0.6982  reg_loss=0.3559  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1770s  iter_time=131.302s
2025-08-05 23:41:02 Train INFO: [Train]: [017][00013/00051] (26.9%)  Loss=1.0444  cls_loss=0.6918  reg_loss=0.3526  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1626s  iter_time=8.815s
2025-08-05 23:41:04 Train INFO: [Train]: [017][00014/00051] (28.8%)  Loss=1.0499  cls_loss=0.6955  reg_loss=0.3544  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1483s  iter_time=2.329s
2025-08-05 23:41:06 Train INFO: [Train]: [017][00015/00051] (30.8%)  Loss=1.0475  cls_loss=0.6942  reg_loss=0.3532  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1358s  iter_time=2.308s
2025-08-05 23:43:14 Train INFO: [Train]: [017][00016/00051] (32.7%)  Loss=1.0450  cls_loss=0.6922  reg_loss=0.3528  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1506s  iter_time=127.874s
2025-08-05 23:43:17 Train INFO: [Train]: [017][00017/00051] (34.6%)  Loss=1.0357  cls_loss=0.6858  reg_loss=0.3499  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1387s  iter_time=3.000s
2025-08-05 23:43:19 Train INFO: [Train]: [017][00018/00051] (36.5%)  Loss=1.0280  cls_loss=0.6827  reg_loss=0.3452  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1280s  iter_time=2.299s
2025-08-05 23:43:22 Train INFO: [Train]: [017][00019/00051] (38.5%)  Loss=1.0368  cls_loss=0.6869  reg_loss=0.3499  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1182s  iter_time=2.233s
2025-08-05 23:45:31 Train INFO: [Train]: [017][00020/00051] (40.4%)  Loss=1.0256  cls_loss=0.6802  reg_loss=0.3454  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1283s  iter_time=129.796s
2025-08-05 23:45:35 Train INFO: [Train]: [017][00021/00051] (42.3%)  Loss=1.0261  cls_loss=0.6804  reg_loss=0.3457  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1189s  iter_time=3.446s
2025-08-05 23:45:37 Train INFO: [Train]: [017][00022/00051] (44.2%)  Loss=1.0337  cls_loss=0.6847  reg_loss=0.3490  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1103s  iter_time=2.265s
2025-08-05 23:45:39 Train INFO: [Train]: [017][00023/00051] (46.2%)  Loss=1.0331  cls_loss=0.6840  reg_loss=0.3491  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1023s  iter_time=2.305s
2025-08-05 23:47:53 Train INFO: [Train]: [017][00024/00051] (48.1%)  Loss=1.0411  cls_loss=0.6886  reg_loss=0.3525  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1091s  iter_time=133.332s
2025-08-05 23:47:58 Train INFO: [Train]: [017][00025/00051] (50.0%)  Loss=1.0427  cls_loss=0.6886  reg_loss=0.3541  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1015s  iter_time=4.968s
2025-08-05 23:48:00 Train INFO: [Train]: [017][00026/00051] (51.9%)  Loss=1.0489  cls_loss=0.6919  reg_loss=0.3570  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=942s  iter_time=2.335s
2025-08-05 23:48:02 Train INFO: [Train]: [017][00027/00051] (53.8%)  Loss=1.0503  cls_loss=0.6933  reg_loss=0.3570  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=874s  iter_time=2.286s
2025-08-05 23:50:10 Train INFO: [Train]: [017][00028/00051] (55.8%)  Loss=1.0503  cls_loss=0.6934  reg_loss=0.3569  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=910s  iter_time=127.438s
2025-08-05 23:50:21 Train INFO: [Train]: [017][00029/00051] (57.7%)  Loss=1.0465  cls_loss=0.6913  reg_loss=0.3552  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=850s  iter_time=11.374s
2025-08-05 23:50:23 Train INFO: [Train]: [017][00030/00051] (59.6%)  Loss=1.0421  cls_loss=0.6892  reg_loss=0.3529  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=786s  iter_time=2.270s
2025-08-05 23:50:26 Train INFO: [Train]: [017][00031/00051] (61.5%)  Loss=1.0415  cls_loss=0.6897  reg_loss=0.3518  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=727s  iter_time=2.269s
2025-08-05 23:52:33 Train INFO: [Train]: [017][00032/00051] (63.5%)  Loss=1.0441  cls_loss=0.6920  reg_loss=0.3520  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=743s  iter_time=126.820s
2025-08-05 23:52:44 Train INFO: [Train]: [017][00033/00051] (65.4%)  Loss=1.0428  cls_loss=0.6909  reg_loss=0.3519  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=689s  iter_time=11.015s
2025-08-05 23:52:46 Train INFO: [Train]: [017][00034/00051] (67.3%)  Loss=1.0508  cls_loss=0.6963  reg_loss=0.3545  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=633s  iter_time=2.338s
2025-08-05 23:52:48 Train INFO: [Train]: [017][00035/00051] (69.2%)  Loss=1.0470  cls_loss=0.6944  reg_loss=0.3525  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=580s  iter_time=2.207s
2025-08-05 23:54:57 Train INFO: [Train]: [017][00036/00051] (71.2%)  Loss=1.0469  cls_loss=0.6942  reg_loss=0.3527  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=581s  iter_time=128.601s
2025-08-05 23:55:07 Train INFO: [Train]: [017][00037/00051] (73.1%)  Loss=1.0429  cls_loss=0.6910  reg_loss=0.3519  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=532s  iter_time=10.613s
2025-08-05 23:55:10 Train INFO: [Train]: [017][00038/00051] (75.0%)  Loss=1.0406  cls_loss=0.6895  reg_loss=0.3512  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=482s  iter_time=2.355s
2025-08-05 23:55:12 Train INFO: [Train]: [017][00039/00051] (76.9%)  Loss=1.0408  cls_loss=0.6888  reg_loss=0.3519  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=435s  iter_time=2.254s
2025-08-05 23:57:15 Train INFO: [Train]: [017][00040/00051] (78.8%)  Loss=1.0387  cls_loss=0.6875  reg_loss=0.3512  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=422s  iter_time=122.915s
2025-08-05 23:57:25 Train INFO: [Train]: [017][00041/00051] (80.8%)  Loss=1.0387  cls_loss=0.6873  reg_loss=0.3514  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=377s  iter_time=10.211s
2025-08-05 23:57:27 Train INFO: [Train]: [017][00042/00051] (82.7%)  Loss=1.0435  cls_loss=0.6904  reg_loss=0.3531  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=332s  iter_time=2.275s
2025-08-05 23:57:30 Train INFO: [Train]: [017][00043/00051] (84.6%)  Loss=1.0438  cls_loss=0.6904  reg_loss=0.3534  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=289s  iter_time=2.231s
2025-08-05 23:59:40 Train INFO: [Train]: [017][00044/00051] (86.5%)  Loss=1.0402  cls_loss=0.6882  reg_loss=0.3520  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=267s  iter_time=130.022s
2025-08-05 23:59:45 Train INFO: [Train]: [017][00045/00051] (88.5%)  Loss=1.0361  cls_loss=0.6856  reg_loss=0.3505  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=225s  iter_time=5.427s
2025-08-05 23:59:48 Train INFO: [Train]: [017][00046/00051] (90.4%)  Loss=1.0353  cls_loss=0.6847  reg_loss=0.3506  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=184s  iter_time=2.590s
2025-08-05 23:59:50 Train INFO: [Train]: [017][00047/00051] (92.3%)  Loss=1.0339  cls_loss=0.6837  reg_loss=0.3502  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.364s
2025-08-06 00:01:55 Train INFO: [Train]: [017][00048/00051] (94.2%)  Loss=1.0353  cls_loss=0.6845  reg_loss=0.3508  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=125.496s
2025-08-06 00:02:00 Train INFO: [Train]: [017][00049/00051] (96.2%)  Loss=1.0375  cls_loss=0.6858  reg_loss=0.3517  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=4.047s
2025-08-06 00:02:02 Train INFO: [Train]: [017][00050/00051] (98.1%)  Loss=1.0386  cls_loss=0.6860  reg_loss=0.3527  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.159s
2025-08-06 00:02:04 Train INFO: [Train]: [017][00051/00051] (100.0%)  Loss=1.0370  cls_loss=0.6846  reg_loss=0.3523  lr_det=5.2e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.036s
2025-08-06 00:02:04 Train INFO: [Train]: Epoch 17 completed in 1861.8s (avg 35.804s/iter)
2025-08-06 00:02:04 Train INFO: [Train]: Final Loss=1.0370
2025-08-06 00:02:04 Train INFO: [Train]: Epoch 18 started (Total iterations: 52)
2025-08-06 00:04:45 Train INFO: [Train]: [018][00001/00051] (3.8%)  Loss=0.9807  cls_loss=0.6311  reg_loss=0.3497  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4021s  iter_time=160.859s  fwd=2.126s/bwd=0.069s/opt=0.024s
2025-08-06 00:04:48 Train INFO: [Train]: [018][00002/00051] (5.8%)  Loss=1.0290  cls_loss=0.6684  reg_loss=0.3607  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2666s  iter_time=2.360s
2025-08-06 00:04:50 Train INFO: [Train]: [018][00003/00051] (7.7%)  Loss=1.0476  cls_loss=0.6811  reg_loss=0.3665  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1987s  iter_time=2.327s
2025-08-06 00:07:05 Train INFO: [Train]: [018][00004/00051] (9.6%)  Loss=1.0057  cls_loss=0.6569  reg_loss=0.3488  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2824s  iter_time=134.914s
2025-08-06 00:07:07 Train INFO: [Train]: [018][00005/00051] (11.5%)  Loss=1.0455  cls_loss=0.6795  reg_loss=0.3660  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2321s  iter_time=2.328s
2025-08-06 00:07:10 Train INFO: [Train]: [018][00006/00051] (13.5%)  Loss=1.0541  cls_loss=0.6863  reg_loss=0.3678  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1961s  iter_time=2.306s
2025-08-06 00:07:12 Train INFO: [Train]: [018][00007/00051] (15.4%)  Loss=1.0582  cls_loss=0.6895  reg_loss=0.3688  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1690s  iter_time=2.238s
2025-08-06 00:09:18 Train INFO: [Train]: [018][00008/00051] (17.3%)  Loss=1.0505  cls_loss=0.6856  reg_loss=0.3649  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2074s  iter_time=126.652s
2025-08-06 00:09:28 Train INFO: [Train]: [018][00009/00051] (19.2%)  Loss=1.0482  cls_loss=0.6851  reg_loss=0.3631  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1862s  iter_time=9.409s
2025-08-06 00:09:30 Train INFO: [Train]: [018][00010/00051] (21.2%)  Loss=1.0627  cls_loss=0.6947  reg_loss=0.3680  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1661s  iter_time=2.330s
2025-08-06 00:09:32 Train INFO: [Train]: [018][00011/00051] (23.1%)  Loss=1.0650  cls_loss=0.6962  reg_loss=0.3687  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1493s  iter_time=2.290s
2025-08-06 00:11:39 Train INFO: [Train]: [018][00012/00051] (25.0%)  Loss=1.0544  cls_loss=0.6909  reg_loss=0.3635  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1724s  iter_time=126.734s
2025-08-06 00:11:50 Train INFO: [Train]: [018][00013/00051] (26.9%)  Loss=1.0577  cls_loss=0.6948  reg_loss=0.3629  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1589s  iter_time=10.591s
2025-08-06 00:11:52 Train INFO: [Train]: [018][00014/00051] (28.8%)  Loss=1.0615  cls_loss=0.6990  reg_loss=0.3625  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1450s  iter_time=2.359s
2025-08-06 00:11:54 Train INFO: [Train]: [018][00015/00051] (30.8%)  Loss=1.0583  cls_loss=0.6964  reg_loss=0.3619  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1328s  iter_time=2.288s
2025-08-06 00:14:01 Train INFO: [Train]: [018][00016/00051] (32.7%)  Loss=1.0502  cls_loss=0.6926  reg_loss=0.3576  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1476s  iter_time=126.922s
2025-08-06 00:14:06 Train INFO: [Train]: [018][00017/00051] (34.6%)  Loss=1.0473  cls_loss=0.6905  reg_loss=0.3568  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1362s  iter_time=4.198s
2025-08-06 00:14:08 Train INFO: [Train]: [018][00018/00051] (36.5%)  Loss=1.0448  cls_loss=0.6888  reg_loss=0.3560  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1257s  iter_time=2.345s
2025-08-06 00:14:10 Train INFO: [Train]: [018][00019/00051] (38.5%)  Loss=1.0464  cls_loss=0.6899  reg_loss=0.3565  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1161s  iter_time=2.313s
2025-08-06 00:16:19 Train INFO: [Train]: [018][00020/00051] (40.4%)  Loss=1.0304  cls_loss=0.6790  reg_loss=0.3513  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1261s  iter_time=128.614s
2025-08-06 00:16:22 Train INFO: [Train]: [018][00021/00051] (42.3%)  Loss=1.0336  cls_loss=0.6817  reg_loss=0.3519  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1169s  iter_time=2.785s
2025-08-06 00:16:30 Train INFO: [Train]: [018][00022/00051] (44.2%)  Loss=1.0397  cls_loss=0.6848  reg_loss=0.3549  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1091s  iter_time=8.017s
2025-08-06 00:16:32 Train INFO: [Train]: [018][00023/00051] (46.2%)  Loss=1.0388  cls_loss=0.6833  reg_loss=0.3555  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1012s  iter_time=2.353s
2025-08-06 00:18:43 Train INFO: [Train]: [018][00024/00051] (48.1%)  Loss=1.0444  cls_loss=0.6871  reg_loss=0.3573  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1079s  iter_time=131.204s
2025-08-06 00:18:46 Train INFO: [Train]: [018][00025/00051] (50.0%)  Loss=1.0456  cls_loss=0.6875  reg_loss=0.3581  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1001s  iter_time=2.695s
2025-08-06 00:18:52 Train INFO: [Train]: [018][00026/00051] (51.9%)  Loss=1.0481  cls_loss=0.6890  reg_loss=0.3591  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=933s  iter_time=5.765s
2025-08-06 00:18:54 Train INFO: [Train]: [018][00027/00051] (53.8%)  Loss=1.0517  cls_loss=0.6896  reg_loss=0.3620  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=865s  iter_time=2.397s
2025-08-06 00:21:00 Train INFO: [Train]: [018][00028/00051] (55.8%)  Loss=1.0487  cls_loss=0.6875  reg_loss=0.3613  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=901s  iter_time=126.246s
2025-08-06 00:21:08 Train INFO: [Train]: [018][00029/00051] (57.7%)  Loss=1.0461  cls_loss=0.6861  reg_loss=0.3600  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=838s  iter_time=7.230s
2025-08-06 00:21:11 Train INFO: [Train]: [018][00030/00051] (59.6%)  Loss=1.0421  cls_loss=0.6839  reg_loss=0.3582  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=777s  iter_time=3.359s
2025-08-06 00:21:14 Train INFO: [Train]: [018][00031/00051] (61.5%)  Loss=1.0401  cls_loss=0.6829  reg_loss=0.3571  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=719s  iter_time=3.166s
2025-08-06 00:23:20 Train INFO: [Train]: [018][00032/00051] (63.5%)  Loss=1.0421  cls_loss=0.6842  reg_loss=0.3580  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=734s  iter_time=125.675s
2025-08-06 00:23:22 Train INFO: [Train]: [018][00033/00051] (65.4%)  Loss=1.0374  cls_loss=0.6810  reg_loss=0.3564  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=676s  iter_time=2.365s
2025-08-06 00:23:30 Train INFO: [Train]: [018][00034/00051] (67.3%)  Loss=1.0409  cls_loss=0.6832  reg_loss=0.3578  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=624s  iter_time=8.032s
2025-08-06 00:23:35 Train INFO: [Train]: [018][00035/00051] (69.2%)  Loss=1.0349  cls_loss=0.6796  reg_loss=0.3553  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=5.217s
2025-08-06 00:25:41 Train INFO: [Train]: [018][00036/00051] (71.2%)  Loss=1.0306  cls_loss=0.6768  reg_loss=0.3538  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=126.135s
2025-08-06 00:25:44 Train INFO: [Train]: [018][00037/00051] (73.1%)  Loss=1.0317  cls_loss=0.6770  reg_loss=0.3547  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=523s  iter_time=2.349s
2025-08-06 00:25:50 Train INFO: [Train]: [018][00038/00051] (75.0%)  Loss=1.0353  cls_loss=0.6787  reg_loss=0.3566  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=475s  iter_time=5.749s
2025-08-06 00:25:58 Train INFO: [Train]: [018][00039/00051] (76.9%)  Loss=1.0363  cls_loss=0.6793  reg_loss=0.3570  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=8.479s
2025-08-06 00:28:04 Train INFO: [Train]: [018][00040/00051] (78.8%)  Loss=1.0350  cls_loss=0.6785  reg_loss=0.3565  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=418s  iter_time=125.515s
2025-08-06 00:28:06 Train INFO: [Train]: [018][00041/00051] (80.8%)  Loss=1.0317  cls_loss=0.6767  reg_loss=0.3549  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=372s  iter_time=2.330s
2025-08-06 00:28:11 Train INFO: [Train]: [018][00042/00051] (82.7%)  Loss=1.0328  cls_loss=0.6776  reg_loss=0.3552  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=4.987s
2025-08-06 00:28:22 Train INFO: [Train]: [018][00043/00051] (84.6%)  Loss=1.0281  cls_loss=0.6750  reg_loss=0.3532  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=287s  iter_time=10.944s
2025-08-06 00:30:28 Train INFO: [Train]: [018][00044/00051] (86.5%)  Loss=1.0270  cls_loss=0.6738  reg_loss=0.3532  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=265s  iter_time=126.341s
2025-08-06 00:30:30 Train INFO: [Train]: [018][00045/00051] (88.5%)  Loss=1.0269  cls_loss=0.6741  reg_loss=0.3528  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.239s
2025-08-06 00:30:36 Train INFO: [Train]: [018][00046/00051] (90.4%)  Loss=1.0255  cls_loss=0.6733  reg_loss=0.3523  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=5.493s
2025-08-06 00:30:38 Train INFO: [Train]: [018][00047/00051] (92.3%)  Loss=1.0235  cls_loss=0.6726  reg_loss=0.3509  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.291s
2025-08-06 00:32:48 Train INFO: [Train]: [018][00048/00051] (94.2%)  Loss=1.0227  cls_loss=0.6727  reg_loss=0.3500  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=129.779s
2025-08-06 00:32:50 Train INFO: [Train]: [018][00049/00051] (96.2%)  Loss=1.0238  cls_loss=0.6740  reg_loss=0.3498  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.166s
2025-08-06 00:32:52 Train INFO: [Train]: [018][00050/00051] (98.1%)  Loss=1.0252  cls_loss=0.6749  reg_loss=0.3503  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.341s
2025-08-06 00:32:55 Train INFO: [Train]: [018][00051/00051] (100.0%)  Loss=1.0259  cls_loss=0.6751  reg_loss=0.3508  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.040s
2025-08-06 00:32:55 Train INFO: [Train]: Epoch 18 completed in 1850.8s (avg 35.593s/iter)
2025-08-06 00:32:55 Train INFO: [Train]: Final Loss=1.0259
2025-08-06 00:32:55 Train INFO: [Train]: Epoch 19 started (Total iterations: 52)
2025-08-06 00:35:35 Train INFO: [Train]: [019][00001/00051] (3.8%)  Loss=0.9666  cls_loss=0.6296  reg_loss=0.3370  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3992s  iter_time=159.662s  fwd=2.118s/bwd=0.075s/opt=0.020s
2025-08-06 00:35:37 Train INFO: [Train]: [019][00002/00051] (5.8%)  Loss=0.9643  cls_loss=0.6360  reg_loss=0.3283  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2645s  iter_time=2.274s
2025-08-06 00:35:39 Train INFO: [Train]: [019][00003/00051] (7.7%)  Loss=1.0197  cls_loss=0.6667  reg_loss=0.3530  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1971s  iter_time=2.275s
2025-08-06 00:37:55 Train INFO: [Train]: [019][00004/00051] (9.6%)  Loss=1.0320  cls_loss=0.6787  reg_loss=0.3533  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2816s  iter_time=135.364s
2025-08-06 00:38:01 Train INFO: [Train]: [019][00005/00051] (11.5%)  Loss=1.0355  cls_loss=0.6816  reg_loss=0.3539  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2341s  iter_time=5.825s
2025-08-06 00:38:03 Train INFO: [Train]: [019][00006/00051] (13.5%)  Loss=1.0665  cls_loss=0.7005  reg_loss=0.3660  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1978s  iter_time=2.308s
2025-08-06 00:38:05 Train INFO: [Train]: [019][00007/00051] (15.4%)  Loss=1.0703  cls_loss=0.7040  reg_loss=0.3663  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1705s  iter_time=2.263s
2025-08-06 00:40:10 Train INFO: [Train]: [019][00008/00051] (17.3%)  Loss=1.0424  cls_loss=0.6866  reg_loss=0.3559  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2079s  iter_time=125.059s
2025-08-06 00:40:23 Train INFO: [Train]: [019][00009/00051] (19.2%)  Loss=1.0443  cls_loss=0.6906  reg_loss=0.3536  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1880s  iter_time=12.697s
2025-08-06 00:40:25 Train INFO: [Train]: [019][00010/00051] (21.2%)  Loss=1.0391  cls_loss=0.6885  reg_loss=0.3506  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1677s  iter_time=2.323s
2025-08-06 00:40:28 Train INFO: [Train]: [019][00011/00051] (23.1%)  Loss=1.0710  cls_loss=0.7084  reg_loss=0.3626  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1508s  iter_time=2.276s
2025-08-06 00:42:31 Train INFO: [Train]: [019][00012/00051] (25.0%)  Loss=1.0684  cls_loss=0.7070  reg_loss=0.3614  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1726s  iter_time=122.930s
2025-08-06 00:42:49 Train INFO: [Train]: [019][00013/00051] (26.9%)  Loss=1.0687  cls_loss=0.7074  reg_loss=0.3613  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1610s  iter_time=17.999s
2025-08-06 00:42:51 Train INFO: [Train]: [019][00014/00051] (28.8%)  Loss=1.0602  cls_loss=0.7014  reg_loss=0.3588  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1469s  iter_time=2.330s
2025-08-06 00:42:53 Train INFO: [Train]: [019][00015/00051] (30.8%)  Loss=1.0520  cls_loss=0.6964  reg_loss=0.3556  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1345s  iter_time=2.309s
2025-08-06 00:44:51 Train INFO: [Train]: [019][00016/00051] (32.7%)  Loss=1.0446  cls_loss=0.6905  reg_loss=0.3540  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1474s  iter_time=118.060s
2025-08-06 00:45:05 Train INFO: [Train]: [019][00017/00051] (34.6%)  Loss=1.0397  cls_loss=0.6870  reg_loss=0.3527  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1378s  iter_time=13.586s
2025-08-06 00:45:07 Train INFO: [Train]: [019][00018/00051] (36.5%)  Loss=1.0347  cls_loss=0.6841  reg_loss=0.3506  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1271s  iter_time=2.279s
2025-08-06 00:45:09 Train INFO: [Train]: [019][00019/00051] (38.5%)  Loss=1.0455  cls_loss=0.6920  reg_loss=0.3535  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1175s  iter_time=2.298s
2025-08-06 00:47:06 Train INFO: [Train]: [019][00020/00051] (40.4%)  Loss=1.0354  cls_loss=0.6844  reg_loss=0.3510  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1256s  iter_time=116.783s
2025-08-06 00:47:22 Train INFO: [Train]: [019][00021/00051] (42.3%)  Loss=1.0377  cls_loss=0.6863  reg_loss=0.3514  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1182s  iter_time=15.631s
2025-08-06 00:47:24 Train INFO: [Train]: [019][00022/00051] (44.2%)  Loss=1.0415  cls_loss=0.6880  reg_loss=0.3535  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1096s  iter_time=2.295s
2025-08-06 00:47:26 Train INFO: [Train]: [019][00023/00051] (46.2%)  Loss=1.0323  cls_loss=0.6820  reg_loss=0.3503  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1016s  iter_time=2.283s
2025-08-06 00:49:29 Train INFO: [Train]: [019][00024/00051] (48.1%)  Loss=1.0367  cls_loss=0.6848  reg_loss=0.3519  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1073s  iter_time=122.154s
2025-08-06 00:49:49 Train INFO: [Train]: [019][00025/00051] (50.0%)  Loss=1.0320  cls_loss=0.6818  reg_loss=0.3502  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1014s  iter_time=20.641s
2025-08-06 00:49:51 Train INFO: [Train]: [019][00026/00051] (51.9%)  Loss=1.0370  cls_loss=0.6854  reg_loss=0.3516  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=941s  iter_time=2.304s
2025-08-06 00:49:54 Train INFO: [Train]: [019][00027/00051] (53.8%)  Loss=1.0350  cls_loss=0.6841  reg_loss=0.3508  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=873s  iter_time=2.312s
2025-08-06 00:51:44 Train INFO: [Train]: [019][00028/00051] (55.8%)  Loss=1.0350  cls_loss=0.6835  reg_loss=0.3515  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=895s  iter_time=110.269s
2025-08-06 00:52:12 Train INFO: [Train]: [019][00029/00051] (57.7%)  Loss=1.0341  cls_loss=0.6828  reg_loss=0.3513  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=848s  iter_time=27.608s
2025-08-06 00:52:14 Train INFO: [Train]: [019][00030/00051] (59.6%)  Loss=1.0321  cls_loss=0.6821  reg_loss=0.3501  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=785s  iter_time=2.289s
2025-08-06 00:52:16 Train INFO: [Train]: [019][00031/00051] (61.5%)  Loss=1.0276  cls_loss=0.6786  reg_loss=0.3490  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=726s  iter_time=2.367s
2025-08-06 00:54:06 Train INFO: [Train]: [019][00032/00051] (63.5%)  Loss=1.0254  cls_loss=0.6776  reg_loss=0.3479  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=732s  iter_time=109.923s
2025-08-06 00:54:30 Train INFO: [Train]: [019][00033/00051] (65.4%)  Loss=1.0228  cls_loss=0.6762  reg_loss=0.3466  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=685s  iter_time=23.774s
2025-08-06 00:54:32 Train INFO: [Train]: [019][00034/00051] (67.3%)  Loss=1.0229  cls_loss=0.6763  reg_loss=0.3466  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=630s  iter_time=2.299s
2025-08-06 00:54:35 Train INFO: [Train]: [019][00035/00051] (69.2%)  Loss=1.0253  cls_loss=0.6777  reg_loss=0.3477  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=578s  iter_time=2.301s
2025-08-06 00:56:31 Train INFO: [Train]: [019][00036/00051] (71.2%)  Loss=1.0268  cls_loss=0.6788  reg_loss=0.3480  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=116.609s
2025-08-06 00:56:52 Train INFO: [Train]: [019][00037/00051] (73.1%)  Loss=1.0257  cls_loss=0.6779  reg_loss=0.3479  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=529s  iter_time=20.557s
2025-08-06 00:56:54 Train INFO: [Train]: [019][00038/00051] (75.0%)  Loss=1.0277  cls_loss=0.6787  reg_loss=0.3490  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=480s  iter_time=2.345s
2025-08-06 00:56:56 Train INFO: [Train]: [019][00039/00051] (76.9%)  Loss=1.0260  cls_loss=0.6773  reg_loss=0.3487  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=432s  iter_time=2.226s
2025-08-06 00:58:49 Train INFO: [Train]: [019][00040/00051] (78.8%)  Loss=1.0245  cls_loss=0.6759  reg_loss=0.3486  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=417s  iter_time=112.995s
2025-08-06 00:59:13 Train INFO: [Train]: [019][00041/00051] (80.8%)  Loss=1.0229  cls_loss=0.6745  reg_loss=0.3484  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=376s  iter_time=23.812s
2025-08-06 00:59:15 Train INFO: [Train]: [019][00042/00051] (82.7%)  Loss=1.0259  cls_loss=0.6762  reg_loss=0.3497  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=331s  iter_time=2.269s
2025-08-06 00:59:18 Train INFO: [Train]: [019][00043/00051] (84.6%)  Loss=1.0253  cls_loss=0.6755  reg_loss=0.3499  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=288s  iter_time=2.323s
2025-08-06 01:01:10 Train INFO: [Train]: [019][00044/00051] (86.5%)  Loss=1.0228  cls_loss=0.6740  reg_loss=0.3488  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=264s  iter_time=112.656s
2025-08-06 01:01:30 Train INFO: [Train]: [019][00045/00051] (88.5%)  Loss=1.0195  cls_loss=0.6720  reg_loss=0.3475  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=19.522s
2025-08-06 01:01:32 Train INFO: [Train]: [019][00046/00051] (90.4%)  Loss=1.0186  cls_loss=0.6713  reg_loss=0.3474  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=2.337s
2025-08-06 01:01:35 Train INFO: [Train]: [019][00047/00051] (92.3%)  Loss=1.0203  cls_loss=0.6724  reg_loss=0.3479  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.274s
2025-08-06 01:03:35 Train INFO: [Train]: [019][00048/00051] (94.2%)  Loss=1.0200  cls_loss=0.6726  reg_loss=0.3474  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=120.417s
2025-08-06 01:03:47 Train INFO: [Train]: [019][00049/00051] (96.2%)  Loss=1.0219  cls_loss=0.6738  reg_loss=0.3481  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=11.737s
2025-08-06 01:03:49 Train INFO: [Train]: [019][00050/00051] (98.1%)  Loss=1.0204  cls_loss=0.6729  reg_loss=0.3475  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.154s
2025-08-06 01:03:51 Train INFO: [Train]: [019][00051/00051] (100.0%)  Loss=1.0180  cls_loss=0.6716  reg_loss=0.3464  lr_det=4.8e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.033s
2025-08-06 01:03:52 Train INFO: [Train]: Epoch 19 completed in 1856.4s (avg 35.700s/iter)
2025-08-06 01:03:52 Train INFO: [Train]: Final Loss=1.0180
2025-08-06 01:03:52 Train INFO: [Val]: Epoch 19 Loss
2025-08-06 01:23:03 Train INFO: [Val]: [019]  Loss=1.4334  cls_loss=1.0594  reg_loss=0.3739  Average-mAP=0.18%
2025-08-06 01:23:04 Train INFO: Checkpoint saved at epoch 19
2025-08-06 01:23:04 Train INFO: [Train]: Epoch 20 started (Total iterations: 52)
2025-08-06 01:25:41 Train INFO: [Train]: [020][00001/00051] (3.8%)  Loss=0.9468  cls_loss=0.6365  reg_loss=0.3102  lr_det=5.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3917s  iter_time=156.669s  fwd=2.253s/bwd=0.048s/opt=0.020s
2025-08-06 01:25:43 Train INFO: [Train]: [020][00002/00051] (5.8%)  Loss=1.0560  cls_loss=0.7039  reg_loss=0.3520  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2597s  iter_time=2.336s
2025-08-06 01:25:45 Train INFO: [Train]: [020][00003/00051] (7.7%)  Loss=1.0575  cls_loss=0.7060  reg_loss=0.3515  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1936s  iter_time=2.293s
2025-08-06 01:27:57 Train INFO: [Train]: [020][00004/00051] (9.6%)  Loss=1.0356  cls_loss=0.6923  reg_loss=0.3433  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2750s  iter_time=131.229s
2025-08-06 01:28:05 Train INFO: [Train]: [020][00005/00051] (11.5%)  Loss=1.0484  cls_loss=0.6985  reg_loss=0.3498  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2304s  iter_time=7.974s
2025-08-06 01:28:07 Train INFO: [Train]: [020][00006/00051] (13.5%)  Loss=1.0687  cls_loss=0.7119  reg_loss=0.3569  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1947s  iter_time=2.328s
2025-08-06 01:28:09 Train INFO: [Train]: [020][00007/00051] (15.4%)  Loss=1.0601  cls_loss=0.7064  reg_loss=0.3537  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1678s  iter_time=2.294s
2025-08-06 01:30:15 Train INFO: [Train]: [020][00008/00051] (17.3%)  Loss=1.0419  cls_loss=0.6942  reg_loss=0.3477  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2060s  iter_time=125.933s
2025-08-06 01:30:25 Train INFO: [Train]: [020][00009/00051] (19.2%)  Loss=1.0458  cls_loss=0.6985  reg_loss=0.3473  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1853s  iter_time=10.209s
2025-08-06 01:30:28 Train INFO: [Train]: [020][00010/00051] (21.2%)  Loss=1.0605  cls_loss=0.7066  reg_loss=0.3538  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1653s  iter_time=2.301s
2025-08-06 01:30:30 Train INFO: [Train]: [020][00011/00051] (23.1%)  Loss=1.0660  cls_loss=0.7093  reg_loss=0.3567  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1486s  iter_time=2.267s
2025-08-06 01:32:37 Train INFO: [Train]: [020][00012/00051] (25.0%)  Loss=1.0622  cls_loss=0.7092  reg_loss=0.3529  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1719s  iter_time=127.188s
2025-08-06 01:32:46 Train INFO: [Train]: [020][00013/00051] (26.9%)  Loss=1.0571  cls_loss=0.7052  reg_loss=0.3518  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1580s  iter_time=8.984s
2025-08-06 01:32:48 Train INFO: [Train]: [020][00014/00051] (28.8%)  Loss=1.0750  cls_loss=0.7150  reg_loss=0.3601  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1441s  iter_time=2.320s
2025-08-06 01:32:51 Train INFO: [Train]: [020][00015/00051] (30.8%)  Loss=1.0668  cls_loss=0.7104  reg_loss=0.3564  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1320s  iter_time=2.317s
2025-08-06 01:35:01 Train INFO: [Train]: [020][00016/00051] (32.7%)  Loss=1.0593  cls_loss=0.7064  reg_loss=0.3529  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1476s  iter_time=130.102s
2025-08-06 01:35:03 Train INFO: [Train]: [020][00017/00051] (34.6%)  Loss=1.0520  cls_loss=0.7015  reg_loss=0.3505  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1358s  iter_time=2.324s
2025-08-06 01:35:07 Train INFO: [Train]: [020][00018/00051] (36.5%)  Loss=1.0521  cls_loss=0.7016  reg_loss=0.3505  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1256s  iter_time=4.184s
2025-08-06 01:35:10 Train INFO: [Train]: [020][00019/00051] (38.5%)  Loss=1.0500  cls_loss=0.7009  reg_loss=0.3491  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1161s  iter_time=2.286s
2025-08-06 01:37:20 Train INFO: [Train]: [020][00020/00051] (40.4%)  Loss=1.0395  cls_loss=0.6937  reg_loss=0.3458  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1264s  iter_time=130.543s
2025-08-06 01:37:22 Train INFO: [Train]: [020][00021/00051] (42.3%)  Loss=1.0434  cls_loss=0.6965  reg_loss=0.3469  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1171s  iter_time=2.352s
2025-08-06 01:37:30 Train INFO: [Train]: [020][00022/00051] (44.2%)  Loss=1.0504  cls_loss=0.7009  reg_loss=0.3495  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1092s  iter_time=7.896s
2025-08-06 01:37:33 Train INFO: [Train]: [020][00023/00051] (46.2%)  Loss=1.0472  cls_loss=0.6980  reg_loss=0.3492  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1013s  iter_time=2.303s
2025-08-06 01:39:45 Train INFO: [Train]: [020][00024/00051] (48.1%)  Loss=1.0519  cls_loss=0.7009  reg_loss=0.3510  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1081s  iter_time=132.576s
2025-08-06 01:39:48 Train INFO: [Train]: [020][00025/00051] (50.0%)  Loss=1.0459  cls_loss=0.6962  reg_loss=0.3497  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1004s  iter_time=2.289s
2025-08-06 01:39:55 Train INFO: [Train]: [020][00026/00051] (51.9%)  Loss=1.0463  cls_loss=0.6970  reg_loss=0.3493  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=936s  iter_time=7.557s
2025-08-06 01:39:57 Train INFO: [Train]: [020][00027/00051] (53.8%)  Loss=1.0511  cls_loss=0.6999  reg_loss=0.3511  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=869s  iter_time=2.375s
2025-08-06 01:42:05 Train INFO: [Train]: [020][00028/00051] (55.8%)  Loss=1.0476  cls_loss=0.6970  reg_loss=0.3506  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=905s  iter_time=127.791s
2025-08-06 01:42:08 Train INFO: [Train]: [020][00029/00051] (57.7%)  Loss=1.0487  cls_loss=0.6972  reg_loss=0.3514  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=839s  iter_time=2.280s
2025-08-06 01:42:15 Train INFO: [Train]: [020][00030/00051] (59.6%)  Loss=1.0478  cls_loss=0.6961  reg_loss=0.3517  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=780s  iter_time=7.697s
2025-08-06 01:42:18 Train INFO: [Train]: [020][00031/00051] (61.5%)  Loss=1.0478  cls_loss=0.6956  reg_loss=0.3521  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=721s  iter_time=2.300s
2025-08-06 01:44:24 Train INFO: [Train]: [020][00032/00051] (63.5%)  Loss=1.0488  cls_loss=0.6963  reg_loss=0.3525  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=126.233s
2025-08-06 01:44:26 Train INFO: [Train]: [020][00033/00051] (65.4%)  Loss=1.0455  cls_loss=0.6941  reg_loss=0.3513  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=679s  iter_time=2.345s
2025-08-06 01:44:34 Train INFO: [Train]: [020][00034/00051] (67.3%)  Loss=1.0479  cls_loss=0.6952  reg_loss=0.3527  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=626s  iter_time=7.644s
2025-08-06 01:44:36 Train INFO: [Train]: [020][00035/00051] (69.2%)  Loss=1.0450  cls_loss=0.6932  reg_loss=0.3518  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=2.318s
2025-08-06 01:46:45 Train INFO: [Train]: [020][00036/00051] (71.2%)  Loss=1.0468  cls_loss=0.6943  reg_loss=0.3525  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=128.602s
2025-08-06 01:46:47 Train INFO: [Train]: [020][00037/00051] (73.1%)  Loss=1.0452  cls_loss=0.6927  reg_loss=0.3525  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=524s  iter_time=2.309s
2025-08-06 01:46:57 Train INFO: [Train]: [020][00038/00051] (75.0%)  Loss=1.0423  cls_loss=0.6904  reg_loss=0.3518  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=478s  iter_time=9.561s
2025-08-06 01:46:59 Train INFO: [Train]: [020][00039/00051] (76.9%)  Loss=1.0438  cls_loss=0.6909  reg_loss=0.3529  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.286s
2025-08-06 01:49:03 Train INFO: [Train]: [020][00040/00051] (78.8%)  Loss=1.0421  cls_loss=0.6892  reg_loss=0.3529  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=418s  iter_time=124.570s
2025-08-06 01:49:06 Train INFO: [Train]: [020][00041/00051] (80.8%)  Loss=1.0411  cls_loss=0.6884  reg_loss=0.3527  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=372s  iter_time=2.275s
2025-08-06 01:49:17 Train INFO: [Train]: [020][00042/00051] (82.7%)  Loss=1.0422  cls_loss=0.6894  reg_loss=0.3528  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=329s  iter_time=11.146s
2025-08-06 01:49:19 Train INFO: [Train]: [020][00043/00051] (84.6%)  Loss=1.0411  cls_loss=0.6883  reg_loss=0.3528  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.276s
2025-08-06 01:51:27 Train INFO: [Train]: [020][00044/00051] (86.5%)  Loss=1.0403  cls_loss=0.6873  reg_loss=0.3530  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=265s  iter_time=127.753s
2025-08-06 01:51:29 Train INFO: [Train]: [020][00045/00051] (88.5%)  Loss=1.0365  cls_loss=0.6848  reg_loss=0.3517  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=222s  iter_time=2.290s
2025-08-06 01:51:40 Train INFO: [Train]: [020][00046/00051] (90.4%)  Loss=1.0356  cls_loss=0.6840  reg_loss=0.3516  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=10.696s
2025-08-06 01:51:42 Train INFO: [Train]: [020][00047/00051] (92.3%)  Loss=1.0327  cls_loss=0.6823  reg_loss=0.3504  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.301s
2025-08-06 01:53:42 Train INFO: [Train]: [020][00048/00051] (94.2%)  Loss=1.0307  cls_loss=0.6811  reg_loss=0.3495  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=119.898s
2025-08-06 01:53:44 Train INFO: [Train]: [020][00049/00051] (96.2%)  Loss=1.0326  cls_loss=0.6823  reg_loss=0.3504  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.165s
2025-08-06 01:53:48 Train INFO: [Train]: [020][00050/00051] (98.1%)  Loss=1.0368  cls_loss=0.6850  reg_loss=0.3519  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=3.753s
2025-08-06 01:53:50 Train INFO: [Train]: [020][00051/00051] (100.0%)  Loss=1.0344  cls_loss=0.6833  reg_loss=0.3511  lr_det=1.0e-04  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.029s
2025-08-06 01:53:51 Train INFO: [Train]: Epoch 20 completed in 1846.7s (avg 35.514s/iter)
2025-08-06 01:53:51 Train INFO: [Train]: Final Loss=1.0344
2025-08-06 01:53:51 Train INFO: [Train]: Epoch 21 started (Total iterations: 52)
2025-08-06 01:56:34 Train INFO: [Train]: [021][00001/00051] (3.8%)  Loss=1.0460  cls_loss=0.6668  reg_loss=0.3792  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4077s  iter_time=163.085s  fwd=2.113s/bwd=0.072s/opt=0.027s
2025-08-06 01:56:36 Train INFO: [Train]: [021][00002/00051] (5.8%)  Loss=1.0622  cls_loss=0.6927  reg_loss=0.3694  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2701s  iter_time=2.294s
2025-08-06 01:56:38 Train INFO: [Train]: [021][00003/00051] (7.7%)  Loss=1.0304  cls_loss=0.6718  reg_loss=0.3586  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2013s  iter_time=2.333s
2025-08-06 01:58:51 Train INFO: [Train]: [021][00004/00051] (9.6%)  Loss=1.0391  cls_loss=0.6750  reg_loss=0.3641  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2825s  iter_time=132.811s
2025-08-06 01:58:54 Train INFO: [Train]: [021][00005/00051] (11.5%)  Loss=1.0442  cls_loss=0.6784  reg_loss=0.3658  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2323s  iter_time=2.483s
2025-08-06 01:58:56 Train INFO: [Train]: [021][00006/00051] (13.5%)  Loss=1.0834  cls_loss=0.7039  reg_loss=0.3794  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1963s  iter_time=2.336s
2025-08-06 01:58:58 Train INFO: [Train]: [021][00007/00051] (15.4%)  Loss=1.0680  cls_loss=0.6989  reg_loss=0.3692  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1692s  iter_time=2.333s
2025-08-06 02:01:08 Train INFO: [Train]: [021][00008/00051] (17.3%)  Loss=1.0461  cls_loss=0.6848  reg_loss=0.3612  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2089s  iter_time=129.633s
2025-08-06 02:01:13 Train INFO: [Train]: [021][00009/00051] (19.2%)  Loss=1.0355  cls_loss=0.6790  reg_loss=0.3565  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1858s  iter_time=5.116s
2025-08-06 02:01:16 Train INFO: [Train]: [021][00010/00051] (21.2%)  Loss=1.0201  cls_loss=0.6694  reg_loss=0.3507  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1658s  iter_time=2.309s
2025-08-06 02:01:18 Train INFO: [Train]: [021][00011/00051] (23.1%)  Loss=1.0360  cls_loss=0.6808  reg_loss=0.3552  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1490s  iter_time=2.362s
2025-08-06 02:03:31 Train INFO: [Train]: [021][00012/00051] (25.0%)  Loss=1.0349  cls_loss=0.6826  reg_loss=0.3523  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1741s  iter_time=133.141s
2025-08-06 02:03:38 Train INFO: [Train]: [021][00013/00051] (26.9%)  Loss=1.0497  cls_loss=0.6935  reg_loss=0.3562  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1594s  iter_time=7.174s
2025-08-06 02:03:40 Train INFO: [Train]: [021][00014/00051] (28.8%)  Loss=1.0571  cls_loss=0.6997  reg_loss=0.3574  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1455s  iter_time=2.312s
2025-08-06 02:03:43 Train INFO: [Train]: [021][00015/00051] (30.8%)  Loss=1.0611  cls_loss=0.7011  reg_loss=0.3601  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1332s  iter_time=2.328s
2025-08-06 02:05:53 Train INFO: [Train]: [021][00016/00051] (32.7%)  Loss=1.0671  cls_loss=0.7063  reg_loss=0.3608  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1487s  iter_time=130.308s
2025-08-06 02:05:55 Train INFO: [Train]: [021][00017/00051] (34.6%)  Loss=1.0612  cls_loss=0.7024  reg_loss=0.3587  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1369s  iter_time=2.312s
2025-08-06 02:05:58 Train INFO: [Train]: [021][00018/00051] (36.5%)  Loss=1.0604  cls_loss=0.7023  reg_loss=0.3581  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1263s  iter_time=2.252s
2025-08-06 02:06:00 Train INFO: [Train]: [021][00019/00051] (38.5%)  Loss=1.0603  cls_loss=0.7025  reg_loss=0.3578  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1167s  iter_time=2.321s
2025-08-06 02:08:13 Train INFO: [Train]: [021][00020/00051] (40.4%)  Loss=1.0527  cls_loss=0.6982  reg_loss=0.3545  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1273s  iter_time=132.881s
2025-08-06 02:08:15 Train INFO: [Train]: [021][00021/00051] (42.3%)  Loss=1.0493  cls_loss=0.6962  reg_loss=0.3531  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1179s  iter_time=2.383s
2025-08-06 02:08:21 Train INFO: [Train]: [021][00022/00051] (44.2%)  Loss=1.0509  cls_loss=0.6968  reg_loss=0.3541  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1097s  iter_time=5.732s
2025-08-06 02:08:23 Train INFO: [Train]: [021][00023/00051] (46.2%)  Loss=1.0402  cls_loss=0.6898  reg_loss=0.3505  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1018s  iter_time=2.331s
2025-08-06 02:10:33 Train INFO: [Train]: [021][00024/00051] (48.1%)  Loss=1.0476  cls_loss=0.6938  reg_loss=0.3539  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1083s  iter_time=130.089s
2025-08-06 02:10:36 Train INFO: [Train]: [021][00025/00051] (50.0%)  Loss=1.0416  cls_loss=0.6894  reg_loss=0.3523  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1005s  iter_time=2.289s
2025-08-06 02:10:39 Train INFO: [Train]: [021][00026/00051] (51.9%)  Loss=1.0477  cls_loss=0.6927  reg_loss=0.3550  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=934s  iter_time=3.535s
2025-08-06 02:10:42 Train INFO: [Train]: [021][00027/00051] (53.8%)  Loss=1.0492  cls_loss=0.6940  reg_loss=0.3552  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=866s  iter_time=2.314s
2025-08-06 02:12:50 Train INFO: [Train]: [021][00028/00051] (55.8%)  Loss=1.0455  cls_loss=0.6918  reg_loss=0.3538  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=904s  iter_time=128.420s
2025-08-06 02:12:54 Train INFO: [Train]: [021][00029/00051] (57.7%)  Loss=1.0483  cls_loss=0.6945  reg_loss=0.3538  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=838s  iter_time=3.596s
2025-08-06 02:12:59 Train INFO: [Train]: [021][00030/00051] (59.6%)  Loss=1.0472  cls_loss=0.6933  reg_loss=0.3539  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=778s  iter_time=5.332s
2025-08-06 02:13:01 Train INFO: [Train]: [021][00031/00051] (61.5%)  Loss=1.0467  cls_loss=0.6937  reg_loss=0.3529  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=719s  iter_time=2.302s
2025-08-06 02:15:10 Train INFO: [Train]: [021][00032/00051] (63.5%)  Loss=1.0440  cls_loss=0.6929  reg_loss=0.3511  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=736s  iter_time=128.318s
2025-08-06 02:15:12 Train INFO: [Train]: [021][00033/00051] (65.4%)  Loss=1.0413  cls_loss=0.6916  reg_loss=0.3498  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=678s  iter_time=2.294s
2025-08-06 02:15:20 Train INFO: [Train]: [021][00034/00051] (67.3%)  Loss=1.0407  cls_loss=0.6918  reg_loss=0.3488  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=626s  iter_time=8.109s
2025-08-06 02:15:22 Train INFO: [Train]: [021][00035/00051] (69.2%)  Loss=1.0407  cls_loss=0.6922  reg_loss=0.3485  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=2.291s
2025-08-06 02:17:33 Train INFO: [Train]: [021][00036/00051] (71.2%)  Loss=1.0415  cls_loss=0.6923  reg_loss=0.3491  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=130.400s
2025-08-06 02:17:35 Train INFO: [Train]: [021][00037/00051] (73.1%)  Loss=1.0416  cls_loss=0.6920  reg_loss=0.3496  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=525s  iter_time=2.376s
2025-08-06 02:17:43 Train INFO: [Train]: [021][00038/00051] (75.0%)  Loss=1.0437  cls_loss=0.6933  reg_loss=0.3505  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=477s  iter_time=7.592s
2025-08-06 02:17:45 Train INFO: [Train]: [021][00039/00051] (76.9%)  Loss=1.0441  cls_loss=0.6930  reg_loss=0.3511  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.286s
2025-08-06 02:19:55 Train INFO: [Train]: [021][00040/00051] (78.8%)  Loss=1.0441  cls_loss=0.6930  reg_loss=0.3511  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=129.942s
2025-08-06 02:19:57 Train INFO: [Train]: [021][00041/00051] (80.8%)  Loss=1.0426  cls_loss=0.6915  reg_loss=0.3511  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=373s  iter_time=2.343s
2025-08-06 02:20:02 Train INFO: [Train]: [021][00042/00051] (82.7%)  Loss=1.0479  cls_loss=0.6951  reg_loss=0.3528  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=329s  iter_time=5.169s
2025-08-06 02:20:05 Train INFO: [Train]: [021][00043/00051] (84.6%)  Loss=1.0453  cls_loss=0.6929  reg_loss=0.3524  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.295s
2025-08-06 02:22:20 Train INFO: [Train]: [021][00044/00051] (86.5%)  Loss=1.0460  cls_loss=0.6931  reg_loss=0.3528  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=135.330s
2025-08-06 02:22:22 Train INFO: [Train]: [021][00045/00051] (88.5%)  Loss=1.0431  cls_loss=0.6913  reg_loss=0.3519  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.271s
2025-08-06 02:22:27 Train INFO: [Train]: [021][00046/00051] (90.4%)  Loss=1.0427  cls_loss=0.6902  reg_loss=0.3525  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=5.036s
2025-08-06 02:22:30 Train INFO: [Train]: [021][00047/00051] (92.3%)  Loss=1.0419  cls_loss=0.6894  reg_loss=0.3525  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.284s
2025-08-06 02:24:34 Train INFO: [Train]: [021][00048/00051] (94.2%)  Loss=1.0397  cls_loss=0.6881  reg_loss=0.3515  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=124.625s
2025-08-06 02:24:36 Train INFO: [Train]: [021][00049/00051] (96.2%)  Loss=1.0395  cls_loss=0.6883  reg_loss=0.3512  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.171s
2025-08-06 02:24:40 Train INFO: [Train]: [021][00050/00051] (98.1%)  Loss=1.0431  cls_loss=0.6903  reg_loss=0.3528  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=3.943s
2025-08-06 02:24:42 Train INFO: [Train]: [021][00051/00051] (100.0%)  Loss=1.0386  cls_loss=0.6873  reg_loss=0.3513  lr_det=5.2e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.040s
2025-08-06 02:24:43 Train INFO: [Train]: Epoch 21 completed in 1852.3s (avg 35.622s/iter)
2025-08-06 02:24:43 Train INFO: [Train]: Final Loss=1.0386
2025-08-06 02:24:43 Train INFO: [Train]: Epoch 22 started (Total iterations: 52)
2025-08-06 02:27:24 Train INFO: [Train]: [022][00001/00051] (3.8%)  Loss=0.9878  cls_loss=0.6422  reg_loss=0.3456  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4018s  iter_time=160.725s  fwd=2.135s/bwd=0.047s/opt=0.024s
2025-08-06 02:27:26 Train INFO: [Train]: [022][00002/00051] (5.8%)  Loss=1.0729  cls_loss=0.7004  reg_loss=0.3724  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2663s  iter_time=2.336s
2025-08-06 02:27:28 Train INFO: [Train]: [022][00003/00051] (7.7%)  Loss=1.0817  cls_loss=0.7067  reg_loss=0.3750  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1983s  iter_time=2.224s
2025-08-06 02:29:45 Train INFO: [Train]: [022][00004/00051] (9.6%)  Loss=1.0536  cls_loss=0.6892  reg_loss=0.3644  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2835s  iter_time=136.272s
2025-08-06 02:29:47 Train INFO: [Train]: [022][00005/00051] (11.5%)  Loss=1.0450  cls_loss=0.6841  reg_loss=0.3608  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2330s  iter_time=2.321s
2025-08-06 02:29:49 Train INFO: [Train]: [022][00006/00051] (13.5%)  Loss=1.0485  cls_loss=0.6886  reg_loss=0.3599  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1968s  iter_time=2.272s
2025-08-06 02:29:52 Train INFO: [Train]: [022][00007/00051] (15.4%)  Loss=1.0488  cls_loss=0.6878  reg_loss=0.3610  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1696s  iter_time=2.255s
2025-08-06 02:32:04 Train INFO: [Train]: [022][00008/00051] (17.3%)  Loss=1.0347  cls_loss=0.6814  reg_loss=0.3533  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2105s  iter_time=132.234s
2025-08-06 02:32:08 Train INFO: [Train]: [022][00009/00051] (19.2%)  Loss=1.0513  cls_loss=0.6928  reg_loss=0.3585  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1867s  iter_time=3.860s
2025-08-06 02:32:10 Train INFO: [Train]: [022][00010/00051] (21.2%)  Loss=1.0528  cls_loss=0.6930  reg_loss=0.3598  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1665s  iter_time=2.317s
2025-08-06 02:32:12 Train INFO: [Train]: [022][00011/00051] (23.1%)  Loss=1.0529  cls_loss=0.6936  reg_loss=0.3592  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1497s  iter_time=2.375s
2025-08-06 02:34:27 Train INFO: [Train]: [022][00012/00051] (25.0%)  Loss=1.0389  cls_loss=0.6855  reg_loss=0.3534  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1751s  iter_time=134.599s
2025-08-06 02:34:31 Train INFO: [Train]: [022][00013/00051] (26.9%)  Loss=1.0405  cls_loss=0.6852  reg_loss=0.3553  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1596s  iter_time=4.254s
2025-08-06 02:34:35 Train INFO: [Train]: [022][00014/00051] (28.8%)  Loss=1.0443  cls_loss=0.6880  reg_loss=0.3563  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1461s  iter_time=4.140s
2025-08-06 02:34:38 Train INFO: [Train]: [022][00015/00051] (30.8%)  Loss=1.0406  cls_loss=0.6873  reg_loss=0.3533  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1338s  iter_time=2.309s
2025-08-06 02:36:50 Train INFO: [Train]: [022][00016/00051] (32.7%)  Loss=1.0378  cls_loss=0.6853  reg_loss=0.3525  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1498s  iter_time=132.865s
2025-08-06 02:36:53 Train INFO: [Train]: [022][00017/00051] (34.6%)  Loss=1.0329  cls_loss=0.6808  reg_loss=0.3521  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1378s  iter_time=2.334s
2025-08-06 02:36:55 Train INFO: [Train]: [022][00018/00051] (36.5%)  Loss=1.0236  cls_loss=0.6757  reg_loss=0.3478  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1271s  iter_time=2.293s
2025-08-06 02:36:57 Train INFO: [Train]: [022][00019/00051] (38.5%)  Loss=1.0295  cls_loss=0.6791  reg_loss=0.3504  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1175s  iter_time=2.276s
2025-08-06 02:39:08 Train INFO: [Train]: [022][00020/00051] (40.4%)  Loss=1.0296  cls_loss=0.6783  reg_loss=0.3512  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1277s  iter_time=130.456s
2025-08-06 02:39:10 Train INFO: [Train]: [022][00021/00051] (42.3%)  Loss=1.0281  cls_loss=0.6772  reg_loss=0.3508  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1182s  iter_time=2.275s
2025-08-06 02:39:12 Train INFO: [Train]: [022][00022/00051] (44.2%)  Loss=1.0370  cls_loss=0.6820  reg_loss=0.3551  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1096s  iter_time=2.298s
2025-08-06 02:39:15 Train INFO: [Train]: [022][00023/00051] (46.2%)  Loss=1.0283  cls_loss=0.6762  reg_loss=0.3521  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1017s  iter_time=2.300s
2025-08-06 02:41:27 Train INFO: [Train]: [022][00024/00051] (48.1%)  Loss=1.0358  cls_loss=0.6809  reg_loss=0.3548  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1084s  iter_time=132.110s
2025-08-06 02:41:29 Train INFO: [Train]: [022][00025/00051] (50.0%)  Loss=1.0344  cls_loss=0.6800  reg_loss=0.3544  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1006s  iter_time=2.322s
2025-08-06 02:41:32 Train INFO: [Train]: [022][00026/00051] (51.9%)  Loss=1.0407  cls_loss=0.6838  reg_loss=0.3569  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=935s  iter_time=3.231s
2025-08-06 02:41:35 Train INFO: [Train]: [022][00027/00051] (53.8%)  Loss=1.0442  cls_loss=0.6852  reg_loss=0.3590  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=867s  iter_time=2.281s
2025-08-06 02:43:43 Train INFO: [Train]: [022][00028/00051] (55.8%)  Loss=1.0448  cls_loss=0.6855  reg_loss=0.3593  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=904s  iter_time=128.718s
2025-08-06 02:43:51 Train INFO: [Train]: [022][00029/00051] (57.7%)  Loss=1.0459  cls_loss=0.6864  reg_loss=0.3595  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=842s  iter_time=7.959s
2025-08-06 02:43:54 Train INFO: [Train]: [022][00030/00051] (59.6%)  Loss=1.0447  cls_loss=0.6860  reg_loss=0.3587  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=779s  iter_time=2.336s
2025-08-06 02:43:56 Train INFO: [Train]: [022][00031/00051] (61.5%)  Loss=1.0451  cls_loss=0.6865  reg_loss=0.3586  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=721s  iter_time=2.284s
2025-08-06 02:46:03 Train INFO: [Train]: [022][00032/00051] (63.5%)  Loss=1.0461  cls_loss=0.6868  reg_loss=0.3593  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=127.522s
2025-08-06 02:46:08 Train INFO: [Train]: [022][00033/00051] (65.4%)  Loss=1.0408  cls_loss=0.6832  reg_loss=0.3576  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=680s  iter_time=4.716s
2025-08-06 02:46:11 Train INFO: [Train]: [022][00034/00051] (67.3%)  Loss=1.0434  cls_loss=0.6847  reg_loss=0.3587  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=626s  iter_time=2.699s
2025-08-06 02:46:13 Train INFO: [Train]: [022][00035/00051] (69.2%)  Loss=1.0431  cls_loss=0.6847  reg_loss=0.3585  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=573s  iter_time=2.310s
2025-08-06 02:48:27 Train INFO: [Train]: [022][00036/00051] (71.2%)  Loss=1.0436  cls_loss=0.6849  reg_loss=0.3587  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=577s  iter_time=133.432s
2025-08-06 02:48:29 Train INFO: [Train]: [022][00037/00051] (73.1%)  Loss=1.0390  cls_loss=0.6815  reg_loss=0.3575  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=525s  iter_time=2.349s
2025-08-06 02:48:36 Train INFO: [Train]: [022][00038/00051] (75.0%)  Loss=1.0394  cls_loss=0.6813  reg_loss=0.3581  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=478s  iter_time=7.258s
2025-08-06 02:48:39 Train INFO: [Train]: [022][00039/00051] (76.9%)  Loss=1.0371  cls_loss=0.6792  reg_loss=0.3580  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=431s  iter_time=2.354s
2025-08-06 02:50:49 Train INFO: [Train]: [022][00040/00051] (78.8%)  Loss=1.0384  cls_loss=0.6798  reg_loss=0.3587  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=130.206s
2025-08-06 02:50:51 Train INFO: [Train]: [022][00041/00051] (80.8%)  Loss=1.0359  cls_loss=0.6782  reg_loss=0.3577  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=373s  iter_time=2.366s
2025-08-06 02:50:56 Train INFO: [Train]: [022][00042/00051] (82.7%)  Loss=1.0386  cls_loss=0.6803  reg_loss=0.3584  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=329s  iter_time=4.590s
2025-08-06 02:50:58 Train INFO: [Train]: [022][00043/00051] (84.6%)  Loss=1.0406  cls_loss=0.6813  reg_loss=0.3592  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.341s
2025-08-06 02:53:16 Train INFO: [Train]: [022][00044/00051] (86.5%)  Loss=1.0375  cls_loss=0.6793  reg_loss=0.3582  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=137.385s
2025-08-06 02:53:18 Train INFO: [Train]: [022][00045/00051] (88.5%)  Loss=1.0377  cls_loss=0.6802  reg_loss=0.3576  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=2.373s
2025-08-06 02:53:24 Train INFO: [Train]: [022][00046/00051] (90.4%)  Loss=1.0373  cls_loss=0.6798  reg_loss=0.3574  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=6.159s
2025-08-06 02:53:26 Train INFO: [Train]: [022][00047/00051] (92.3%)  Loss=1.0352  cls_loss=0.6785  reg_loss=0.3567  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.330s
2025-08-06 02:55:31 Train INFO: [Train]: [022][00048/00051] (94.2%)  Loss=1.0330  cls_loss=0.6773  reg_loss=0.3557  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=124.600s
2025-08-06 02:55:33 Train INFO: [Train]: [022][00049/00051] (96.2%)  Loss=1.0303  cls_loss=0.6758  reg_loss=0.3545  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.185s
2025-08-06 02:55:36 Train INFO: [Train]: [022][00050/00051] (98.1%)  Loss=1.0289  cls_loss=0.6754  reg_loss=0.3535  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.634s
2025-08-06 02:55:38 Train INFO: [Train]: [022][00051/00051] (100.0%)  Loss=1.0268  cls_loss=0.6744  reg_loss=0.3524  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.043s
2025-08-06 02:55:39 Train INFO: [Train]: Epoch 22 completed in 1855.5s (avg 35.682s/iter)
2025-08-06 02:55:39 Train INFO: [Train]: Final Loss=1.0268
2025-08-06 02:55:39 Train INFO: [Train]: Epoch 23 started (Total iterations: 52)
2025-08-06 02:58:20 Train INFO: [Train]: [023][00001/00051] (3.8%)  Loss=0.9675  cls_loss=0.6365  reg_loss=0.3310  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4023s  iter_time=160.939s  fwd=2.122s/bwd=0.080s/opt=0.014s
2025-08-06 02:58:22 Train INFO: [Train]: [023][00002/00051] (5.8%)  Loss=1.0515  cls_loss=0.6938  reg_loss=0.3577  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2665s  iter_time=2.236s
2025-08-06 02:58:24 Train INFO: [Train]: [023][00003/00051] (7.7%)  Loss=1.0719  cls_loss=0.7039  reg_loss=0.3679  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1986s  iter_time=2.325s
2025-08-06 03:00:39 Train INFO: [Train]: [023][00004/00051] (9.6%)  Loss=1.0281  cls_loss=0.6763  reg_loss=0.3518  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2824s  iter_time=134.902s
2025-08-06 03:00:44 Train INFO: [Train]: [023][00005/00051] (11.5%)  Loss=1.0437  cls_loss=0.6875  reg_loss=0.3562  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2342s  iter_time=5.079s
2025-08-06 03:00:46 Train INFO: [Train]: [023][00006/00051] (13.5%)  Loss=1.0636  cls_loss=0.6986  reg_loss=0.3650  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1979s  iter_time=2.339s
2025-08-06 03:00:49 Train INFO: [Train]: [023][00007/00051] (15.4%)  Loss=1.0791  cls_loss=0.7106  reg_loss=0.3685  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1706s  iter_time=2.306s
2025-08-06 03:02:55 Train INFO: [Train]: [023][00008/00051] (17.3%)  Loss=1.0437  cls_loss=0.6884  reg_loss=0.3553  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2086s  iter_time=126.418s
2025-08-06 03:03:05 Train INFO: [Train]: [023][00009/00051] (19.2%)  Loss=1.0273  cls_loss=0.6779  reg_loss=0.3494  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1875s  iter_time=9.766s
2025-08-06 03:03:09 Train INFO: [Train]: [023][00010/00051] (21.2%)  Loss=1.0298  cls_loss=0.6798  reg_loss=0.3500  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1680s  iter_time=4.418s
2025-08-06 03:03:12 Train INFO: [Train]: [023][00011/00051] (23.1%)  Loss=1.0334  cls_loss=0.6794  reg_loss=0.3540  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1510s  iter_time=2.312s
2025-08-06 03:05:17 Train INFO: [Train]: [023][00012/00051] (25.0%)  Loss=1.0241  cls_loss=0.6737  reg_loss=0.3504  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1736s  iter_time=125.483s
2025-08-06 03:05:27 Train INFO: [Train]: [023][00013/00051] (26.9%)  Loss=1.0272  cls_loss=0.6762  reg_loss=0.3510  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1596s  iter_time=9.517s
2025-08-06 03:05:34 Train INFO: [Train]: [023][00014/00051] (28.8%)  Loss=1.0342  cls_loss=0.6802  reg_loss=0.3540  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1470s  iter_time=7.855s
2025-08-06 03:05:37 Train INFO: [Train]: [023][00015/00051] (30.8%)  Loss=1.0319  cls_loss=0.6780  reg_loss=0.3539  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1346s  iter_time=2.333s
2025-08-06 03:07:40 Train INFO: [Train]: [023][00016/00051] (32.7%)  Loss=1.0325  cls_loss=0.6782  reg_loss=0.3543  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1485s  iter_time=122.993s
2025-08-06 03:07:42 Train INFO: [Train]: [023][00017/00051] (34.6%)  Loss=1.0234  cls_loss=0.6724  reg_loss=0.3510  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1367s  iter_time=2.344s
2025-08-06 03:07:51 Train INFO: [Train]: [023][00018/00051] (36.5%)  Loss=1.0213  cls_loss=0.6708  reg_loss=0.3505  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1271s  iter_time=8.462s
2025-08-06 03:07:53 Train INFO: [Train]: [023][00019/00051] (38.5%)  Loss=1.0232  cls_loss=0.6725  reg_loss=0.3507  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1175s  iter_time=2.319s
2025-08-06 03:09:59 Train INFO: [Train]: [023][00020/00051] (40.4%)  Loss=1.0161  cls_loss=0.6681  reg_loss=0.3480  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1270s  iter_time=126.059s
2025-08-06 03:10:01 Train INFO: [Train]: [023][00021/00051] (42.3%)  Loss=1.0166  cls_loss=0.6680  reg_loss=0.3486  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1176s  iter_time=2.310s
2025-08-06 03:10:15 Train INFO: [Train]: [023][00022/00051] (44.2%)  Loss=1.0224  cls_loss=0.6712  reg_loss=0.3512  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1105s  iter_time=13.446s
2025-08-06 03:10:17 Train INFO: [Train]: [023][00023/00051] (46.2%)  Loss=1.0186  cls_loss=0.6696  reg_loss=0.3490  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1025s  iter_time=2.339s
2025-08-06 03:12:20 Train INFO: [Train]: [023][00024/00051] (48.1%)  Loss=1.0315  cls_loss=0.6777  reg_loss=0.3538  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1082s  iter_time=123.156s
2025-08-06 03:12:23 Train INFO: [Train]: [023][00025/00051] (50.0%)  Loss=1.0327  cls_loss=0.6785  reg_loss=0.3542  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1004s  iter_time=2.655s
2025-08-06 03:12:34 Train INFO: [Train]: [023][00026/00051] (51.9%)  Loss=1.0347  cls_loss=0.6798  reg_loss=0.3549  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=940s  iter_time=10.787s
2025-08-06 03:12:36 Train INFO: [Train]: [023][00027/00051] (53.8%)  Loss=1.0313  cls_loss=0.6777  reg_loss=0.3536  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=872s  iter_time=2.336s
2025-08-06 03:14:39 Train INFO: [Train]: [023][00028/00051] (55.8%)  Loss=1.0278  cls_loss=0.6757  reg_loss=0.3521  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=905s  iter_time=123.039s
2025-08-06 03:14:45 Train INFO: [Train]: [023][00029/00051] (57.7%)  Loss=1.0287  cls_loss=0.6772  reg_loss=0.3515  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=841s  iter_time=6.319s
2025-08-06 03:14:51 Train INFO: [Train]: [023][00030/00051] (59.6%)  Loss=1.0245  cls_loss=0.6748  reg_loss=0.3497  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=780s  iter_time=5.166s
2025-08-06 03:14:53 Train INFO: [Train]: [023][00031/00051] (61.5%)  Loss=1.0208  cls_loss=0.6721  reg_loss=0.3487  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=721s  iter_time=2.297s
2025-08-06 03:16:58 Train INFO: [Train]: [023][00032/00051] (63.5%)  Loss=1.0232  cls_loss=0.6747  reg_loss=0.3485  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=125.573s
2025-08-06 03:17:01 Train INFO: [Train]: [023][00033/00051] (65.4%)  Loss=1.0217  cls_loss=0.6742  reg_loss=0.3475  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=679s  iter_time=2.731s
2025-08-06 03:17:09 Train INFO: [Train]: [023][00034/00051] (67.3%)  Loss=1.0208  cls_loss=0.6745  reg_loss=0.3464  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=627s  iter_time=7.455s
2025-08-06 03:17:11 Train INFO: [Train]: [023][00035/00051] (69.2%)  Loss=1.0200  cls_loss=0.6743  reg_loss=0.3457  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=2.286s
2025-08-06 03:19:22 Train INFO: [Train]: [023][00036/00051] (71.2%)  Loss=1.0192  cls_loss=0.6742  reg_loss=0.3450  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=577s  iter_time=131.449s
2025-08-06 03:19:25 Train INFO: [Train]: [023][00037/00051] (73.1%)  Loss=1.0194  cls_loss=0.6741  reg_loss=0.3453  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=525s  iter_time=2.256s
2025-08-06 03:19:30 Train INFO: [Train]: [023][00038/00051] (75.0%)  Loss=1.0240  cls_loss=0.6769  reg_loss=0.3471  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=477s  iter_time=5.699s
2025-08-06 03:19:33 Train INFO: [Train]: [023][00039/00051] (76.9%)  Loss=1.0264  cls_loss=0.6785  reg_loss=0.3479  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.318s
2025-08-06 03:21:42 Train INFO: [Train]: [023][00040/00051] (78.8%)  Loss=1.0271  cls_loss=0.6791  reg_loss=0.3480  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=129.835s
2025-08-06 03:21:45 Train INFO: [Train]: [023][00041/00051] (80.8%)  Loss=1.0264  cls_loss=0.6781  reg_loss=0.3483  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=373s  iter_time=2.355s
2025-08-06 03:21:47 Train INFO: [Train]: [023][00042/00051] (82.7%)  Loss=1.0282  cls_loss=0.6787  reg_loss=0.3495  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=2.643s
2025-08-06 03:21:50 Train INFO: [Train]: [023][00043/00051] (84.6%)  Loss=1.0259  cls_loss=0.6768  reg_loss=0.3491  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.280s
2025-08-06 03:24:06 Train INFO: [Train]: [023][00044/00051] (86.5%)  Loss=1.0252  cls_loss=0.6763  reg_loss=0.3489  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=136.506s
2025-08-06 03:24:09 Train INFO: [Train]: [023][00045/00051] (88.5%)  Loss=1.0238  cls_loss=0.6752  reg_loss=0.3485  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.281s
2025-08-06 03:24:11 Train INFO: [Train]: [023][00046/00051] (90.4%)  Loss=1.0253  cls_loss=0.6761  reg_loss=0.3491  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=2.323s
2025-08-06 03:24:13 Train INFO: [Train]: [023][00047/00051] (92.3%)  Loss=1.0237  cls_loss=0.6747  reg_loss=0.3489  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.232s
2025-08-06 03:26:22 Train INFO: [Train]: [023][00048/00051] (94.2%)  Loss=1.0236  cls_loss=0.6749  reg_loss=0.3488  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=128.786s
2025-08-06 03:26:24 Train INFO: [Train]: [023][00049/00051] (96.2%)  Loss=1.0250  cls_loss=0.6754  reg_loss=0.3496  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.167s
2025-08-06 03:26:26 Train INFO: [Train]: [023][00050/00051] (98.1%)  Loss=1.0283  cls_loss=0.6777  reg_loss=0.3505  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.209s
2025-08-06 03:26:28 Train INFO: [Train]: [023][00051/00051] (100.0%)  Loss=1.0277  cls_loss=0.6771  reg_loss=0.3506  lr_det=4.8e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.064s
2025-08-06 03:26:29 Train INFO: [Train]: Epoch 23 completed in 1850.5s (avg 35.586s/iter)
2025-08-06 03:26:29 Train INFO: [Train]: Final Loss=1.0277
2025-08-06 03:26:29 Train INFO: [Train]: Epoch 24 started (Total iterations: 52)
2025-08-06 03:29:11 Train INFO: [Train]: [024][00001/00051] (3.8%)  Loss=0.8350  cls_loss=0.5525  reg_loss=0.2825  lr_det=5.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4040s  iter_time=161.612s  fwd=2.121s/bwd=0.069s/opt=0.022s
2025-08-06 03:29:13 Train INFO: [Train]: [024][00002/00051] (5.8%)  Loss=0.9246  cls_loss=0.6109  reg_loss=0.3137  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2677s  iter_time=2.286s
2025-08-06 03:29:15 Train INFO: [Train]: [024][00003/00051] (7.7%)  Loss=0.9873  cls_loss=0.6515  reg_loss=0.3359  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1995s  iter_time=2.318s
2025-08-06 03:31:31 Train INFO: [Train]: [024][00004/00051] (9.6%)  Loss=0.9605  cls_loss=0.6350  reg_loss=0.3255  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2841s  iter_time=136.051s
2025-08-06 03:31:34 Train INFO: [Train]: [024][00005/00051] (11.5%)  Loss=0.9527  cls_loss=0.6339  reg_loss=0.3189  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2337s  iter_time=2.553s
2025-08-06 03:31:36 Train INFO: [Train]: [024][00006/00051] (13.5%)  Loss=0.9727  cls_loss=0.6443  reg_loss=0.3284  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1974s  iter_time=2.275s
2025-08-06 03:31:39 Train INFO: [Train]: [024][00007/00051] (15.4%)  Loss=0.9766  cls_loss=0.6470  reg_loss=0.3296  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1702s  iter_time=2.334s
2025-08-06 03:33:44 Train INFO: [Train]: [024][00008/00051] (17.3%)  Loss=0.9655  cls_loss=0.6407  reg_loss=0.3248  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2078s  iter_time=125.486s
2025-08-06 03:33:54 Train INFO: [Train]: [024][00009/00051] (19.2%)  Loss=0.9722  cls_loss=0.6462  reg_loss=0.3260  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1870s  iter_time=10.268s
2025-08-06 03:33:57 Train INFO: [Train]: [024][00010/00051] (21.2%)  Loss=0.9846  cls_loss=0.6546  reg_loss=0.3300  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1668s  iter_time=2.275s
2025-08-06 03:33:59 Train INFO: [Train]: [024][00011/00051] (23.1%)  Loss=1.0116  cls_loss=0.6693  reg_loss=0.3423  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1499s  iter_time=2.314s
2025-08-06 03:36:04 Train INFO: [Train]: [024][00012/00051] (25.0%)  Loss=1.0159  cls_loss=0.6755  reg_loss=0.3404  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1724s  iter_time=124.983s
2025-08-06 03:36:18 Train INFO: [Train]: [024][00013/00051] (26.9%)  Loss=1.0276  cls_loss=0.6847  reg_loss=0.3429  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1599s  iter_time=14.382s
2025-08-06 03:36:20 Train INFO: [Train]: [024][00014/00051] (28.8%)  Loss=1.0230  cls_loss=0.6828  reg_loss=0.3402  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1459s  iter_time=2.284s
2025-08-06 03:36:23 Train INFO: [Train]: [024][00015/00051] (30.8%)  Loss=1.0184  cls_loss=0.6803  reg_loss=0.3381  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1336s  iter_time=2.262s
2025-08-06 03:38:29 Train INFO: [Train]: [024][00016/00051] (32.7%)  Loss=1.0203  cls_loss=0.6820  reg_loss=0.3383  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1483s  iter_time=126.658s
2025-08-06 03:38:36 Train INFO: [Train]: [024][00017/00051] (34.6%)  Loss=1.0099  cls_loss=0.6752  reg_loss=0.3347  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1373s  iter_time=6.278s
2025-08-06 03:38:38 Train INFO: [Train]: [024][00018/00051] (36.5%)  Loss=1.0038  cls_loss=0.6710  reg_loss=0.3329  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1266s  iter_time=2.254s
2025-08-06 03:38:40 Train INFO: [Train]: [024][00019/00051] (38.5%)  Loss=1.0085  cls_loss=0.6738  reg_loss=0.3347  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1170s  iter_time=2.242s
2025-08-06 03:40:46 Train INFO: [Train]: [024][00020/00051] (40.4%)  Loss=1.0023  cls_loss=0.6687  reg_loss=0.3336  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1265s  iter_time=126.141s
2025-08-06 03:40:52 Train INFO: [Train]: [024][00021/00051] (42.3%)  Loss=1.0038  cls_loss=0.6705  reg_loss=0.3333  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1177s  iter_time=5.683s
2025-08-06 03:40:54 Train INFO: [Train]: [024][00022/00051] (44.2%)  Loss=1.0183  cls_loss=0.6796  reg_loss=0.3388  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1091s  iter_time=2.331s
2025-08-06 03:40:57 Train INFO: [Train]: [024][00023/00051] (46.2%)  Loss=1.0133  cls_loss=0.6757  reg_loss=0.3376  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1012s  iter_time=2.312s
2025-08-06 03:43:07 Train INFO: [Train]: [024][00024/00051] (48.1%)  Loss=1.0190  cls_loss=0.6798  reg_loss=0.3392  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1078s  iter_time=130.400s
2025-08-06 03:43:18 Train INFO: [Train]: [024][00025/00051] (50.0%)  Loss=1.0146  cls_loss=0.6769  reg_loss=0.3376  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1009s  iter_time=10.784s
2025-08-06 03:43:20 Train INFO: [Train]: [024][00026/00051] (51.9%)  Loss=1.0183  cls_loss=0.6797  reg_loss=0.3386  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=936s  iter_time=2.296s
2025-08-06 03:43:22 Train INFO: [Train]: [024][00027/00051] (53.8%)  Loss=1.0186  cls_loss=0.6805  reg_loss=0.3380  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=869s  iter_time=2.241s
2025-08-06 03:45:28 Train INFO: [Train]: [024][00028/00051] (55.8%)  Loss=1.0171  cls_loss=0.6785  reg_loss=0.3385  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=903s  iter_time=125.445s
2025-08-06 03:45:40 Train INFO: [Train]: [024][00029/00051] (57.7%)  Loss=1.0210  cls_loss=0.6810  reg_loss=0.3401  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=844s  iter_time=11.808s
2025-08-06 03:45:42 Train INFO: [Train]: [024][00030/00051] (59.6%)  Loss=1.0221  cls_loss=0.6811  reg_loss=0.3410  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=781s  iter_time=2.293s
2025-08-06 03:45:44 Train INFO: [Train]: [024][00031/00051] (61.5%)  Loss=1.0167  cls_loss=0.6771  reg_loss=0.3395  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=722s  iter_time=2.302s
2025-08-06 03:47:46 Train INFO: [Train]: [024][00032/00051] (63.5%)  Loss=1.0205  cls_loss=0.6801  reg_loss=0.3405  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=735s  iter_time=122.114s
2025-08-06 03:47:58 Train INFO: [Train]: [024][00033/00051] (65.4%)  Loss=1.0149  cls_loss=0.6766  reg_loss=0.3383  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=682s  iter_time=11.201s
2025-08-06 03:48:00 Train INFO: [Train]: [024][00034/00051] (67.3%)  Loss=1.0161  cls_loss=0.6775  reg_loss=0.3386  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=627s  iter_time=2.389s
2025-08-06 03:48:02 Train INFO: [Train]: [024][00035/00051] (69.2%)  Loss=1.0206  cls_loss=0.6799  reg_loss=0.3407  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=575s  iter_time=2.240s
2025-08-06 03:50:13 Train INFO: [Train]: [024][00036/00051] (71.2%)  Loss=1.0234  cls_loss=0.6818  reg_loss=0.3416  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=577s  iter_time=130.350s
2025-08-06 03:50:24 Train INFO: [Train]: [024][00037/00051] (73.1%)  Loss=1.0209  cls_loss=0.6799  reg_loss=0.3410  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=529s  iter_time=11.578s
2025-08-06 03:50:26 Train INFO: [Train]: [024][00038/00051] (75.0%)  Loss=1.0257  cls_loss=0.6828  reg_loss=0.3429  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=479s  iter_time=2.284s
2025-08-06 03:50:29 Train INFO: [Train]: [024][00039/00051] (76.9%)  Loss=1.0264  cls_loss=0.6830  reg_loss=0.3434  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=432s  iter_time=2.204s
2025-08-06 03:52:34 Train INFO: [Train]: [024][00040/00051] (78.8%)  Loss=1.0270  cls_loss=0.6835  reg_loss=0.3435  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=125.319s
2025-08-06 03:52:45 Train INFO: [Train]: [024][00041/00051] (80.8%)  Loss=1.0248  cls_loss=0.6822  reg_loss=0.3427  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=375s  iter_time=10.967s
2025-08-06 03:52:47 Train INFO: [Train]: [024][00042/00051] (82.7%)  Loss=1.0271  cls_loss=0.6837  reg_loss=0.3434  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=330s  iter_time=2.309s
2025-08-06 03:52:49 Train INFO: [Train]: [024][00043/00051] (84.6%)  Loss=1.0269  cls_loss=0.6828  reg_loss=0.3441  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=287s  iter_time=2.215s
2025-08-06 03:54:58 Train INFO: [Train]: [024][00044/00051] (86.5%)  Loss=1.0283  cls_loss=0.6829  reg_loss=0.3454  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=128.592s
2025-08-06 03:55:04 Train INFO: [Train]: [024][00045/00051] (88.5%)  Loss=1.0272  cls_loss=0.6825  reg_loss=0.3447  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=6.243s
2025-08-06 03:55:07 Train INFO: [Train]: [024][00046/00051] (90.4%)  Loss=1.0283  cls_loss=0.6828  reg_loss=0.3455  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=2.434s
2025-08-06 03:55:09 Train INFO: [Train]: [024][00047/00051] (92.3%)  Loss=1.0279  cls_loss=0.6821  reg_loss=0.3457  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.316s
2025-08-06 03:57:18 Train INFO: [Train]: [024][00048/00051] (94.2%)  Loss=1.0273  cls_loss=0.6819  reg_loss=0.3454  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=129.062s
2025-08-06 03:57:23 Train INFO: [Train]: [024][00049/00051] (96.2%)  Loss=1.0265  cls_loss=0.6812  reg_loss=0.3453  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=5.053s
2025-08-06 03:57:25 Train INFO: [Train]: [024][00050/00051] (98.1%)  Loss=1.0261  cls_loss=0.6814  reg_loss=0.3447  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.167s
2025-08-06 03:57:27 Train INFO: [Train]: [024][00051/00051] (100.0%)  Loss=1.0244  cls_loss=0.6802  reg_loss=0.3442  lr_det=1.0e-04  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.027s
2025-08-06 03:57:28 Train INFO: [Train]: Epoch 24 completed in 1859.0s (avg 35.749s/iter)
2025-08-06 03:57:28 Train INFO: [Train]: Final Loss=1.0244
2025-08-06 03:57:28 Train INFO: [Val]: Epoch 24 Loss
2025-08-06 04:16:26 Train INFO: [Val]: [024]  Loss=1.4595  cls_loss=1.0819  reg_loss=0.3776  Average-mAP=0.22%
2025-08-06 04:16:27 Train INFO: Checkpoint saved at epoch 24
2025-08-06 04:16:27 Train INFO: [Train]: Epoch 25 started (Total iterations: 52)
2025-08-06 04:19:06 Train INFO: [Train]: [025][00001/00051] (3.8%)  Loss=1.0170  cls_loss=0.6921  reg_loss=0.3249  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3975s  iter_time=159.012s  fwd=2.142s/bwd=0.073s/opt=0.014s
2025-08-06 04:19:08 Train INFO: [Train]: [025][00002/00051] (5.8%)  Loss=1.0967  cls_loss=0.7366  reg_loss=0.3602  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2636s  iter_time=2.387s
2025-08-06 04:19:11 Train INFO: [Train]: [025][00003/00051] (7.7%)  Loss=1.0730  cls_loss=0.7221  reg_loss=0.3509  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1964s  iter_time=2.237s
2025-08-06 04:21:26 Train INFO: [Train]: [025][00004/00051] (9.6%)  Loss=1.0287  cls_loss=0.6886  reg_loss=0.3400  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2807s  iter_time=134.931s
2025-08-06 04:21:29 Train INFO: [Train]: [025][00005/00051] (11.5%)  Loss=1.0392  cls_loss=0.6942  reg_loss=0.3450  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2313s  iter_time=3.166s
2025-08-06 04:21:31 Train INFO: [Train]: [025][00006/00051] (13.5%)  Loss=1.0595  cls_loss=0.7085  reg_loss=0.3510  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1955s  iter_time=2.330s
2025-08-06 04:21:33 Train INFO: [Train]: [025][00007/00051] (15.4%)  Loss=1.0700  cls_loss=0.7143  reg_loss=0.3557  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1685s  iter_time=2.250s
2025-08-06 04:23:42 Train INFO: [Train]: [025][00008/00051] (17.3%)  Loss=1.0579  cls_loss=0.7059  reg_loss=0.3520  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2078s  iter_time=128.633s
2025-08-06 04:23:53 Train INFO: [Train]: [025][00009/00051] (19.2%)  Loss=1.0559  cls_loss=0.7045  reg_loss=0.3515  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1875s  iter_time=11.385s
2025-08-06 04:23:56 Train INFO: [Train]: [025][00010/00051] (21.2%)  Loss=1.0518  cls_loss=0.7005  reg_loss=0.3513  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1672s  iter_time=2.341s
2025-08-06 04:23:58 Train INFO: [Train]: [025][00011/00051] (23.1%)  Loss=1.0584  cls_loss=0.7032  reg_loss=0.3552  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1503s  iter_time=2.336s
2025-08-06 04:26:02 Train INFO: [Train]: [025][00012/00051] (25.0%)  Loss=1.0629  cls_loss=0.7062  reg_loss=0.3567  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1724s  iter_time=123.699s
2025-08-06 04:26:17 Train INFO: [Train]: [025][00013/00051] (26.9%)  Loss=1.0675  cls_loss=0.7102  reg_loss=0.3573  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1602s  iter_time=15.349s
2025-08-06 04:26:19 Train INFO: [Train]: [025][00014/00051] (28.8%)  Loss=1.0666  cls_loss=0.7097  reg_loss=0.3570  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1461s  iter_time=2.295s
2025-08-06 04:26:22 Train INFO: [Train]: [025][00015/00051] (30.8%)  Loss=1.0646  cls_loss=0.7076  reg_loss=0.3570  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1338s  iter_time=2.275s
2025-08-06 04:28:23 Train INFO: [Train]: [025][00016/00051] (32.7%)  Loss=1.0504  cls_loss=0.6982  reg_loss=0.3522  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1475s  iter_time=121.574s
2025-08-06 04:28:32 Train INFO: [Train]: [025][00017/00051] (34.6%)  Loss=1.0499  cls_loss=0.6968  reg_loss=0.3531  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1370s  iter_time=8.824s
2025-08-06 04:28:34 Train INFO: [Train]: [025][00018/00051] (36.5%)  Loss=1.0469  cls_loss=0.6948  reg_loss=0.3522  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1263s  iter_time=2.304s
2025-08-06 04:28:37 Train INFO: [Train]: [025][00019/00051] (38.5%)  Loss=1.0471  cls_loss=0.6942  reg_loss=0.3529  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1167s  iter_time=2.258s
2025-08-06 04:30:44 Train INFO: [Train]: [025][00020/00051] (40.4%)  Loss=1.0402  cls_loss=0.6888  reg_loss=0.3514  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1265s  iter_time=127.220s
2025-08-06 04:30:49 Train INFO: [Train]: [025][00021/00051] (42.3%)  Loss=1.0470  cls_loss=0.6935  reg_loss=0.3535  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1176s  iter_time=5.292s
2025-08-06 04:30:51 Train INFO: [Train]: [025][00022/00051] (44.2%)  Loss=1.0413  cls_loss=0.6909  reg_loss=0.3504  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1090s  iter_time=2.330s
2025-08-06 04:30:54 Train INFO: [Train]: [025][00023/00051] (46.2%)  Loss=1.0326  cls_loss=0.6849  reg_loss=0.3476  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1011s  iter_time=2.320s
2025-08-06 04:33:06 Train INFO: [Train]: [025][00024/00051] (48.1%)  Loss=1.0302  cls_loss=0.6828  reg_loss=0.3474  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1079s  iter_time=132.405s
2025-08-06 04:33:16 Train INFO: [Train]: [025][00025/00051] (50.0%)  Loss=1.0311  cls_loss=0.6830  reg_loss=0.3480  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1009s  iter_time=10.190s
2025-08-06 04:33:19 Train INFO: [Train]: [025][00026/00051] (51.9%)  Loss=1.0379  cls_loss=0.6875  reg_loss=0.3504  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=937s  iter_time=2.336s
2025-08-06 04:33:21 Train INFO: [Train]: [025][00027/00051] (53.8%)  Loss=1.0461  cls_loss=0.6921  reg_loss=0.3539  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=869s  iter_time=2.318s
2025-08-06 04:35:23 Train INFO: [Train]: [025][00028/00051] (55.8%)  Loss=1.0462  cls_loss=0.6919  reg_loss=0.3543  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=901s  iter_time=121.951s
2025-08-06 04:35:40 Train INFO: [Train]: [025][00029/00051] (57.7%)  Loss=1.0465  cls_loss=0.6923  reg_loss=0.3542  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=845s  iter_time=16.965s
2025-08-06 04:35:42 Train INFO: [Train]: [025][00030/00051] (59.6%)  Loss=1.0420  cls_loss=0.6902  reg_loss=0.3518  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=783s  iter_time=2.356s
2025-08-06 04:35:45 Train INFO: [Train]: [025][00031/00051] (61.5%)  Loss=1.0379  cls_loss=0.6874  reg_loss=0.3505  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=724s  iter_time=2.320s
2025-08-06 04:37:45 Train INFO: [Train]: [025][00032/00051] (63.5%)  Loss=1.0409  cls_loss=0.6898  reg_loss=0.3511  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=736s  iter_time=119.891s
2025-08-06 04:37:59 Train INFO: [Train]: [025][00033/00051] (65.4%)  Loss=1.0367  cls_loss=0.6866  reg_loss=0.3502  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=684s  iter_time=14.116s
2025-08-06 04:38:01 Train INFO: [Train]: [025][00034/00051] (67.3%)  Loss=1.0413  cls_loss=0.6893  reg_loss=0.3521  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=628s  iter_time=2.264s
2025-08-06 04:38:03 Train INFO: [Train]: [025][00035/00051] (69.2%)  Loss=1.0409  cls_loss=0.6887  reg_loss=0.3522  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=2.340s
2025-08-06 04:40:10 Train INFO: [Train]: [025][00036/00051] (71.2%)  Loss=1.0385  cls_loss=0.6871  reg_loss=0.3513  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=577s  iter_time=126.952s
2025-08-06 04:40:21 Train INFO: [Train]: [025][00037/00051] (73.1%)  Loss=1.0379  cls_loss=0.6864  reg_loss=0.3516  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=528s  iter_time=10.442s
2025-08-06 04:40:23 Train INFO: [Train]: [025][00038/00051] (75.0%)  Loss=1.0387  cls_loss=0.6867  reg_loss=0.3521  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=479s  iter_time=2.339s
2025-08-06 04:40:25 Train INFO: [Train]: [025][00039/00051] (76.9%)  Loss=1.0384  cls_loss=0.6861  reg_loss=0.3524  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=431s  iter_time=2.198s
2025-08-06 04:42:30 Train INFO: [Train]: [025][00040/00051] (78.8%)  Loss=1.0424  cls_loss=0.6884  reg_loss=0.3540  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=419s  iter_time=124.813s
2025-08-06 04:42:39 Train INFO: [Train]: [025][00041/00051] (80.8%)  Loss=1.0409  cls_loss=0.6874  reg_loss=0.3535  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=374s  iter_time=8.603s
2025-08-06 04:42:41 Train INFO: [Train]: [025][00042/00051] (82.7%)  Loss=1.0435  cls_loss=0.6891  reg_loss=0.3544  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=329s  iter_time=2.350s
2025-08-06 04:42:43 Train INFO: [Train]: [025][00043/00051] (84.6%)  Loss=1.0420  cls_loss=0.6881  reg_loss=0.3539  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=287s  iter_time=2.279s
2025-08-06 04:44:55 Train INFO: [Train]: [025][00044/00051] (86.5%)  Loss=1.0397  cls_loss=0.6860  reg_loss=0.3537  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=131.872s
2025-08-06 04:44:58 Train INFO: [Train]: [025][00045/00051] (88.5%)  Loss=1.0364  cls_loss=0.6838  reg_loss=0.3525  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.968s
2025-08-06 04:45:02 Train INFO: [Train]: [025][00046/00051] (90.4%)  Loss=1.0364  cls_loss=0.6836  reg_loss=0.3529  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=4.383s
2025-08-06 04:45:05 Train INFO: [Train]: [025][00047/00051] (92.3%)  Loss=1.0358  cls_loss=0.6828  reg_loss=0.3530  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.336s
2025-08-06 04:47:12 Train INFO: [Train]: [025][00048/00051] (94.2%)  Loss=1.0346  cls_loss=0.6823  reg_loss=0.3523  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=126.821s
2025-08-06 04:47:14 Train INFO: [Train]: [025][00049/00051] (96.2%)  Loss=1.0353  cls_loss=0.6828  reg_loss=0.3525  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.355s
2025-08-06 04:47:16 Train INFO: [Train]: [025][00050/00051] (98.1%)  Loss=1.0351  cls_loss=0.6828  reg_loss=0.3523  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.202s
2025-08-06 04:47:18 Train INFO: [Train]: [025][00051/00051] (100.0%)  Loss=1.0319  cls_loss=0.6809  reg_loss=0.3510  lr_det=5.2e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.049s
2025-08-06 04:47:19 Train INFO: [Train]: Epoch 25 completed in 1851.9s (avg 35.614s/iter)
2025-08-06 04:47:19 Train INFO: [Train]: Final Loss=1.0319
2025-08-06 04:47:19 Train INFO: [Train]: Epoch 26 started (Total iterations: 52)
2025-08-06 04:49:59 Train INFO: [Train]: [026][00001/00051] (3.8%)  Loss=0.9110  cls_loss=0.5915  reg_loss=0.3195  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4012s  iter_time=160.468s  fwd=2.129s/bwd=0.078s/opt=0.026s
2025-08-06 04:50:02 Train INFO: [Train]: [026][00002/00051] (5.8%)  Loss=1.0081  cls_loss=0.6561  reg_loss=0.3519  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2659s  iter_time=2.328s
2025-08-06 04:50:04 Train INFO: [Train]: [026][00003/00051] (7.7%)  Loss=1.0658  cls_loss=0.6943  reg_loss=0.3715  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1981s  iter_time=2.259s
2025-08-06 04:52:20 Train INFO: [Train]: [026][00004/00051] (9.6%)  Loss=1.0307  cls_loss=0.6709  reg_loss=0.3598  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2826s  iter_time=135.540s
2025-08-06 04:52:24 Train INFO: [Train]: [026][00005/00051] (11.5%)  Loss=1.0164  cls_loss=0.6633  reg_loss=0.3531  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2338s  iter_time=4.341s
2025-08-06 04:52:26 Train INFO: [Train]: [026][00006/00051] (13.5%)  Loss=1.0247  cls_loss=0.6706  reg_loss=0.3541  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1975s  iter_time=2.286s
2025-08-06 04:52:28 Train INFO: [Train]: [026][00007/00051] (15.4%)  Loss=1.0447  cls_loss=0.6868  reg_loss=0.3579  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1702s  iter_time=2.277s
2025-08-06 04:54:34 Train INFO: [Train]: [026][00008/00051] (17.3%)  Loss=1.0257  cls_loss=0.6737  reg_loss=0.3519  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2078s  iter_time=125.513s
2025-08-06 04:54:44 Train INFO: [Train]: [026][00009/00051] (19.2%)  Loss=1.0167  cls_loss=0.6709  reg_loss=0.3458  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1868s  iter_time=9.744s
2025-08-06 04:54:46 Train INFO: [Train]: [026][00010/00051] (21.2%)  Loss=1.0159  cls_loss=0.6723  reg_loss=0.3436  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1666s  iter_time=2.298s
2025-08-06 04:54:48 Train INFO: [Train]: [026][00011/00051] (23.1%)  Loss=1.0205  cls_loss=0.6748  reg_loss=0.3457  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1498s  iter_time=2.275s
2025-08-06 04:56:58 Train INFO: [Train]: [026][00012/00051] (25.0%)  Loss=1.0230  cls_loss=0.6773  reg_loss=0.3457  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1737s  iter_time=129.682s
2025-08-06 04:57:08 Train INFO: [Train]: [026][00013/00051] (26.9%)  Loss=1.0301  cls_loss=0.6831  reg_loss=0.3470  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1598s  iter_time=9.710s
2025-08-06 04:57:10 Train INFO: [Train]: [026][00014/00051] (28.8%)  Loss=1.0335  cls_loss=0.6852  reg_loss=0.3484  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1458s  iter_time=2.342s
2025-08-06 04:57:12 Train INFO: [Train]: [026][00015/00051] (30.8%)  Loss=1.0282  cls_loss=0.6829  reg_loss=0.3453  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1335s  iter_time=2.312s
2025-08-06 04:59:20 Train INFO: [Train]: [026][00016/00051] (32.7%)  Loss=1.0308  cls_loss=0.6856  reg_loss=0.3452  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1485s  iter_time=127.876s
2025-08-06 04:59:25 Train INFO: [Train]: [026][00017/00051] (34.6%)  Loss=1.0250  cls_loss=0.6807  reg_loss=0.3443  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1372s  iter_time=4.885s
2025-08-06 04:59:27 Train INFO: [Train]: [026][00018/00051] (36.5%)  Loss=1.0213  cls_loss=0.6779  reg_loss=0.3435  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1265s  iter_time=2.341s
2025-08-06 04:59:30 Train INFO: [Train]: [026][00019/00051] (38.5%)  Loss=1.0344  cls_loss=0.6863  reg_loss=0.3481  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1169s  iter_time=2.293s
2025-08-06 05:01:36 Train INFO: [Train]: [026][00020/00051] (40.4%)  Loss=1.0273  cls_loss=0.6817  reg_loss=0.3455  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1266s  iter_time=126.579s
2025-08-06 05:01:42 Train INFO: [Train]: [026][00021/00051] (42.3%)  Loss=1.0338  cls_loss=0.6846  reg_loss=0.3492  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1177s  iter_time=5.935s
2025-08-06 05:01:45 Train INFO: [Train]: [026][00022/00051] (44.2%)  Loss=1.0384  cls_loss=0.6856  reg_loss=0.3528  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1091s  iter_time=2.290s
2025-08-06 05:01:47 Train INFO: [Train]: [026][00023/00051] (46.2%)  Loss=1.0291  cls_loss=0.6796  reg_loss=0.3495  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1013s  iter_time=2.290s
2025-08-06 05:03:55 Train INFO: [Train]: [026][00024/00051] (48.1%)  Loss=1.0333  cls_loss=0.6824  reg_loss=0.3509  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1075s  iter_time=127.678s
2025-08-06 05:04:05 Train INFO: [Train]: [026][00025/00051] (50.0%)  Loss=1.0287  cls_loss=0.6792  reg_loss=0.3495  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1006s  iter_time=9.999s
2025-08-06 05:04:07 Train INFO: [Train]: [026][00026/00051] (51.9%)  Loss=1.0283  cls_loss=0.6792  reg_loss=0.3491  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=933s  iter_time=2.323s
2025-08-06 05:04:09 Train INFO: [Train]: [026][00027/00051] (53.8%)  Loss=1.0283  cls_loss=0.6783  reg_loss=0.3500  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=866s  iter_time=2.347s
2025-08-06 05:06:16 Train INFO: [Train]: [026][00028/00051] (55.8%)  Loss=1.0263  cls_loss=0.6761  reg_loss=0.3502  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=902s  iter_time=126.536s
2025-08-06 05:06:30 Train INFO: [Train]: [026][00029/00051] (57.7%)  Loss=1.0233  cls_loss=0.6743  reg_loss=0.3490  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=844s  iter_time=14.124s
2025-08-06 05:06:32 Train INFO: [Train]: [026][00030/00051] (59.6%)  Loss=1.0220  cls_loss=0.6737  reg_loss=0.3484  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=781s  iter_time=2.325s
2025-08-06 05:06:35 Train INFO: [Train]: [026][00031/00051] (61.5%)  Loss=1.0202  cls_loss=0.6721  reg_loss=0.3480  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=722s  iter_time=2.312s
2025-08-06 05:08:36 Train INFO: [Train]: [026][00032/00051] (63.5%)  Loss=1.0259  cls_loss=0.6759  reg_loss=0.3500  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=735s  iter_time=121.160s
2025-08-06 05:08:49 Train INFO: [Train]: [026][00033/00051] (65.4%)  Loss=1.0212  cls_loss=0.6730  reg_loss=0.3481  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=683s  iter_time=13.018s
2025-08-06 05:08:51 Train INFO: [Train]: [026][00034/00051] (67.3%)  Loss=1.0258  cls_loss=0.6761  reg_loss=0.3497  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=628s  iter_time=2.259s
2025-08-06 05:08:53 Train INFO: [Train]: [026][00035/00051] (69.2%)  Loss=1.0226  cls_loss=0.6739  reg_loss=0.3486  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=575s  iter_time=2.306s
2025-08-06 05:11:00 Train INFO: [Train]: [026][00036/00051] (71.2%)  Loss=1.0251  cls_loss=0.6760  reg_loss=0.3491  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=126.564s
2025-08-06 05:11:11 Train INFO: [Train]: [026][00037/00051] (73.1%)  Loss=1.0244  cls_loss=0.6752  reg_loss=0.3492  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=527s  iter_time=10.924s
2025-08-06 05:11:13 Train INFO: [Train]: [026][00038/00051] (75.0%)  Loss=1.0251  cls_loss=0.6757  reg_loss=0.3494  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=478s  iter_time=2.361s
2025-08-06 05:11:15 Train INFO: [Train]: [026][00039/00051] (76.9%)  Loss=1.0269  cls_loss=0.6763  reg_loss=0.3506  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=431s  iter_time=2.290s
2025-08-06 05:13:17 Train INFO: [Train]: [026][00040/00051] (78.8%)  Loss=1.0259  cls_loss=0.6759  reg_loss=0.3501  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=418s  iter_time=121.146s
2025-08-06 05:13:34 Train INFO: [Train]: [026][00041/00051] (80.8%)  Loss=1.0278  cls_loss=0.6772  reg_loss=0.3506  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=375s  iter_time=17.618s
2025-08-06 05:13:37 Train INFO: [Train]: [026][00042/00051] (82.7%)  Loss=1.0287  cls_loss=0.6779  reg_loss=0.3508  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=330s  iter_time=2.306s
2025-08-06 05:13:39 Train INFO: [Train]: [026][00043/00051] (84.6%)  Loss=1.0297  cls_loss=0.6780  reg_loss=0.3517  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=287s  iter_time=2.340s
2025-08-06 05:15:41 Train INFO: [Train]: [026][00044/00051] (86.5%)  Loss=1.0297  cls_loss=0.6782  reg_loss=0.3515  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=265s  iter_time=121.831s
2025-08-06 05:15:53 Train INFO: [Train]: [026][00045/00051] (88.5%)  Loss=1.0291  cls_loss=0.6779  reg_loss=0.3512  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=12.284s
2025-08-06 05:15:55 Train INFO: [Train]: [026][00046/00051] (90.4%)  Loss=1.0252  cls_loss=0.6754  reg_loss=0.3498  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=183s  iter_time=2.248s
2025-08-06 05:15:57 Train INFO: [Train]: [026][00047/00051] (92.3%)  Loss=1.0271  cls_loss=0.6771  reg_loss=0.3500  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.252s
2025-08-06 05:17:57 Train INFO: [Train]: [026][00048/00051] (94.2%)  Loss=1.0256  cls_loss=0.6763  reg_loss=0.3493  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=119.724s
2025-08-06 05:18:07 Train INFO: [Train]: [026][00049/00051] (96.2%)  Loss=1.0245  cls_loss=0.6759  reg_loss=0.3486  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=9.739s
2025-08-06 05:18:09 Train INFO: [Train]: [026][00050/00051] (98.1%)  Loss=1.0284  cls_loss=0.6782  reg_loss=0.3502  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.162s
2025-08-06 05:18:11 Train INFO: [Train]: [026][00051/00051] (100.0%)  Loss=1.0271  cls_loss=0.6772  reg_loss=0.3499  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.032s
2025-08-06 05:18:12 Train INFO: [Train]: Epoch 26 completed in 1852.8s (avg 35.631s/iter)
2025-08-06 05:18:12 Train INFO: [Train]: Final Loss=1.0271
2025-08-06 05:18:12 Train INFO: [Train]: Epoch 27 started (Total iterations: 52)
2025-08-06 05:20:55 Train INFO: [Train]: [027][00001/00051] (3.8%)  Loss=0.9835  cls_loss=0.6382  reg_loss=0.3453  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4083s  iter_time=163.310s  fwd=2.128s/bwd=0.081s/opt=0.018s
2025-08-06 05:20:57 Train INFO: [Train]: [027][00002/00051] (5.8%)  Loss=1.0402  cls_loss=0.6732  reg_loss=0.3670  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2706s  iter_time=2.339s
2025-08-06 05:21:00 Train INFO: [Train]: [027][00003/00051] (7.7%)  Loss=1.0175  cls_loss=0.6656  reg_loss=0.3518  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2015s  iter_time=2.277s
2025-08-06 05:23:14 Train INFO: [Train]: [027][00004/00051] (9.6%)  Loss=1.0003  cls_loss=0.6573  reg_loss=0.3430  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2842s  iter_time=134.392s
2025-08-06 05:23:16 Train INFO: [Train]: [027][00005/00051] (11.5%)  Loss=0.9966  cls_loss=0.6556  reg_loss=0.3410  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2336s  iter_time=2.330s
2025-08-06 05:23:19 Train INFO: [Train]: [027][00006/00051] (13.5%)  Loss=1.0050  cls_loss=0.6621  reg_loss=0.3429  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1974s  iter_time=2.354s
2025-08-06 05:23:21 Train INFO: [Train]: [027][00007/00051] (15.4%)  Loss=1.0010  cls_loss=0.6600  reg_loss=0.3411  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1701s  iter_time=2.239s
2025-08-06 05:25:33 Train INFO: [Train]: [027][00008/00051] (17.3%)  Loss=0.9912  cls_loss=0.6541  reg_loss=0.3372  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2107s  iter_time=131.799s
2025-08-06 05:25:36 Train INFO: [Train]: [027][00009/00051] (19.2%)  Loss=0.9827  cls_loss=0.6503  reg_loss=0.3324  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1866s  iter_time=3.228s
2025-08-06 05:25:38 Train INFO: [Train]: [027][00010/00051] (21.2%)  Loss=0.9772  cls_loss=0.6478  reg_loss=0.3294  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1664s  iter_time=2.285s
2025-08-06 05:25:41 Train INFO: [Train]: [027][00011/00051] (23.1%)  Loss=1.0068  cls_loss=0.6681  reg_loss=0.3387  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1496s  iter_time=2.257s
2025-08-06 05:27:55 Train INFO: [Train]: [027][00012/00051] (25.0%)  Loss=1.0134  cls_loss=0.6719  reg_loss=0.3415  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1750s  iter_time=134.450s
2025-08-06 05:28:02 Train INFO: [Train]: [027][00013/00051] (26.9%)  Loss=1.0256  cls_loss=0.6810  reg_loss=0.3446  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1603s  iter_time=7.134s
2025-08-06 05:28:05 Train INFO: [Train]: [027][00014/00051] (28.8%)  Loss=1.0412  cls_loss=0.6907  reg_loss=0.3506  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1462s  iter_time=2.318s
2025-08-06 05:28:07 Train INFO: [Train]: [027][00015/00051] (30.8%)  Loss=1.0362  cls_loss=0.6866  reg_loss=0.3496  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1339s  iter_time=2.370s
2025-08-06 05:30:15 Train INFO: [Train]: [027][00016/00051] (32.7%)  Loss=1.0392  cls_loss=0.6876  reg_loss=0.3516  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1489s  iter_time=128.033s
2025-08-06 05:30:20 Train INFO: [Train]: [027][00017/00051] (34.6%)  Loss=1.0307  cls_loss=0.6818  reg_loss=0.3488  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1376s  iter_time=5.150s
2025-08-06 05:30:22 Train INFO: [Train]: [027][00018/00051] (36.5%)  Loss=1.0224  cls_loss=0.6758  reg_loss=0.3465  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1269s  iter_time=2.346s
2025-08-06 05:30:25 Train INFO: [Train]: [027][00019/00051] (38.5%)  Loss=1.0242  cls_loss=0.6769  reg_loss=0.3472  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1173s  iter_time=2.320s
2025-08-06 05:32:32 Train INFO: [Train]: [027][00020/00051] (40.4%)  Loss=1.0201  cls_loss=0.6743  reg_loss=0.3458  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1270s  iter_time=127.514s
2025-08-06 05:32:37 Train INFO: [Train]: [027][00021/00051] (42.3%)  Loss=1.0217  cls_loss=0.6755  reg_loss=0.3462  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1180s  iter_time=4.743s
2025-08-06 05:32:39 Train INFO: [Train]: [027][00022/00051] (44.2%)  Loss=1.0309  cls_loss=0.6809  reg_loss=0.3500  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1094s  iter_time=2.293s
2025-08-06 05:32:42 Train INFO: [Train]: [027][00023/00051] (46.2%)  Loss=1.0231  cls_loss=0.6757  reg_loss=0.3474  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1015s  iter_time=2.336s
2025-08-06 05:34:53 Train INFO: [Train]: [027][00024/00051] (48.1%)  Loss=1.0334  cls_loss=0.6827  reg_loss=0.3507  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1082s  iter_time=131.817s
2025-08-06 05:34:59 Train INFO: [Train]: [027][00025/00051] (50.0%)  Loss=1.0337  cls_loss=0.6820  reg_loss=0.3517  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1007s  iter_time=5.512s
2025-08-06 05:35:03 Train INFO: [Train]: [027][00026/00051] (51.9%)  Loss=1.0332  cls_loss=0.6815  reg_loss=0.3517  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=936s  iter_time=3.580s
2025-08-06 05:35:05 Train INFO: [Train]: [027][00027/00051] (53.8%)  Loss=1.0323  cls_loss=0.6810  reg_loss=0.3513  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=868s  iter_time=2.278s
2025-08-06 05:37:11 Train INFO: [Train]: [027][00028/00051] (55.8%)  Loss=1.0269  cls_loss=0.6772  reg_loss=0.3496  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=904s  iter_time=126.221s
2025-08-06 05:37:19 Train INFO: [Train]: [027][00029/00051] (57.7%)  Loss=1.0319  cls_loss=0.6809  reg_loss=0.3510  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=841s  iter_time=7.590s
2025-08-06 05:37:21 Train INFO: [Train]: [027][00030/00051] (59.6%)  Loss=1.0305  cls_loss=0.6803  reg_loss=0.3502  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=779s  iter_time=2.632s
2025-08-06 05:37:24 Train INFO: [Train]: [027][00031/00051] (61.5%)  Loss=1.0272  cls_loss=0.6779  reg_loss=0.3493  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=720s  iter_time=2.326s
2025-08-06 05:39:32 Train INFO: [Train]: [027][00032/00051] (63.5%)  Loss=1.0247  cls_loss=0.6767  reg_loss=0.3480  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=128.600s
2025-08-06 05:39:37 Train INFO: [Train]: [027][00033/00051] (65.4%)  Loss=1.0240  cls_loss=0.6768  reg_loss=0.3472  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=681s  iter_time=4.992s
2025-08-06 05:39:41 Train INFO: [Train]: [027][00034/00051] (67.3%)  Loss=1.0292  cls_loss=0.6798  reg_loss=0.3494  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=626s  iter_time=3.745s
2025-08-06 05:39:43 Train INFO: [Train]: [027][00035/00051] (69.2%)  Loss=1.0263  cls_loss=0.6779  reg_loss=0.3484  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=2.321s
2025-08-06 05:41:54 Train INFO: [Train]: [027][00036/00051] (71.2%)  Loss=1.0281  cls_loss=0.6788  reg_loss=0.3493  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=130.430s
2025-08-06 05:41:56 Train INFO: [Train]: [027][00037/00051] (73.1%)  Loss=1.0247  cls_loss=0.6760  reg_loss=0.3487  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=525s  iter_time=2.406s
2025-08-06 05:42:01 Train INFO: [Train]: [027][00038/00051] (75.0%)  Loss=1.0279  cls_loss=0.6776  reg_loss=0.3503  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=476s  iter_time=4.978s
2025-08-06 05:42:03 Train INFO: [Train]: [027][00039/00051] (76.9%)  Loss=1.0253  cls_loss=0.6761  reg_loss=0.3491  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=429s  iter_time=2.312s
2025-08-06 05:44:13 Train INFO: [Train]: [027][00040/00051] (78.8%)  Loss=1.0264  cls_loss=0.6763  reg_loss=0.3502  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=419s  iter_time=129.092s
2025-08-06 05:44:15 Train INFO: [Train]: [027][00041/00051] (80.8%)  Loss=1.0229  cls_loss=0.6741  reg_loss=0.3488  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=372s  iter_time=2.278s
2025-08-06 05:44:19 Train INFO: [Train]: [027][00042/00051] (82.7%)  Loss=1.0251  cls_loss=0.6755  reg_loss=0.3496  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=4.590s
2025-08-06 05:44:22 Train INFO: [Train]: [027][00043/00051] (84.6%)  Loss=1.0257  cls_loss=0.6756  reg_loss=0.3500  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=285s  iter_time=2.313s
2025-08-06 05:46:36 Train INFO: [Train]: [027][00044/00051] (86.5%)  Loss=1.0249  cls_loss=0.6755  reg_loss=0.3494  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=265s  iter_time=134.377s
2025-08-06 05:46:38 Train INFO: [Train]: [027][00045/00051] (88.5%)  Loss=1.0213  cls_loss=0.6726  reg_loss=0.3487  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.305s
2025-08-06 05:46:43 Train INFO: [Train]: [027][00046/00051] (90.4%)  Loss=1.0197  cls_loss=0.6713  reg_loss=0.3485  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=4.308s
2025-08-06 05:46:45 Train INFO: [Train]: [027][00047/00051] (92.3%)  Loss=1.0199  cls_loss=0.6711  reg_loss=0.3488  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.298s
2025-08-06 05:48:53 Train INFO: [Train]: [027][00048/00051] (94.2%)  Loss=1.0175  cls_loss=0.6699  reg_loss=0.3476  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=127.886s
2025-08-06 05:48:55 Train INFO: [Train]: [027][00049/00051] (96.2%)  Loss=1.0176  cls_loss=0.6704  reg_loss=0.3472  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.170s
2025-08-06 05:48:57 Train INFO: [Train]: [027][00050/00051] (98.1%)  Loss=1.0180  cls_loss=0.6702  reg_loss=0.3478  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.435s
2025-08-06 05:49:00 Train INFO: [Train]: [027][00051/00051] (100.0%)  Loss=1.0185  cls_loss=0.6703  reg_loss=0.3482  lr_det=4.8e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.039s
2025-08-06 05:49:00 Train INFO: [Train]: Epoch 27 completed in 1848.4s (avg 35.546s/iter)
2025-08-06 05:49:00 Train INFO: [Train]: Final Loss=1.0185
2025-08-06 05:49:00 Train INFO: [Train]: Epoch 28 started (Total iterations: 52)
2025-08-06 05:51:43 Train INFO: [Train]: [028][00001/00051] (3.8%)  Loss=1.0353  cls_loss=0.6866  reg_loss=0.3487  lr_det=5.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4081s  iter_time=163.226s  fwd=2.113s/bwd=0.069s/opt=0.024s
2025-08-06 05:51:46 Train INFO: [Train]: [028][00002/00051] (5.8%)  Loss=1.0928  cls_loss=0.7205  reg_loss=0.3723  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2704s  iter_time=2.342s
2025-08-06 05:51:48 Train INFO: [Train]: [028][00003/00051] (7.7%)  Loss=1.1069  cls_loss=0.7277  reg_loss=0.3793  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2013s  iter_time=2.218s
2025-08-06 05:54:06 Train INFO: [Train]: [028][00004/00051] (9.6%)  Loss=1.0873  cls_loss=0.7158  reg_loss=0.3715  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2870s  iter_time=137.491s
2025-08-06 05:54:08 Train INFO: [Train]: [028][00005/00051] (11.5%)  Loss=1.0823  cls_loss=0.7153  reg_loss=0.3670  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2358s  iter_time=2.330s
2025-08-06 05:54:10 Train INFO: [Train]: [028][00006/00051] (13.5%)  Loss=1.0817  cls_loss=0.7140  reg_loss=0.3677  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1993s  iter_time=2.369s
2025-08-06 05:54:12 Train INFO: [Train]: [028][00007/00051] (15.4%)  Loss=1.0958  cls_loss=0.7237  reg_loss=0.3721  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1717s  iter_time=2.253s
2025-08-06 05:56:20 Train INFO: [Train]: [028][00008/00051] (17.3%)  Loss=1.0813  cls_loss=0.7134  reg_loss=0.3679  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2102s  iter_time=127.789s
2025-08-06 05:56:28 Train INFO: [Train]: [028][00009/00051] (19.2%)  Loss=1.0820  cls_loss=0.7147  reg_loss=0.3674  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1882s  iter_time=7.953s
2025-08-06 05:56:31 Train INFO: [Train]: [028][00010/00051] (21.2%)  Loss=1.0646  cls_loss=0.7032  reg_loss=0.3614  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1678s  iter_time=2.311s
2025-08-06 05:56:33 Train INFO: [Train]: [028][00011/00051] (23.1%)  Loss=1.0655  cls_loss=0.7034  reg_loss=0.3620  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1509s  iter_time=2.324s
2025-08-06 05:58:45 Train INFO: [Train]: [028][00012/00051] (25.0%)  Loss=1.0494  cls_loss=0.6956  reg_loss=0.3538  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1754s  iter_time=131.990s
2025-08-06 05:58:54 Train INFO: [Train]: [028][00013/00051] (26.9%)  Loss=1.0542  cls_loss=0.6992  reg_loss=0.3550  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1611s  iter_time=8.971s
2025-08-06 05:58:56 Train INFO: [Train]: [028][00014/00051] (28.8%)  Loss=1.0636  cls_loss=0.7057  reg_loss=0.3579  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1470s  iter_time=2.334s
2025-08-06 05:58:58 Train INFO: [Train]: [028][00015/00051] (30.8%)  Loss=1.0519  cls_loss=0.6982  reg_loss=0.3538  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1346s  iter_time=2.342s
2025-08-06 06:01:07 Train INFO: [Train]: [028][00016/00051] (32.7%)  Loss=1.0419  cls_loss=0.6935  reg_loss=0.3483  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1495s  iter_time=128.046s
2025-08-06 06:01:09 Train INFO: [Train]: [028][00017/00051] (34.6%)  Loss=1.0401  cls_loss=0.6921  reg_loss=0.3480  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1376s  iter_time=2.335s
2025-08-06 06:01:11 Train INFO: [Train]: [028][00018/00051] (36.5%)  Loss=1.0358  cls_loss=0.6893  reg_loss=0.3465  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1269s  iter_time=2.266s
2025-08-06 06:01:13 Train INFO: [Train]: [028][00019/00051] (38.5%)  Loss=1.0442  cls_loss=0.6946  reg_loss=0.3495  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1173s  iter_time=2.322s
2025-08-06 06:03:27 Train INFO: [Train]: [028][00020/00051] (40.4%)  Loss=1.0284  cls_loss=0.6840  reg_loss=0.3444  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1279s  iter_time=133.266s
2025-08-06 06:03:29 Train INFO: [Train]: [028][00021/00051] (42.3%)  Loss=1.0400  cls_loss=0.6917  reg_loss=0.3483  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1185s  iter_time=2.318s
2025-08-06 06:03:35 Train INFO: [Train]: [028][00022/00051] (44.2%)  Loss=1.0530  cls_loss=0.6992  reg_loss=0.3538  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1103s  iter_time=5.983s
2025-08-06 06:03:37 Train INFO: [Train]: [028][00023/00051] (46.2%)  Loss=1.0498  cls_loss=0.6955  reg_loss=0.3542  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1023s  iter_time=2.292s
2025-08-06 06:05:52 Train INFO: [Train]: [028][00024/00051] (48.1%)  Loss=1.0516  cls_loss=0.6959  reg_loss=0.3557  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1092s  iter_time=134.287s
2025-08-06 06:05:54 Train INFO: [Train]: [028][00025/00051] (50.0%)  Loss=1.0467  cls_loss=0.6910  reg_loss=0.3557  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1014s  iter_time=2.287s
2025-08-06 06:06:02 Train INFO: [Train]: [028][00026/00051] (51.9%)  Loss=1.0515  cls_loss=0.6940  reg_loss=0.3575  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=946s  iter_time=8.166s
2025-08-06 06:06:04 Train INFO: [Train]: [028][00027/00051] (53.8%)  Loss=1.0493  cls_loss=0.6924  reg_loss=0.3569  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=878s  iter_time=2.362s
2025-08-06 06:08:08 Train INFO: [Train]: [028][00028/00051] (55.8%)  Loss=1.0542  cls_loss=0.6954  reg_loss=0.3588  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=910s  iter_time=123.393s
2025-08-06 06:08:15 Train INFO: [Train]: [028][00029/00051] (57.7%)  Loss=1.0578  cls_loss=0.6975  reg_loss=0.3603  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=847s  iter_time=7.360s
2025-08-06 06:08:18 Train INFO: [Train]: [028][00030/00051] (59.6%)  Loss=1.0523  cls_loss=0.6936  reg_loss=0.3588  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=785s  iter_time=3.190s
2025-08-06 06:08:21 Train INFO: [Train]: [028][00031/00051] (61.5%)  Loss=1.0509  cls_loss=0.6930  reg_loss=0.3580  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=725s  iter_time=2.308s
2025-08-06 06:10:29 Train INFO: [Train]: [028][00032/00051] (63.5%)  Loss=1.0495  cls_loss=0.6919  reg_loss=0.3577  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=742s  iter_time=128.653s
2025-08-06 06:10:32 Train INFO: [Train]: [028][00033/00051] (65.4%)  Loss=1.0470  cls_loss=0.6897  reg_loss=0.3573  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=684s  iter_time=2.363s
2025-08-06 06:10:37 Train INFO: [Train]: [028][00034/00051] (67.3%)  Loss=1.0485  cls_loss=0.6908  reg_loss=0.3578  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=630s  iter_time=5.578s
2025-08-06 06:10:40 Train INFO: [Train]: [028][00035/00051] (69.2%)  Loss=1.0467  cls_loss=0.6891  reg_loss=0.3577  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=577s  iter_time=2.299s
2025-08-06 06:12:54 Train INFO: [Train]: [028][00036/00051] (71.2%)  Loss=1.0470  cls_loss=0.6894  reg_loss=0.3576  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=581s  iter_time=134.305s
2025-08-06 06:12:57 Train INFO: [Train]: [028][00037/00051] (73.1%)  Loss=1.0447  cls_loss=0.6870  reg_loss=0.3577  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=529s  iter_time=2.796s
2025-08-06 06:13:04 Train INFO: [Train]: [028][00038/00051] (75.0%)  Loss=1.0455  cls_loss=0.6871  reg_loss=0.3584  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=481s  iter_time=6.937s
2025-08-06 06:13:06 Train INFO: [Train]: [028][00039/00051] (76.9%)  Loss=1.0484  cls_loss=0.6889  reg_loss=0.3595  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=434s  iter_time=2.304s
2025-08-06 06:15:14 Train INFO: [Train]: [028][00040/00051] (78.8%)  Loss=1.0478  cls_loss=0.6885  reg_loss=0.3593  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=422s  iter_time=127.759s
2025-08-06 06:15:16 Train INFO: [Train]: [028][00041/00051] (80.8%)  Loss=1.0449  cls_loss=0.6866  reg_loss=0.3582  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=375s  iter_time=2.344s
2025-08-06 06:15:23 Train INFO: [Train]: [028][00042/00051] (82.7%)  Loss=1.0450  cls_loss=0.6864  reg_loss=0.3586  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=331s  iter_time=7.228s
2025-08-06 06:15:26 Train INFO: [Train]: [028][00043/00051] (84.6%)  Loss=1.0400  cls_loss=0.6833  reg_loss=0.3566  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=288s  iter_time=2.356s
2025-08-06 06:17:36 Train INFO: [Train]: [028][00044/00051] (86.5%)  Loss=1.0387  cls_loss=0.6826  reg_loss=0.3561  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=267s  iter_time=130.632s
2025-08-06 06:17:39 Train INFO: [Train]: [028][00045/00051] (88.5%)  Loss=1.0333  cls_loss=0.6791  reg_loss=0.3542  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=224s  iter_time=2.318s
2025-08-06 06:17:45 Train INFO: [Train]: [028][00046/00051] (90.4%)  Loss=1.0302  cls_loss=0.6767  reg_loss=0.3536  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=184s  iter_time=6.813s
2025-08-06 06:17:48 Train INFO: [Train]: [028][00047/00051] (92.3%)  Loss=1.0319  cls_loss=0.6781  reg_loss=0.3538  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=144s  iter_time=2.336s
2025-08-06 06:19:58 Train INFO: [Train]: [028][00048/00051] (94.2%)  Loss=1.0309  cls_loss=0.6775  reg_loss=0.3533  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=114s  iter_time=129.914s
2025-08-06 06:20:00 Train INFO: [Train]: [028][00049/00051] (96.2%)  Loss=1.0323  cls_loss=0.6783  reg_loss=0.3540  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.162s
2025-08-06 06:20:03 Train INFO: [Train]: [028][00050/00051] (98.1%)  Loss=1.0320  cls_loss=0.6780  reg_loss=0.3540  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=37s  iter_time=2.806s
2025-08-06 06:20:05 Train INFO: [Train]: [028][00051/00051] (100.0%)  Loss=1.0319  cls_loss=0.6775  reg_loss=0.3544  lr_det=1.0e-04  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.043s
2025-08-06 06:20:05 Train INFO: [Train]: Epoch 28 completed in 1865.1s (avg 35.867s/iter)
2025-08-06 06:20:05 Train INFO: [Train]: Final Loss=1.0319
2025-08-06 06:20:05 Train INFO: [Train]: Epoch 29 started (Total iterations: 52)
2025-08-06 06:22:43 Train INFO: [Train]: [029][00001/00051] (3.8%)  Loss=0.9563  cls_loss=0.6207  reg_loss=0.3356  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3944s  iter_time=157.743s  fwd=2.125s/bwd=0.048s/opt=0.013s
2025-08-06 06:22:45 Train INFO: [Train]: [029][00002/00051] (5.8%)  Loss=1.0166  cls_loss=0.6621  reg_loss=0.3546  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2614s  iter_time=2.310s
2025-08-06 06:22:48 Train INFO: [Train]: [029][00003/00051] (7.7%)  Loss=1.0300  cls_loss=0.6732  reg_loss=0.3567  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1948s  iter_time=2.255s
2025-08-06 06:25:01 Train INFO: [Train]: [029][00004/00051] (9.6%)  Loss=1.0087  cls_loss=0.6607  reg_loss=0.3480  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2778s  iter_time=133.209s
2025-08-06 06:25:05 Train INFO: [Train]: [029][00005/00051] (11.5%)  Loss=1.0269  cls_loss=0.6712  reg_loss=0.3557  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2300s  iter_time=4.537s
2025-08-06 06:25:08 Train INFO: [Train]: [029][00006/00051] (13.5%)  Loss=1.0345  cls_loss=0.6773  reg_loss=0.3571  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1943s  iter_time=2.257s
2025-08-06 06:25:10 Train INFO: [Train]: [029][00007/00051] (15.4%)  Loss=1.0571  cls_loss=0.6928  reg_loss=0.3642  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1675s  iter_time=2.308s
2025-08-06 06:27:20 Train INFO: [Train]: [029][00008/00051] (17.3%)  Loss=1.0298  cls_loss=0.6764  reg_loss=0.3534  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2076s  iter_time=129.850s
2025-08-06 06:27:30 Train INFO: [Train]: [029][00009/00051] (19.2%)  Loss=1.0273  cls_loss=0.6751  reg_loss=0.3521  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1868s  iter_time=10.224s
2025-08-06 06:27:32 Train INFO: [Train]: [029][00010/00051] (21.2%)  Loss=1.0327  cls_loss=0.6767  reg_loss=0.3559  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1666s  iter_time=2.274s
2025-08-06 06:27:35 Train INFO: [Train]: [029][00011/00051] (23.1%)  Loss=1.0265  cls_loss=0.6729  reg_loss=0.3536  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1498s  iter_time=2.330s
2025-08-06 06:29:45 Train INFO: [Train]: [029][00012/00051] (25.0%)  Loss=1.0220  cls_loss=0.6718  reg_loss=0.3502  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1738s  iter_time=129.891s
2025-08-06 06:29:55 Train INFO: [Train]: [029][00013/00051] (26.9%)  Loss=1.0263  cls_loss=0.6772  reg_loss=0.3491  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1600s  iter_time=10.212s
2025-08-06 06:29:57 Train INFO: [Train]: [029][00014/00051] (28.8%)  Loss=1.0302  cls_loss=0.6826  reg_loss=0.3475  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1460s  iter_time=2.342s
2025-08-06 06:29:59 Train INFO: [Train]: [029][00015/00051] (30.8%)  Loss=1.0294  cls_loss=0.6822  reg_loss=0.3472  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1336s  iter_time=2.223s
2025-08-06 06:32:07 Train INFO: [Train]: [029][00016/00051] (32.7%)  Loss=1.0323  cls_loss=0.6845  reg_loss=0.3478  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1487s  iter_time=128.158s
2025-08-06 06:32:10 Train INFO: [Train]: [029][00017/00051] (34.6%)  Loss=1.0236  cls_loss=0.6783  reg_loss=0.3453  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1368s  iter_time=2.339s
2025-08-06 06:32:12 Train INFO: [Train]: [029][00018/00051] (36.5%)  Loss=1.0221  cls_loss=0.6779  reg_loss=0.3442  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1262s  iter_time=2.306s
2025-08-06 06:32:14 Train INFO: [Train]: [029][00019/00051] (38.5%)  Loss=1.0318  cls_loss=0.6845  reg_loss=0.3472  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1166s  iter_time=2.247s
2025-08-06 06:34:28 Train INFO: [Train]: [029][00020/00051] (40.4%)  Loss=1.0250  cls_loss=0.6790  reg_loss=0.3460  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1273s  iter_time=133.515s
2025-08-06 06:34:30 Train INFO: [Train]: [029][00021/00051] (42.3%)  Loss=1.0356  cls_loss=0.6864  reg_loss=0.3492  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1179s  iter_time=2.357s
2025-08-06 06:34:33 Train INFO: [Train]: [029][00022/00051] (44.2%)  Loss=1.0454  cls_loss=0.6932  reg_loss=0.3522  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1093s  iter_time=2.297s
2025-08-06 06:34:35 Train INFO: [Train]: [029][00023/00051] (46.2%)  Loss=1.0394  cls_loss=0.6887  reg_loss=0.3507  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1014s  iter_time=2.312s
2025-08-06 06:36:49 Train INFO: [Train]: [029][00024/00051] (48.1%)  Loss=1.0412  cls_loss=0.6900  reg_loss=0.3511  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1084s  iter_time=134.189s
2025-08-06 06:36:51 Train INFO: [Train]: [029][00025/00051] (50.0%)  Loss=1.0396  cls_loss=0.6879  reg_loss=0.3518  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1006s  iter_time=2.323s
2025-08-06 06:36:54 Train INFO: [Train]: [029][00026/00051] (51.9%)  Loss=1.0470  cls_loss=0.6925  reg_loss=0.3545  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=934s  iter_time=2.369s
2025-08-06 06:36:56 Train INFO: [Train]: [029][00027/00051] (53.8%)  Loss=1.0504  cls_loss=0.6943  reg_loss=0.3562  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=866s  iter_time=2.237s
2025-08-06 06:39:08 Train INFO: [Train]: [029][00028/00051] (55.8%)  Loss=1.0532  cls_loss=0.6963  reg_loss=0.3569  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=906s  iter_time=131.736s
2025-08-06 06:39:13 Train INFO: [Train]: [029][00029/00051] (57.7%)  Loss=1.0520  cls_loss=0.6952  reg_loss=0.3568  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=841s  iter_time=5.106s
2025-08-06 06:39:15 Train INFO: [Train]: [029][00030/00051] (59.6%)  Loss=1.0497  cls_loss=0.6937  reg_loss=0.3561  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=779s  iter_time=2.270s
2025-08-06 06:39:17 Train INFO: [Train]: [029][00031/00051] (61.5%)  Loss=1.0475  cls_loss=0.6922  reg_loss=0.3553  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=720s  iter_time=2.323s
2025-08-06 06:41:27 Train INFO: [Train]: [029][00032/00051] (63.5%)  Loss=1.0466  cls_loss=0.6915  reg_loss=0.3552  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=738s  iter_time=129.557s
2025-08-06 06:41:29 Train INFO: [Train]: [029][00033/00051] (65.4%)  Loss=1.0439  cls_loss=0.6902  reg_loss=0.3537  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=680s  iter_time=2.345s
2025-08-06 06:41:32 Train INFO: [Train]: [029][00034/00051] (67.3%)  Loss=1.0427  cls_loss=0.6900  reg_loss=0.3527  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=625s  iter_time=2.323s
2025-08-06 06:41:34 Train INFO: [Train]: [029][00035/00051] (69.2%)  Loss=1.0406  cls_loss=0.6884  reg_loss=0.3522  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=573s  iter_time=2.225s
2025-08-06 06:43:53 Train INFO: [Train]: [029][00036/00051] (71.2%)  Loss=1.0416  cls_loss=0.6887  reg_loss=0.3530  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=579s  iter_time=139.182s
2025-08-06 06:43:55 Train INFO: [Train]: [029][00037/00051] (73.1%)  Loss=1.0403  cls_loss=0.6876  reg_loss=0.3527  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=527s  iter_time=2.332s
2025-08-06 06:43:58 Train INFO: [Train]: [029][00038/00051] (75.0%)  Loss=1.0428  cls_loss=0.6888  reg_loss=0.3539  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=477s  iter_time=2.283s
2025-08-06 06:44:00 Train INFO: [Train]: [029][00039/00051] (76.9%)  Loss=1.0424  cls_loss=0.6882  reg_loss=0.3542  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.280s
2025-08-06 06:46:10 Train INFO: [Train]: [029][00040/00051] (78.8%)  Loss=1.0442  cls_loss=0.6888  reg_loss=0.3554  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=420s  iter_time=130.397s
2025-08-06 06:46:14 Train INFO: [Train]: [029][00041/00051] (80.8%)  Loss=1.0474  cls_loss=0.6909  reg_loss=0.3565  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=373s  iter_time=3.368s
2025-08-06 06:46:16 Train INFO: [Train]: [029][00042/00051] (82.7%)  Loss=1.0495  cls_loss=0.6923  reg_loss=0.3572  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=329s  iter_time=2.269s
2025-08-06 06:46:18 Train INFO: [Train]: [029][00043/00051] (84.6%)  Loss=1.0456  cls_loss=0.6896  reg_loss=0.3560  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.279s
2025-08-06 06:48:33 Train INFO: [Train]: [029][00044/00051] (86.5%)  Loss=1.0413  cls_loss=0.6871  reg_loss=0.3542  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=266s  iter_time=134.289s
2025-08-06 06:48:35 Train INFO: [Train]: [029][00045/00051] (88.5%)  Loss=1.0379  cls_loss=0.6846  reg_loss=0.3532  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.334s
2025-08-06 06:48:37 Train INFO: [Train]: [029][00046/00051] (90.4%)  Loss=1.0371  cls_loss=0.6839  reg_loss=0.3532  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=2.310s
2025-08-06 06:48:39 Train INFO: [Train]: [029][00047/00051] (92.3%)  Loss=1.0361  cls_loss=0.6832  reg_loss=0.3530  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.255s
2025-08-06 06:50:52 Train INFO: [Train]: [029][00048/00051] (94.2%)  Loss=1.0390  cls_loss=0.6846  reg_loss=0.3544  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=132.736s
2025-08-06 06:50:54 Train INFO: [Train]: [029][00049/00051] (96.2%)  Loss=1.0349  cls_loss=0.6827  reg_loss=0.3522  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.162s
2025-08-06 06:50:57 Train INFO: [Train]: [029][00050/00051] (98.1%)  Loss=1.0353  cls_loss=0.6827  reg_loss=0.3525  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.177s
2025-08-06 06:50:59 Train INFO: [Train]: [029][00051/00051] (100.0%)  Loss=1.0337  cls_loss=0.6817  reg_loss=0.3520  lr_det=5.2e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.047s
2025-08-06 06:50:59 Train INFO: [Train]: Epoch 29 completed in 1854.1s (avg 35.655s/iter)
2025-08-06 06:50:59 Train INFO: [Train]: Final Loss=1.0337
2025-08-06 06:50:59 Train INFO: [Val]: Epoch 29 Loss
2025-08-06 07:09:53 Train INFO: [Val]: [029]  Loss=1.4827  cls_loss=1.1033  reg_loss=0.3794  Average-mAP=0.30%
2025-08-06 07:09:54 Train INFO: Checkpoint saved at epoch 29
2025-08-06 07:09:54 Train INFO: [Train]: Epoch 30 started (Total iterations: 52)
2025-08-06 07:12:32 Train INFO: [Train]: [030][00001/00051] (3.8%)  Loss=0.9100  cls_loss=0.5935  reg_loss=0.3166  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3951s  iter_time=158.059s  fwd=2.303s/bwd=0.050s/opt=0.017s
2025-08-06 07:12:34 Train INFO: [Train]: [030][00002/00051] (5.8%)  Loss=1.0275  cls_loss=0.6747  reg_loss=0.3528  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2619s  iter_time=2.285s
2025-08-06 07:12:37 Train INFO: [Train]: [030][00003/00051] (7.7%)  Loss=1.0550  cls_loss=0.6960  reg_loss=0.3590  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1951s  iter_time=2.249s
2025-08-06 07:14:49 Train INFO: [Train]: [030][00004/00051] (9.6%)  Loss=1.0236  cls_loss=0.6755  reg_loss=0.3481  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2770s  iter_time=132.039s
2025-08-06 07:14:53 Train INFO: [Train]: [030][00005/00051] (11.5%)  Loss=1.0394  cls_loss=0.6859  reg_loss=0.3535  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2295s  iter_time=4.738s
2025-08-06 07:14:56 Train INFO: [Train]: [030][00006/00051] (13.5%)  Loss=1.0420  cls_loss=0.6842  reg_loss=0.3577  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1939s  iter_time=2.274s
2025-08-06 07:14:58 Train INFO: [Train]: [030][00007/00051] (15.4%)  Loss=1.0384  cls_loss=0.6839  reg_loss=0.3545  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1672s  iter_time=2.266s
2025-08-06 07:17:04 Train INFO: [Train]: [030][00008/00051] (17.3%)  Loss=1.0219  cls_loss=0.6739  reg_loss=0.3480  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2055s  iter_time=126.148s
2025-08-06 07:17:13 Train INFO: [Train]: [030][00009/00051] (19.2%)  Loss=1.0266  cls_loss=0.6800  reg_loss=0.3466  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1845s  iter_time=9.190s
2025-08-06 07:17:16 Train INFO: [Train]: [030][00010/00051] (21.2%)  Loss=1.0221  cls_loss=0.6781  reg_loss=0.3440  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1646s  iter_time=2.334s
2025-08-06 07:17:18 Train INFO: [Train]: [030][00011/00051] (23.1%)  Loss=1.0312  cls_loss=0.6839  reg_loss=0.3473  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1480s  iter_time=2.314s
2025-08-06 07:19:28 Train INFO: [Train]: [030][00012/00051] (25.0%)  Loss=1.0236  cls_loss=0.6803  reg_loss=0.3433  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1722s  iter_time=130.144s
2025-08-06 07:19:37 Train INFO: [Train]: [030][00013/00051] (26.9%)  Loss=1.0251  cls_loss=0.6817  reg_loss=0.3434  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1581s  iter_time=8.457s
2025-08-06 07:19:39 Train INFO: [Train]: [030][00014/00051] (28.8%)  Loss=1.0328  cls_loss=0.6879  reg_loss=0.3449  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1443s  iter_time=2.350s
2025-08-06 07:19:41 Train INFO: [Train]: [030][00015/00051] (30.8%)  Loss=1.0312  cls_loss=0.6860  reg_loss=0.3452  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1321s  iter_time=2.307s
2025-08-06 07:21:50 Train INFO: [Train]: [030][00016/00051] (32.7%)  Loss=1.0191  cls_loss=0.6782  reg_loss=0.3408  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1474s  iter_time=128.765s
2025-08-06 07:21:52 Train INFO: [Train]: [030][00017/00051] (34.6%)  Loss=1.0220  cls_loss=0.6802  reg_loss=0.3418  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1357s  iter_time=2.284s
2025-08-06 07:21:55 Train INFO: [Train]: [030][00018/00051] (36.5%)  Loss=1.0170  cls_loss=0.6768  reg_loss=0.3402  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1251s  iter_time=2.253s
2025-08-06 07:21:57 Train INFO: [Train]: [030][00019/00051] (38.5%)  Loss=1.0226  cls_loss=0.6797  reg_loss=0.3429  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1156s  iter_time=2.304s
2025-08-06 07:24:08 Train INFO: [Train]: [030][00020/00051] (40.4%)  Loss=1.0268  cls_loss=0.6814  reg_loss=0.3454  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1261s  iter_time=131.504s
2025-08-06 07:24:11 Train INFO: [Train]: [030][00021/00051] (42.3%)  Loss=1.0315  cls_loss=0.6839  reg_loss=0.3476  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1168s  iter_time=2.353s
2025-08-06 07:24:13 Train INFO: [Train]: [030][00022/00051] (44.2%)  Loss=1.0410  cls_loss=0.6894  reg_loss=0.3516  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1083s  iter_time=2.303s
2025-08-06 07:24:15 Train INFO: [Train]: [030][00023/00051] (46.2%)  Loss=1.0353  cls_loss=0.6852  reg_loss=0.3502  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1005s  iter_time=2.291s
2025-08-06 07:26:29 Train INFO: [Train]: [030][00024/00051] (48.1%)  Loss=1.0414  cls_loss=0.6890  reg_loss=0.3524  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1075s  iter_time=134.067s
2025-08-06 07:26:32 Train INFO: [Train]: [030][00025/00051] (50.0%)  Loss=1.0395  cls_loss=0.6874  reg_loss=0.3521  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=998s  iter_time=2.292s
2025-08-06 07:26:34 Train INFO: [Train]: [030][00026/00051] (51.9%)  Loss=1.0355  cls_loss=0.6855  reg_loss=0.3500  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=926s  iter_time=2.350s
2025-08-06 07:26:36 Train INFO: [Train]: [030][00027/00051] (53.8%)  Loss=1.0349  cls_loss=0.6840  reg_loss=0.3509  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=859s  iter_time=2.251s
2025-08-06 07:28:49 Train INFO: [Train]: [030][00028/00051] (55.8%)  Loss=1.0311  cls_loss=0.6811  reg_loss=0.3501  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=900s  iter_time=133.044s
2025-08-06 07:28:52 Train INFO: [Train]: [030][00029/00051] (57.7%)  Loss=1.0347  cls_loss=0.6833  reg_loss=0.3513  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=834s  iter_time=2.349s
2025-08-06 07:28:54 Train INFO: [Train]: [030][00030/00051] (59.6%)  Loss=1.0339  cls_loss=0.6826  reg_loss=0.3513  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=772s  iter_time=2.359s
2025-08-06 07:28:56 Train INFO: [Train]: [030][00031/00051] (61.5%)  Loss=1.0322  cls_loss=0.6819  reg_loss=0.3503  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=714s  iter_time=2.270s
2025-08-06 07:31:09 Train INFO: [Train]: [030][00032/00051] (63.5%)  Loss=1.0303  cls_loss=0.6809  reg_loss=0.3494  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=734s  iter_time=132.732s
2025-08-06 07:31:11 Train INFO: [Train]: [030][00033/00051] (65.4%)  Loss=1.0286  cls_loss=0.6797  reg_loss=0.3488  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=676s  iter_time=2.356s
2025-08-06 07:31:14 Train INFO: [Train]: [030][00034/00051] (67.3%)  Loss=1.0313  cls_loss=0.6813  reg_loss=0.3500  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=622s  iter_time=2.354s
2025-08-06 07:31:16 Train INFO: [Train]: [030][00035/00051] (69.2%)  Loss=1.0300  cls_loss=0.6800  reg_loss=0.3500  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=570s  iter_time=2.279s
2025-08-06 07:33:30 Train INFO: [Train]: [030][00036/00051] (71.2%)  Loss=1.0279  cls_loss=0.6788  reg_loss=0.3491  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=574s  iter_time=134.364s
2025-08-06 07:33:33 Train INFO: [Train]: [030][00037/00051] (73.1%)  Loss=1.0263  cls_loss=0.6774  reg_loss=0.3490  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=523s  iter_time=2.335s
2025-08-06 07:33:35 Train INFO: [Train]: [030][00038/00051] (75.0%)  Loss=1.0289  cls_loss=0.6789  reg_loss=0.3500  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=474s  iter_time=2.348s
2025-08-06 07:33:37 Train INFO: [Train]: [030][00039/00051] (76.9%)  Loss=1.0285  cls_loss=0.6786  reg_loss=0.3499  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=427s  iter_time=2.323s
2025-08-06 07:35:51 Train INFO: [Train]: [030][00040/00051] (78.8%)  Loss=1.0275  cls_loss=0.6776  reg_loss=0.3499  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=418s  iter_time=133.148s
2025-08-06 07:35:53 Train INFO: [Train]: [030][00041/00051] (80.8%)  Loss=1.0277  cls_loss=0.6774  reg_loss=0.3503  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=371s  iter_time=2.326s
2025-08-06 07:35:55 Train INFO: [Train]: [030][00042/00051] (82.7%)  Loss=1.0304  cls_loss=0.6794  reg_loss=0.3510  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=327s  iter_time=2.240s
2025-08-06 07:35:57 Train INFO: [Train]: [030][00043/00051] (84.6%)  Loss=1.0264  cls_loss=0.6773  reg_loss=0.3491  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=284s  iter_time=2.275s
2025-08-06 07:38:13 Train INFO: [Train]: [030][00044/00051] (86.5%)  Loss=1.0253  cls_loss=0.6763  reg_loss=0.3490  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=264s  iter_time=135.128s
2025-08-06 07:38:15 Train INFO: [Train]: [030][00045/00051] (88.5%)  Loss=1.0231  cls_loss=0.6745  reg_loss=0.3486  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=222s  iter_time=2.315s
2025-08-06 07:38:17 Train INFO: [Train]: [030][00046/00051] (90.4%)  Loss=1.0251  cls_loss=0.6753  reg_loss=0.3498  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=181s  iter_time=2.301s
2025-08-06 07:38:19 Train INFO: [Train]: [030][00047/00051] (92.3%)  Loss=1.0237  cls_loss=0.6744  reg_loss=0.3493  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=142s  iter_time=2.275s
2025-08-06 07:40:27 Train INFO: [Train]: [030][00048/00051] (94.2%)  Loss=1.0212  cls_loss=0.6727  reg_loss=0.3485  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=112s  iter_time=127.908s
2025-08-06 07:40:30 Train INFO: [Train]: [030][00049/00051] (96.2%)  Loss=1.0224  cls_loss=0.6740  reg_loss=0.3484  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=73s  iter_time=2.189s
2025-08-06 07:40:32 Train INFO: [Train]: [030][00050/00051] (98.1%)  Loss=1.0217  cls_loss=0.6738  reg_loss=0.3479  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.173s
2025-08-06 07:40:34 Train INFO: [Train]: [030][00051/00051] (100.0%)  Loss=1.0217  cls_loss=0.6737  reg_loss=0.3480  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.044s
2025-08-06 07:40:34 Train INFO: [Train]: Epoch 30 completed in 1840.3s (avg 35.391s/iter)
2025-08-06 07:40:34 Train INFO: [Train]: Final Loss=1.0217
2025-08-06 07:40:34 Train INFO: [Train]: Epoch 31 started (Total iterations: 52)
2025-08-06 07:43:17 Train INFO: [Train]: [031][00001/00051] (3.8%)  Loss=0.9899  cls_loss=0.6459  reg_loss=0.3440  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4077s  iter_time=163.071s  fwd=2.107s/bwd=0.062s/opt=0.014s
2025-08-06 07:43:20 Train INFO: [Train]: [031][00002/00051] (5.8%)  Loss=1.0295  cls_loss=0.6826  reg_loss=0.3468  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2701s  iter_time=2.304s
2025-08-06 07:43:22 Train INFO: [Train]: [031][00003/00051] (7.7%)  Loss=1.0335  cls_loss=0.6893  reg_loss=0.3442  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2012s  iter_time=2.330s
2025-08-06 07:45:32 Train INFO: [Train]: [031][00004/00051] (9.6%)  Loss=1.0238  cls_loss=0.6792  reg_loss=0.3446  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2799s  iter_time=130.056s
2025-08-06 07:45:40 Train INFO: [Train]: [031][00005/00051] (11.5%)  Loss=1.0375  cls_loss=0.6863  reg_loss=0.3512  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2343s  iter_time=7.895s
2025-08-06 07:45:42 Train INFO: [Train]: [031][00006/00051] (13.5%)  Loss=1.0396  cls_loss=0.6920  reg_loss=0.3476  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1980s  iter_time=2.311s
2025-08-06 07:45:45 Train INFO: [Train]: [031][00007/00051] (15.4%)  Loss=1.0535  cls_loss=0.6993  reg_loss=0.3542  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1707s  iter_time=2.330s
2025-08-06 07:47:50 Train INFO: [Train]: [031][00008/00051] (17.3%)  Loss=1.0373  cls_loss=0.6868  reg_loss=0.3505  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2082s  iter_time=125.532s
2025-08-06 07:48:01 Train INFO: [Train]: [031][00009/00051] (19.2%)  Loss=1.0245  cls_loss=0.6812  reg_loss=0.3433  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1874s  iter_time=10.309s
2025-08-06 07:48:04 Train INFO: [Train]: [031][00010/00051] (21.2%)  Loss=1.0171  cls_loss=0.6758  reg_loss=0.3413  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1675s  iter_time=3.237s
2025-08-06 07:48:06 Train INFO: [Train]: [031][00011/00051] (23.1%)  Loss=1.0350  cls_loss=0.6873  reg_loss=0.3478  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1506s  iter_time=2.349s
2025-08-06 07:50:11 Train INFO: [Train]: [031][00012/00051] (25.0%)  Loss=1.0284  cls_loss=0.6818  reg_loss=0.3466  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1730s  iter_time=124.870s
2025-08-06 07:50:24 Train INFO: [Train]: [031][00013/00051] (26.9%)  Loss=1.0363  cls_loss=0.6868  reg_loss=0.3495  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1600s  iter_time=12.813s
2025-08-06 07:50:26 Train INFO: [Train]: [031][00014/00051] (28.8%)  Loss=1.0327  cls_loss=0.6860  reg_loss=0.3467  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1460s  iter_time=2.275s
2025-08-06 07:50:28 Train INFO: [Train]: [031][00015/00051] (30.8%)  Loss=1.0289  cls_loss=0.6832  reg_loss=0.3457  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1337s  iter_time=2.316s
2025-08-06 07:52:38 Train INFO: [Train]: [031][00016/00051] (32.7%)  Loss=1.0243  cls_loss=0.6808  reg_loss=0.3435  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1490s  iter_time=129.521s
2025-08-06 07:52:41 Train INFO: [Train]: [031][00017/00051] (34.6%)  Loss=1.0143  cls_loss=0.6730  reg_loss=0.3413  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1373s  iter_time=3.396s
2025-08-06 07:52:44 Train INFO: [Train]: [031][00018/00051] (36.5%)  Loss=1.0121  cls_loss=0.6712  reg_loss=0.3409  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1267s  iter_time=2.289s
2025-08-06 07:52:46 Train INFO: [Train]: [031][00019/00051] (38.5%)  Loss=1.0208  cls_loss=0.6764  reg_loss=0.3444  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1170s  iter_time=2.286s
2025-08-06 07:54:58 Train INFO: [Train]: [031][00020/00051] (40.4%)  Loss=1.0084  cls_loss=0.6679  reg_loss=0.3404  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1274s  iter_time=131.815s
2025-08-06 07:55:00 Train INFO: [Train]: [031][00021/00051] (42.3%)  Loss=1.0110  cls_loss=0.6684  reg_loss=0.3426  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1180s  iter_time=2.309s
2025-08-06 07:55:03 Train INFO: [Train]: [031][00022/00051] (44.2%)  Loss=1.0170  cls_loss=0.6720  reg_loss=0.3451  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1095s  iter_time=2.634s
2025-08-06 07:55:05 Train INFO: [Train]: [031][00023/00051] (46.2%)  Loss=1.0134  cls_loss=0.6696  reg_loss=0.3439  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1016s  iter_time=2.283s
2025-08-06 07:57:20 Train INFO: [Train]: [031][00024/00051] (48.1%)  Loss=1.0166  cls_loss=0.6703  reg_loss=0.3463  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1086s  iter_time=135.110s
2025-08-06 07:57:22 Train INFO: [Train]: [031][00025/00051] (50.0%)  Loss=1.0165  cls_loss=0.6695  reg_loss=0.3469  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1008s  iter_time=2.343s
2025-08-06 07:57:25 Train INFO: [Train]: [031][00026/00051] (51.9%)  Loss=1.0255  cls_loss=0.6753  reg_loss=0.3503  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=936s  iter_time=2.327s
2025-08-06 07:57:27 Train INFO: [Train]: [031][00027/00051] (53.8%)  Loss=1.0288  cls_loss=0.6777  reg_loss=0.3511  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=868s  iter_time=2.266s
2025-08-06 07:59:36 Train INFO: [Train]: [031][00028/00051] (55.8%)  Loss=1.0298  cls_loss=0.6780  reg_loss=0.3518  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=906s  iter_time=129.380s
2025-08-06 07:59:43 Train INFO: [Train]: [031][00029/00051] (57.7%)  Loss=1.0296  cls_loss=0.6784  reg_loss=0.3511  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=842s  iter_time=6.163s
2025-08-06 07:59:45 Train INFO: [Train]: [031][00030/00051] (59.6%)  Loss=1.0299  cls_loss=0.6791  reg_loss=0.3508  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=779s  iter_time=2.331s
2025-08-06 07:59:47 Train INFO: [Train]: [031][00031/00051] (61.5%)  Loss=1.0239  cls_loss=0.6747  reg_loss=0.3492  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=720s  iter_time=2.275s
2025-08-06 08:01:54 Train INFO: [Train]: [031][00032/00051] (63.5%)  Loss=1.0279  cls_loss=0.6776  reg_loss=0.3503  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=737s  iter_time=126.594s
2025-08-06 08:02:00 Train INFO: [Train]: [031][00033/00051] (65.4%)  Loss=1.0246  cls_loss=0.6756  reg_loss=0.3489  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=681s  iter_time=6.119s
2025-08-06 08:02:02 Train INFO: [Train]: [031][00034/00051] (67.3%)  Loss=1.0267  cls_loss=0.6768  reg_loss=0.3500  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=625s  iter_time=2.276s
2025-08-06 08:02:04 Train INFO: [Train]: [031][00035/00051] (69.2%)  Loss=1.0251  cls_loss=0.6756  reg_loss=0.3495  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=573s  iter_time=2.334s
2025-08-06 08:04:15 Train INFO: [Train]: [031][00036/00051] (71.2%)  Loss=1.0283  cls_loss=0.6783  reg_loss=0.3499  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=130.794s
2025-08-06 08:04:18 Train INFO: [Train]: [031][00037/00051] (73.1%)  Loss=1.0273  cls_loss=0.6773  reg_loss=0.3500  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=524s  iter_time=2.578s
2025-08-06 08:04:21 Train INFO: [Train]: [031][00038/00051] (75.0%)  Loss=1.0269  cls_loss=0.6768  reg_loss=0.3501  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=475s  iter_time=2.854s
2025-08-06 08:04:23 Train INFO: [Train]: [031][00039/00051] (76.9%)  Loss=1.0262  cls_loss=0.6760  reg_loss=0.3502  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=429s  iter_time=2.264s
2025-08-06 08:06:34 Train INFO: [Train]: [031][00040/00051] (78.8%)  Loss=1.0284  cls_loss=0.6766  reg_loss=0.3518  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=418s  iter_time=131.031s
2025-08-06 08:06:41 Train INFO: [Train]: [031][00041/00051] (80.8%)  Loss=1.0273  cls_loss=0.6756  reg_loss=0.3517  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=373s  iter_time=6.955s
2025-08-06 08:06:43 Train INFO: [Train]: [031][00042/00051] (82.7%)  Loss=1.0313  cls_loss=0.6779  reg_loss=0.3534  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=2.300s
2025-08-06 08:06:46 Train INFO: [Train]: [031][00043/00051] (84.6%)  Loss=1.0276  cls_loss=0.6755  reg_loss=0.3520  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.286s
2025-08-06 08:08:54 Train INFO: [Train]: [031][00044/00051] (86.5%)  Loss=1.0274  cls_loss=0.6754  reg_loss=0.3520  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=264s  iter_time=128.248s
2025-08-06 08:08:57 Train INFO: [Train]: [031][00045/00051] (88.5%)  Loss=1.0246  cls_loss=0.6735  reg_loss=0.3511  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=222s  iter_time=2.785s
2025-08-06 08:09:00 Train INFO: [Train]: [031][00046/00051] (90.4%)  Loss=1.0218  cls_loss=0.6715  reg_loss=0.3503  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=181s  iter_time=2.993s
2025-08-06 08:09:02 Train INFO: [Train]: [031][00047/00051] (92.3%)  Loss=1.0232  cls_loss=0.6722  reg_loss=0.3510  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=142s  iter_time=2.279s
2025-08-06 08:11:11 Train INFO: [Train]: [031][00048/00051] (94.2%)  Loss=1.0200  cls_loss=0.6707  reg_loss=0.3493  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=112s  iter_time=129.225s
2025-08-06 08:11:16 Train INFO: [Train]: [031][00049/00051] (96.2%)  Loss=1.0203  cls_loss=0.6714  reg_loss=0.3489  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=4.697s
2025-08-06 08:11:18 Train INFO: [Train]: [031][00050/00051] (98.1%)  Loss=1.0240  cls_loss=0.6739  reg_loss=0.3500  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.168s
2025-08-06 08:11:20 Train INFO: [Train]: [031][00051/00051] (100.0%)  Loss=1.0225  cls_loss=0.6732  reg_loss=0.3494  lr_det=4.8e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.047s
2025-08-06 08:11:21 Train INFO: [Train]: Epoch 31 completed in 1846.3s (avg 35.505s/iter)
2025-08-06 08:11:21 Train INFO: [Train]: Final Loss=1.0225
2025-08-06 08:11:21 Train INFO: [Train]: Epoch 32 started (Total iterations: 52)
2025-08-06 08:13:59 Train INFO: [Train]: [032][00001/00051] (3.8%)  Loss=0.8818  cls_loss=0.5963  reg_loss=0.2855  lr_det=5.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3969s  iter_time=158.748s  fwd=2.118s/bwd=0.064s/opt=0.021s
2025-08-06 08:14:02 Train INFO: [Train]: [032][00002/00051] (5.8%)  Loss=1.0369  cls_loss=0.6938  reg_loss=0.3431  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2630s  iter_time=2.247s
2025-08-06 08:14:04 Train INFO: [Train]: [032][00003/00051] (7.7%)  Loss=1.0378  cls_loss=0.6927  reg_loss=0.3451  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1960s  iter_time=2.315s
2025-08-06 08:16:17 Train INFO: [Train]: [032][00004/00051] (9.6%)  Loss=1.0282  cls_loss=0.6835  reg_loss=0.3447  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2785s  iter_time=132.927s
2025-08-06 08:16:27 Train INFO: [Train]: [032][00005/00051] (11.5%)  Loss=1.0503  cls_loss=0.6965  reg_loss=0.3538  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2347s  iter_time=9.939s
2025-08-06 08:16:29 Train INFO: [Train]: [032][00006/00051] (13.5%)  Loss=1.0361  cls_loss=0.6908  reg_loss=0.3453  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1984s  iter_time=2.365s
2025-08-06 08:16:32 Train INFO: [Train]: [032][00007/00051] (15.4%)  Loss=1.0377  cls_loss=0.6926  reg_loss=0.3450  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1710s  iter_time=2.319s
2025-08-06 08:18:34 Train INFO: [Train]: [032][00008/00051] (17.3%)  Loss=1.0268  cls_loss=0.6843  reg_loss=0.3425  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2072s  iter_time=122.893s
2025-08-06 08:18:45 Train INFO: [Train]: [032][00009/00051] (19.2%)  Loss=1.0193  cls_loss=0.6802  reg_loss=0.3391  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1868s  iter_time=11.013s
2025-08-06 08:18:48 Train INFO: [Train]: [032][00010/00051] (21.2%)  Loss=1.0184  cls_loss=0.6800  reg_loss=0.3383  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1669s  iter_time=3.051s
2025-08-06 08:18:51 Train INFO: [Train]: [032][00011/00051] (23.1%)  Loss=1.0337  cls_loss=0.6897  reg_loss=0.3441  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1500s  iter_time=2.255s
2025-08-06 08:20:55 Train INFO: [Train]: [032][00012/00051] (25.0%)  Loss=1.0384  cls_loss=0.6937  reg_loss=0.3447  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1722s  iter_time=124.037s
2025-08-06 08:21:09 Train INFO: [Train]: [032][00013/00051] (26.9%)  Loss=1.0492  cls_loss=0.7015  reg_loss=0.3477  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1597s  iter_time=14.340s
2025-08-06 08:21:11 Train INFO: [Train]: [032][00014/00051] (28.8%)  Loss=1.0451  cls_loss=0.6983  reg_loss=0.3468  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1457s  iter_time=2.332s
2025-08-06 08:21:14 Train INFO: [Train]: [032][00015/00051] (30.8%)  Loss=1.0413  cls_loss=0.6966  reg_loss=0.3447  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1334s  iter_time=2.303s
2025-08-06 08:23:16 Train INFO: [Train]: [032][00016/00051] (32.7%)  Loss=1.0357  cls_loss=0.6926  reg_loss=0.3431  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1473s  iter_time=122.539s
2025-08-06 08:23:25 Train INFO: [Train]: [032][00017/00051] (34.6%)  Loss=1.0282  cls_loss=0.6878  reg_loss=0.3404  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1369s  iter_time=8.972s
2025-08-06 08:23:28 Train INFO: [Train]: [032][00018/00051] (36.5%)  Loss=1.0247  cls_loss=0.6849  reg_loss=0.3398  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1263s  iter_time=2.526s
2025-08-06 08:23:30 Train INFO: [Train]: [032][00019/00051] (38.5%)  Loss=1.0271  cls_loss=0.6857  reg_loss=0.3414  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1167s  iter_time=2.215s
2025-08-06 08:25:37 Train INFO: [Train]: [032][00020/00051] (40.4%)  Loss=1.0223  cls_loss=0.6817  reg_loss=0.3406  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1264s  iter_time=127.052s
2025-08-06 08:25:44 Train INFO: [Train]: [032][00021/00051] (42.3%)  Loss=1.0313  cls_loss=0.6866  reg_loss=0.3447  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1177s  iter_time=6.915s
2025-08-06 08:25:46 Train INFO: [Train]: [032][00022/00051] (44.2%)  Loss=1.0397  cls_loss=0.6911  reg_loss=0.3486  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1091s  iter_time=2.312s
2025-08-06 08:25:49 Train INFO: [Train]: [032][00023/00051] (46.2%)  Loss=1.0331  cls_loss=0.6866  reg_loss=0.3465  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1013s  iter_time=2.252s
2025-08-06 08:27:56 Train INFO: [Train]: [032][00024/00051] (48.1%)  Loss=1.0345  cls_loss=0.6878  reg_loss=0.3467  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1075s  iter_time=127.889s
2025-08-06 08:28:04 Train INFO: [Train]: [032][00025/00051] (50.0%)  Loss=1.0314  cls_loss=0.6863  reg_loss=0.3451  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1003s  iter_time=7.372s
2025-08-06 08:28:06 Train INFO: [Train]: [032][00026/00051] (51.9%)  Loss=1.0287  cls_loss=0.6851  reg_loss=0.3436  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=931s  iter_time=2.308s
2025-08-06 08:28:08 Train INFO: [Train]: [032][00027/00051] (53.8%)  Loss=1.0273  cls_loss=0.6844  reg_loss=0.3429  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=864s  iter_time=2.275s
2025-08-06 08:30:17 Train INFO: [Train]: [032][00028/00051] (55.8%)  Loss=1.0293  cls_loss=0.6847  reg_loss=0.3446  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=901s  iter_time=128.578s
2025-08-06 08:30:28 Train INFO: [Train]: [032][00029/00051] (57.7%)  Loss=1.0277  cls_loss=0.6837  reg_loss=0.3440  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=841s  iter_time=10.963s
2025-08-06 08:30:30 Train INFO: [Train]: [032][00030/00051] (59.6%)  Loss=1.0256  cls_loss=0.6818  reg_loss=0.3438  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=779s  iter_time=2.347s
2025-08-06 08:30:33 Train INFO: [Train]: [032][00031/00051] (61.5%)  Loss=1.0270  cls_loss=0.6822  reg_loss=0.3448  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=720s  iter_time=2.241s
2025-08-06 08:32:38 Train INFO: [Train]: [032][00032/00051] (63.5%)  Loss=1.0295  cls_loss=0.6838  reg_loss=0.3456  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=736s  iter_time=125.592s
2025-08-06 08:32:44 Train INFO: [Train]: [032][00033/00051] (65.4%)  Loss=1.0242  cls_loss=0.6807  reg_loss=0.3435  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=679s  iter_time=5.436s
2025-08-06 08:32:48 Train INFO: [Train]: [032][00034/00051] (67.3%)  Loss=1.0292  cls_loss=0.6839  reg_loss=0.3453  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=625s  iter_time=4.475s
2025-08-06 08:32:50 Train INFO: [Train]: [032][00035/00051] (69.2%)  Loss=1.0288  cls_loss=0.6834  reg_loss=0.3454  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=573s  iter_time=2.324s
2025-08-06 08:35:02 Train INFO: [Train]: [032][00036/00051] (71.2%)  Loss=1.0346  cls_loss=0.6867  reg_loss=0.3479  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=131.504s
2025-08-06 08:35:04 Train INFO: [Train]: [032][00037/00051] (73.1%)  Loss=1.0326  cls_loss=0.6842  reg_loss=0.3484  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=524s  iter_time=2.309s
2025-08-06 08:35:11 Train INFO: [Train]: [032][00038/00051] (75.0%)  Loss=1.0331  cls_loss=0.6841  reg_loss=0.3490  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=477s  iter_time=6.486s
2025-08-06 08:35:13 Train INFO: [Train]: [032][00039/00051] (76.9%)  Loss=1.0296  cls_loss=0.6818  reg_loss=0.3478  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.329s
2025-08-06 08:37:23 Train INFO: [Train]: [032][00040/00051] (78.8%)  Loss=1.0323  cls_loss=0.6837  reg_loss=0.3486  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=419s  iter_time=129.594s
2025-08-06 08:37:25 Train INFO: [Train]: [032][00041/00051] (80.8%)  Loss=1.0288  cls_loss=0.6815  reg_loss=0.3473  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=372s  iter_time=2.254s
2025-08-06 08:37:29 Train INFO: [Train]: [032][00042/00051] (82.7%)  Loss=1.0341  cls_loss=0.6847  reg_loss=0.3494  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=4.622s
2025-08-06 08:37:32 Train INFO: [Train]: [032][00043/00051] (84.6%)  Loss=1.0293  cls_loss=0.6813  reg_loss=0.3480  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=286s  iter_time=2.318s
2025-08-06 08:39:45 Train INFO: [Train]: [032][00044/00051] (86.5%)  Loss=1.0306  cls_loss=0.6821  reg_loss=0.3486  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=265s  iter_time=132.964s
2025-08-06 08:39:47 Train INFO: [Train]: [032][00045/00051] (88.5%)  Loss=1.0266  cls_loss=0.6795  reg_loss=0.3470  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=223s  iter_time=2.235s
2025-08-06 08:39:51 Train INFO: [Train]: [032][00046/00051] (90.4%)  Loss=1.0267  cls_loss=0.6797  reg_loss=0.3470  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=3.622s
2025-08-06 08:39:53 Train INFO: [Train]: [032][00047/00051] (92.3%)  Loss=1.0255  cls_loss=0.6789  reg_loss=0.3466  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.264s
2025-08-06 08:42:01 Train INFO: [Train]: [032][00048/00051] (94.2%)  Loss=1.0248  cls_loss=0.6789  reg_loss=0.3459  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=127.719s
2025-08-06 08:42:03 Train INFO: [Train]: [032][00049/00051] (96.2%)  Loss=1.0263  cls_loss=0.6802  reg_loss=0.3460  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.178s
2025-08-06 08:42:05 Train INFO: [Train]: [032][00050/00051] (98.1%)  Loss=1.0287  cls_loss=0.6822  reg_loss=0.3466  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.170s
2025-08-06 08:42:07 Train INFO: [Train]: [032][00051/00051] (100.0%)  Loss=1.0277  cls_loss=0.6817  reg_loss=0.3460  lr_det=1.0e-04  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.033s
2025-08-06 08:42:08 Train INFO: [Train]: Epoch 32 completed in 1847.0s (avg 35.519s/iter)
2025-08-06 08:42:08 Train INFO: [Train]: Final Loss=1.0277
2025-08-06 08:42:08 Train INFO: [Train]: Epoch 33 started (Total iterations: 52)
2025-08-06 08:44:49 Train INFO: [Train]: [033][00001/00051] (3.8%)  Loss=0.9875  cls_loss=0.6652  reg_loss=0.3222  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4033s  iter_time=161.314s  fwd=2.131s/bwd=0.058s/opt=0.022s
2025-08-06 08:44:51 Train INFO: [Train]: [033][00002/00051] (5.8%)  Loss=1.0887  cls_loss=0.7293  reg_loss=0.3594  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2672s  iter_time=2.286s
2025-08-06 08:44:53 Train INFO: [Train]: [033][00003/00051] (7.7%)  Loss=1.0472  cls_loss=0.7019  reg_loss=0.3452  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1990s  iter_time=2.211s
2025-08-06 08:47:08 Train INFO: [Train]: [033][00004/00051] (9.6%)  Loss=1.0430  cls_loss=0.6990  reg_loss=0.3441  lr_det=1.0e-04  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2825s  iter_time=134.721s
2025-08-06 08:47:10 Train INFO: [Train]: [033][00005/00051] (11.5%)  Loss=1.0603  cls_loss=0.7079  reg_loss=0.3524  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2322s  iter_time=2.301s
2025-08-06 08:47:13 Train INFO: [Train]: [033][00006/00051] (13.5%)  Loss=1.0643  cls_loss=0.7067  reg_loss=0.3576  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1962s  iter_time=2.288s
2025-08-06 08:47:15 Train INFO: [Train]: [033][00007/00051] (15.4%)  Loss=1.0619  cls_loss=0.7066  reg_loss=0.3554  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1691s  iter_time=2.268s
2025-08-06 08:49:25 Train INFO: [Train]: [033][00008/00051] (17.3%)  Loss=1.0552  cls_loss=0.6998  reg_loss=0.3554  lr_det=9.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2088s  iter_time=129.556s
2025-08-06 08:49:35 Train INFO: [Train]: [033][00009/00051] (19.2%)  Loss=1.0510  cls_loss=0.6971  reg_loss=0.3539  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1878s  iter_time=10.256s
2025-08-06 08:49:37 Train INFO: [Train]: [033][00010/00051] (21.2%)  Loss=1.0380  cls_loss=0.6884  reg_loss=0.3497  lr_det=9.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1675s  iter_time=2.298s
2025-08-06 08:49:39 Train INFO: [Train]: [033][00011/00051] (23.1%)  Loss=1.0427  cls_loss=0.6900  reg_loss=0.3527  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1506s  iter_time=2.326s
2025-08-06 08:51:49 Train INFO: [Train]: [033][00012/00051] (25.0%)  Loss=1.0343  cls_loss=0.6871  reg_loss=0.3472  lr_det=9.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1745s  iter_time=129.856s
2025-08-06 08:51:58 Train INFO: [Train]: [033][00013/00051] (26.9%)  Loss=1.0601  cls_loss=0.7047  reg_loss=0.3554  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1602s  iter_time=8.607s
2025-08-06 08:52:00 Train INFO: [Train]: [033][00014/00051] (28.8%)  Loss=1.0758  cls_loss=0.7161  reg_loss=0.3596  lr_det=9.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1462s  iter_time=2.305s
2025-08-06 08:52:03 Train INFO: [Train]: [033][00015/00051] (30.8%)  Loss=1.0758  cls_loss=0.7160  reg_loss=0.3598  lr_det=9.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1339s  iter_time=2.397s
2025-08-06 08:54:12 Train INFO: [Train]: [033][00016/00051] (32.7%)  Loss=1.0645  cls_loss=0.7095  reg_loss=0.3550  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1491s  iter_time=129.148s
2025-08-06 08:54:14 Train INFO: [Train]: [033][00017/00051] (34.6%)  Loss=1.0578  cls_loss=0.7039  reg_loss=0.3539  lr_det=9.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1372s  iter_time=2.342s
2025-08-06 08:54:16 Train INFO: [Train]: [033][00018/00051] (36.5%)  Loss=1.0479  cls_loss=0.6980  reg_loss=0.3499  lr_det=9.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1266s  iter_time=2.279s
2025-08-06 08:54:19 Train INFO: [Train]: [033][00019/00051] (38.5%)  Loss=1.0506  cls_loss=0.6987  reg_loss=0.3519  lr_det=9.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1170s  iter_time=2.348s
2025-08-06 08:56:29 Train INFO: [Train]: [033][00020/00051] (40.4%)  Loss=1.0361  cls_loss=0.6879  reg_loss=0.3482  lr_det=9.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1271s  iter_time=129.848s
2025-08-06 08:56:31 Train INFO: [Train]: [033][00021/00051] (42.3%)  Loss=1.0437  cls_loss=0.6939  reg_loss=0.3498  lr_det=9.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1177s  iter_time=2.292s
2025-08-06 08:56:36 Train INFO: [Train]: [033][00022/00051] (44.2%)  Loss=1.0519  cls_loss=0.6984  reg_loss=0.3535  lr_det=8.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1095s  iter_time=5.205s
2025-08-06 08:56:38 Train INFO: [Train]: [033][00023/00051] (46.2%)  Loss=1.0450  cls_loss=0.6933  reg_loss=0.3517  lr_det=8.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1016s  iter_time=2.316s
2025-08-06 08:58:51 Train INFO: [Train]: [033][00024/00051] (48.1%)  Loss=1.0479  cls_loss=0.6950  reg_loss=0.3529  lr_det=8.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1083s  iter_time=132.332s
2025-08-06 08:58:53 Train INFO: [Train]: [033][00025/00051] (50.0%)  Loss=1.0423  cls_loss=0.6919  reg_loss=0.3504  lr_det=8.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1005s  iter_time=2.261s
2025-08-06 08:58:57 Train INFO: [Train]: [033][00026/00051] (51.9%)  Loss=1.0453  cls_loss=0.6937  reg_loss=0.3516  lr_det=8.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=935s  iter_time=4.178s
2025-08-06 08:59:00 Train INFO: [Train]: [033][00027/00051] (53.8%)  Loss=1.0489  cls_loss=0.6952  reg_loss=0.3537  lr_det=8.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=867s  iter_time=2.286s
2025-08-06 09:01:07 Train INFO: [Train]: [033][00028/00051] (55.8%)  Loss=1.0486  cls_loss=0.6941  reg_loss=0.3546  lr_det=8.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=904s  iter_time=127.572s
2025-08-06 09:01:13 Train INFO: [Train]: [033][00029/00051] (57.7%)  Loss=1.0498  cls_loss=0.6949  reg_loss=0.3549  lr_det=8.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=840s  iter_time=5.907s
2025-08-06 09:01:15 Train INFO: [Train]: [033][00030/00051] (59.6%)  Loss=1.0502  cls_loss=0.6950  reg_loss=0.3552  lr_det=8.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=777s  iter_time=2.320s
2025-08-06 09:01:18 Train INFO: [Train]: [033][00031/00051] (61.5%)  Loss=1.0500  cls_loss=0.6951  reg_loss=0.3549  lr_det=8.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=719s  iter_time=2.300s
2025-08-06 09:03:29 Train INFO: [Train]: [033][00032/00051] (63.5%)  Loss=1.0496  cls_loss=0.6943  reg_loss=0.3553  lr_det=7.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=738s  iter_time=131.691s
2025-08-06 09:03:34 Train INFO: [Train]: [033][00033/00051] (65.4%)  Loss=1.0449  cls_loss=0.6912  reg_loss=0.3537  lr_det=7.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=681s  iter_time=4.839s
2025-08-06 09:03:38 Train INFO: [Train]: [033][00034/00051] (67.3%)  Loss=1.0484  cls_loss=0.6934  reg_loss=0.3549  lr_det=7.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=627s  iter_time=3.739s
2025-08-06 09:03:41 Train INFO: [Train]: [033][00035/00051] (69.2%)  Loss=1.0513  cls_loss=0.6947  reg_loss=0.3567  lr_det=7.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=575s  iter_time=2.714s
2025-08-06 09:05:48 Train INFO: [Train]: [033][00036/00051] (71.2%)  Loss=1.0496  cls_loss=0.6938  reg_loss=0.3558  lr_det=7.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=576s  iter_time=127.068s
2025-08-06 09:05:50 Train INFO: [Train]: [033][00037/00051] (73.1%)  Loss=1.0477  cls_loss=0.6925  reg_loss=0.3552  lr_det=7.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=524s  iter_time=2.362s
2025-08-06 09:05:57 Train INFO: [Train]: [033][00038/00051] (75.0%)  Loss=1.0476  cls_loss=0.6925  reg_loss=0.3551  lr_det=7.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=477s  iter_time=7.443s
2025-08-06 09:06:00 Train INFO: [Train]: [033][00039/00051] (76.9%)  Loss=1.0475  cls_loss=0.6923  reg_loss=0.3552  lr_det=6.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=430s  iter_time=2.382s
2025-08-06 09:08:06 Train INFO: [Train]: [033][00040/00051] (78.8%)  Loss=1.0443  cls_loss=0.6900  reg_loss=0.3543  lr_det=6.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=418s  iter_time=126.162s
2025-08-06 09:08:08 Train INFO: [Train]: [033][00041/00051] (80.8%)  Loss=1.0427  cls_loss=0.6890  reg_loss=0.3536  lr_det=6.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=372s  iter_time=2.371s
2025-08-06 09:08:13 Train INFO: [Train]: [033][00042/00051] (82.7%)  Loss=1.0498  cls_loss=0.6932  reg_loss=0.3567  lr_det=6.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=328s  iter_time=4.333s
2025-08-06 09:08:15 Train INFO: [Train]: [033][00043/00051] (84.6%)  Loss=1.0494  cls_loss=0.6925  reg_loss=0.3569  lr_det=6.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=285s  iter_time=2.370s
2025-08-06 09:10:31 Train INFO: [Train]: [033][00044/00051] (86.5%)  Loss=1.0480  cls_loss=0.6912  reg_loss=0.3567  lr_det=6.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=265s  iter_time=135.637s
2025-08-06 09:10:33 Train INFO: [Train]: [033][00045/00051] (88.5%)  Loss=1.0445  cls_loss=0.6893  reg_loss=0.3552  lr_det=6.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=222s  iter_time=2.295s
2025-08-06 09:10:36 Train INFO: [Train]: [033][00046/00051] (90.4%)  Loss=1.0422  cls_loss=0.6879  reg_loss=0.3543  lr_det=5.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=182s  iter_time=2.888s
2025-08-06 09:10:38 Train INFO: [Train]: [033][00047/00051] (92.3%)  Loss=1.0394  cls_loss=0.6860  reg_loss=0.3534  lr_det=5.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=143s  iter_time=2.329s
2025-08-06 09:12:47 Train INFO: [Train]: [033][00048/00051] (94.2%)  Loss=1.0352  cls_loss=0.6838  reg_loss=0.3513  lr_det=5.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=113s  iter_time=128.931s
2025-08-06 09:12:49 Train INFO: [Train]: [033][00049/00051] (96.2%)  Loss=1.0323  cls_loss=0.6822  reg_loss=0.3501  lr_det=5.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=74s  iter_time=2.183s
2025-08-06 09:12:52 Train INFO: [Train]: [033][00050/00051] (98.1%)  Loss=1.0323  cls_loss=0.6825  reg_loss=0.3498  lr_det=5.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=36s  iter_time=2.189s
2025-08-06 09:12:54 Train INFO: [Train]: [033][00051/00051] (100.0%)  Loss=1.0304  cls_loss=0.6815  reg_loss=0.3489  lr_det=5.2e-05  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.066s
2025-08-06 09:12:54 Train INFO: [Train]: Epoch 33 completed in 1846.7s (avg 35.514s/iter)
2025-08-06 09:12:54 Train INFO: [Train]: Final Loss=1.0304
2025-08-06 09:12:54 Train INFO: [Train]: Epoch 34 started (Total iterations: 52)
2025-08-06 09:15:33 Train INFO: [Train]: [034][00001/00051] (3.8%)  Loss=1.0155  cls_loss=0.6794  reg_loss=0.3360  lr_det=4.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=3978s  iter_time=159.102s  fwd=2.148s/bwd=0.085s/opt=0.022s
2025-08-06 09:15:36 Train INFO: [Train]: [034][00002/00051] (5.8%)  Loss=1.0663  cls_loss=0.7073  reg_loss=0.3590  lr_det=4.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2638s  iter_time=2.382s
2025-08-06 09:15:38 Train INFO: [Train]: [034][00003/00051] (7.7%)  Loss=1.0743  cls_loss=0.7130  reg_loss=0.3613  lr_det=4.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1965s  iter_time=2.233s
2025-08-06 09:17:54 Train INFO: [Train]: [034][00004/00051] (9.6%)  Loss=1.0328  cls_loss=0.6825  reg_loss=0.3503  lr_det=4.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2817s  iter_time=135.917s
2025-08-06 09:17:56 Train INFO: [Train]: [034][00005/00051] (11.5%)  Loss=1.0537  cls_loss=0.6964  reg_loss=0.3573  lr_det=4.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2315s  iter_time=2.374s
2025-08-06 09:17:59 Train INFO: [Train]: [034][00006/00051] (13.5%)  Loss=1.0488  cls_loss=0.6944  reg_loss=0.3544  lr_det=4.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1957s  iter_time=2.342s
2025-08-06 09:18:01 Train INFO: [Train]: [034][00007/00051] (15.4%)  Loss=1.0634  cls_loss=0.7058  reg_loss=0.3577  lr_det=4.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1687s  iter_time=2.278s
2025-08-06 09:20:07 Train INFO: [Train]: [034][00008/00051] (17.3%)  Loss=1.0440  cls_loss=0.6938  reg_loss=0.3503  lr_det=3.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2065s  iter_time=125.595s
2025-08-06 09:20:16 Train INFO: [Train]: [034][00009/00051] (19.2%)  Loss=1.0433  cls_loss=0.6924  reg_loss=0.3509  lr_det=3.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1854s  iter_time=9.184s
2025-08-06 09:20:18 Train INFO: [Train]: [034][00010/00051] (21.2%)  Loss=1.0334  cls_loss=0.6856  reg_loss=0.3477  lr_det=3.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1654s  iter_time=2.460s
2025-08-06 09:20:21 Train INFO: [Train]: [034][00011/00051] (23.1%)  Loss=1.0460  cls_loss=0.6955  reg_loss=0.3506  lr_det=3.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1488s  iter_time=2.410s
2025-08-06 09:22:45 Train INFO: [Train]: [034][00012/00051] (25.0%)  Loss=1.0344  cls_loss=0.6892  reg_loss=0.3452  lr_det=3.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1773s  iter_time=144.665s
2025-08-06 09:22:58 Train INFO: [Train]: [034][00013/00051] (26.9%)  Loss=1.0483  cls_loss=0.6978  reg_loss=0.3505  lr_det=3.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1640s  iter_time=13.169s
2025-08-06 09:23:01 Train INFO: [Train]: [034][00014/00051] (28.8%)  Loss=1.0442  cls_loss=0.6958  reg_loss=0.3484  lr_det=2.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1496s  iter_time=2.350s
2025-08-06 09:23:03 Train INFO: [Train]: [034][00015/00051] (30.8%)  Loss=1.0377  cls_loss=0.6926  reg_loss=0.3452  lr_det=2.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1370s  iter_time=2.375s
2025-08-06 09:25:35 Train INFO: [Train]: [034][00016/00051] (32.7%)  Loss=1.0415  cls_loss=0.6953  reg_loss=0.3462  lr_det=2.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1565s  iter_time=151.377s
2025-08-06 09:25:39 Train INFO: [Train]: [034][00017/00051] (34.6%)  Loss=1.0374  cls_loss=0.6925  reg_loss=0.3449  lr_det=2.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1444s  iter_time=4.474s
2025-08-06 09:25:42 Train INFO: [Train]: [034][00018/00051] (36.5%)  Loss=1.0362  cls_loss=0.6914  reg_loss=0.3448  lr_det=2.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1332s  iter_time=2.462s
2025-08-06 09:25:44 Train INFO: [Train]: [034][00019/00051] (38.5%)  Loss=1.0392  cls_loss=0.6938  reg_loss=0.3454  lr_det=2.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1231s  iter_time=2.427s
2025-08-06 09:28:07 Train INFO: [Train]: [034][00020/00051] (40.4%)  Loss=1.0277  cls_loss=0.6862  reg_loss=0.3416  lr_det=2.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1347s  iter_time=142.575s
2025-08-06 09:28:09 Train INFO: [Train]: [034][00021/00051] (42.3%)  Loss=1.0346  cls_loss=0.6903  reg_loss=0.3444  lr_det=2.0e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1247s  iter_time=2.341s
2025-08-06 09:28:12 Train INFO: [Train]: [034][00022/00051] (44.2%)  Loss=1.0423  cls_loss=0.6950  reg_loss=0.3474  lr_det=1.9e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1158s  iter_time=3.510s
2025-08-06 09:28:15 Train INFO: [Train]: [034][00023/00051] (46.2%)  Loss=1.0396  cls_loss=0.6930  reg_loss=0.3466  lr_det=1.8e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1074s  iter_time=2.462s
2025-08-06 09:30:26 Train INFO: [Train]: [034][00024/00051] (48.1%)  Loss=1.0561  cls_loss=0.7037  reg_loss=0.3525  lr_det=1.7e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1136s  iter_time=131.486s
2025-08-06 09:30:32 Train INFO: [Train]: [034][00025/00051] (50.0%)  Loss=1.0558  cls_loss=0.7031  reg_loss=0.3527  lr_det=1.6e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1058s  iter_time=5.920s
2025-08-06 09:30:35 Train INFO: [Train]: [034][00026/00051] (51.9%)  Loss=1.0551  cls_loss=0.7020  reg_loss=0.3531  lr_det=1.5e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=982s  iter_time=2.356s
2025-08-06 09:30:37 Train INFO: [Train]: [034][00027/00051] (53.8%)  Loss=1.0534  cls_loss=0.7007  reg_loss=0.3527  lr_det=1.4e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=911s  iter_time=2.333s
2025-08-06 09:32:44 Train INFO: [Train]: [034][00028/00051] (55.8%)  Loss=1.0478  cls_loss=0.6974  reg_loss=0.3504  lr_det=1.3e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=944s  iter_time=127.333s
2025-08-06 09:32:57 Train INFO: [Train]: [034][00029/00051] (57.7%)  Loss=1.0495  cls_loss=0.6986  reg_loss=0.3509  lr_det=1.2e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=882s  iter_time=12.704s
2025-08-06 09:32:59 Train INFO: [Train]: [034][00030/00051] (59.6%)  Loss=1.0481  cls_loss=0.6971  reg_loss=0.3510  lr_det=1.1e-05  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=816s  iter_time=2.410s
2025-08-06 09:33:02 Train INFO: [Train]: [034][00031/00051] (61.5%)  Loss=1.0475  cls_loss=0.6966  reg_loss=0.3509  lr_det=9.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=755s  iter_time=2.436s
2025-08-06 09:35:07 Train INFO: [Train]: [034][00032/00051] (63.5%)  Loss=1.0474  cls_loss=0.6970  reg_loss=0.3504  lr_det=8.9e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=767s  iter_time=125.355s
2025-08-06 09:35:19 Train INFO: [Train]: [034][00033/00051] (65.4%)  Loss=1.0401  cls_loss=0.6921  reg_loss=0.3480  lr_det=8.0e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=712s  iter_time=12.258s
2025-08-06 09:35:22 Train INFO: [Train]: [034][00034/00051] (67.3%)  Loss=1.0431  cls_loss=0.6942  reg_loss=0.3489  lr_det=7.2e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=655s  iter_time=2.490s
2025-08-06 09:35:24 Train INFO: [Train]: [034][00035/00051] (69.2%)  Loss=1.0383  cls_loss=0.6907  reg_loss=0.3477  lr_det=6.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=600s  iter_time=2.379s
2025-08-06 09:37:32 Train INFO: [Train]: [034][00036/00051] (71.2%)  Loss=1.0401  cls_loss=0.6920  reg_loss=0.3481  lr_det=5.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=599s  iter_time=127.871s
2025-08-06 09:37:44 Train INFO: [Train]: [034][00037/00051] (73.1%)  Loss=1.0371  cls_loss=0.6900  reg_loss=0.3470  lr_det=5.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=549s  iter_time=11.610s
2025-08-06 09:37:46 Train INFO: [Train]: [034][00038/00051] (75.0%)  Loss=1.0354  cls_loss=0.6888  reg_loss=0.3466  lr_det=4.4e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=497s  iter_time=2.357s
2025-08-06 09:37:49 Train INFO: [Train]: [034][00039/00051] (76.9%)  Loss=1.0338  cls_loss=0.6875  reg_loss=0.3463  lr_det=3.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=448s  iter_time=2.424s
2025-08-06 09:39:49 Train INFO: [Train]: [034][00040/00051] (78.8%)  Loss=1.0319  cls_loss=0.6859  reg_loss=0.3460  lr_det=3.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=433s  iter_time=120.155s
2025-08-06 09:40:06 Train INFO: [Train]: [034][00041/00051] (80.8%)  Loss=1.0268  cls_loss=0.6827  reg_loss=0.3441  lr_det=2.7e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=389s  iter_time=17.376s
2025-08-06 09:40:09 Train INFO: [Train]: [034][00042/00051] (82.7%)  Loss=1.0244  cls_loss=0.6816  reg_loss=0.3428  lr_det=2.3e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=342s  iter_time=2.387s
2025-08-06 09:40:11 Train INFO: [Train]: [034][00043/00051] (84.6%)  Loss=1.0212  cls_loss=0.6793  reg_loss=0.3420  lr_det=1.8e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=298s  iter_time=2.263s
2025-08-06 09:42:09 Train INFO: [Train]: [034][00044/00051] (86.5%)  Loss=1.0182  cls_loss=0.6774  reg_loss=0.3408  lr_det=1.5e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=273s  iter_time=118.665s
2025-08-06 09:42:26 Train INFO: [Train]: [034][00045/00051] (88.5%)  Loss=1.0154  cls_loss=0.6753  reg_loss=0.3401  lr_det=1.1e-06  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=231s  iter_time=16.820s
2025-08-06 09:42:29 Train INFO: [Train]: [034][00046/00051] (90.4%)  Loss=1.0160  cls_loss=0.6759  reg_loss=0.3402  lr_det=8.3e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=189s  iter_time=2.378s
2025-08-06 09:42:31 Train INFO: [Train]: [034][00047/00051] (92.3%)  Loss=1.0171  cls_loss=0.6765  reg_loss=0.3406  lr_det=5.8e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=148s  iter_time=2.334s
2025-08-06 09:44:31 Train INFO: [Train]: [034][00048/00051] (94.2%)  Loss=1.0182  cls_loss=0.6766  reg_loss=0.3416  lr_det=3.7e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=116s  iter_time=120.026s
2025-08-06 09:44:43 Train INFO: [Train]: [034][00049/00051] (96.2%)  Loss=1.0180  cls_loss=0.6768  reg_loss=0.3413  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=76s  iter_time=11.796s
2025-08-06 09:44:45 Train INFO: [Train]: [034][00050/00051] (98.1%)  Loss=1.0206  cls_loss=0.6781  reg_loss=0.3425  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=37s  iter_time=2.176s
2025-08-06 09:44:47 Train INFO: [Train]: [034][00051/00051] (100.0%)  Loss=1.0175  cls_loss=0.6760  reg_loss=0.3415  lr_det=3.3e-08  GPU=1392MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=0s  iter_time=2.059s
2025-08-06 09:44:48 Train INFO: [Train]: Epoch 34 completed in 1913.4s (avg 36.796s/iter)
2025-08-06 09:44:48 Train INFO: [Train]: Final Loss=1.0175
2025-08-06 09:44:48 Train INFO: [Val]: Epoch 34 Loss
2025-08-06 10:04:44 Train INFO: [Val]: [034]  Loss=1.4969  cls_loss=1.1160  reg_loss=0.3809  Average-mAP=0.27%
2025-08-06 10:04:46 Train INFO: Checkpoint saved at epoch 34
2025-08-06 10:04:46 Train INFO: [Train]: Epoch 35 started (Total iterations: 52)
2025-08-06 10:07:28 Train INFO: [Train]: [035][00001/00051] (3.8%)  Loss=1.0819  cls_loss=0.7031  reg_loss=0.3788  lr_det=3.3e-08  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=4040s  iter_time=161.606s  fwd=2.370s/bwd=0.065s/opt=0.016s
2025-08-06 10:07:30 Train INFO: [Train]: [035][00002/00051] (5.8%)  Loss=1.1295  cls_loss=0.7367  reg_loss=0.3928  lr_det=1.0e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=2678s  iter_time=2.343s
2025-08-06 10:07:32 Train INFO: [Train]: [035][00003/00051] (7.7%)  Loss=1.0690  cls_loss=0.6993  reg_loss=0.3696  lr_det=2.2e-07  GPU=1428MB(alloc)/12826MB(reserved)/9268MB(max)  ETA=1996s  iter_time=2.393s
2025-08-06 10:28:21 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-06 10:28:22 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
load_from = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_13.pth'
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
resume = True
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=1,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-06 10:28:22 Train INFO: training subset: 831 videos
2025-08-06 10:28:22 Train INFO: validation subset: 111 videos, truncated as 454 windows.
2025-08-06 10:28:22 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-08-06 10:28:23 Train INFO: Using single GPU training...
2025-08-06 10:28:23 Train INFO: Using Model EMA...
2025-08-06 10:28:23 Train INFO: Using Automatic Mixed Precision...
2025-08-06 10:28:23 Train INFO: GPU Memory: 24.0 GB
2025-08-06 10:28:23 Train INFO: Freeze the backbone...
2025-08-06 10:28:23 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_29.pth
2025-08-06 10:28:24 Train INFO: Resume epoch is 29
2025-08-06 10:28:24 Train INFO: Training Starts...

2025-08-06 10:28:24 Train INFO: [Train]: Epoch 30 started (Total iterations: 52)
2025-08-06 10:31:38 Train INFO: [Train]: [030][00001/00051] (3.8%)  Loss=4.3980  cls_loss=3.2673  reg_loss=1.1307  lr_det=4.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=4842s  iter_time=193.693s  fwd=2.334s/bwd=0.086s/opt=0.060s
2025-08-06 10:31:41 Train INFO: [Train]: [030][00002/00051] (5.8%)  Loss=4.0747  cls_loss=3.0130  reg_loss=1.0617  lr_det=4.7e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=3203s  iter_time=2.390s
2025-08-06 10:31:43 Train INFO: [Train]: [030][00003/00051] (7.7%)  Loss=3.7138  cls_loss=2.7443  reg_loss=0.9695  lr_det=4.5e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2381s  iter_time=2.321s
2025-08-06 10:34:12 Train INFO: [Train]: [030][00004/00051] (9.6%)  Loss=3.4607  cls_loss=2.5437  reg_loss=0.9170  lr_det=4.4e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=3267s  iter_time=149.133s
2025-08-06 10:34:21 Train INFO: [Train]: [030][00005/00051] (11.5%)  Loss=3.2643  cls_loss=2.3954  reg_loss=0.8689  lr_det=4.2e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2736s  iter_time=9.331s
2025-08-06 10:34:24 Train INFO: [Train]: [030][00006/00051] (13.5%)  Loss=3.0865  cls_loss=2.2588  reg_loss=0.8277  lr_det=4.1e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2310s  iter_time=2.473s
2025-08-06 10:34:26 Train INFO: [Train]: [030][00007/00051] (15.4%)  Loss=2.9072  cls_loss=2.1202  reg_loss=0.7871  lr_det=4.0e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1990s  iter_time=2.425s
2025-08-06 10:36:52 Train INFO: [Train]: [030][00008/00051] (17.3%)  Loss=2.7588  cls_loss=2.0070  reg_loss=0.7518  lr_det=3.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2425s  iter_time=145.735s
2025-08-06 10:37:06 Train INFO: [Train]: [030][00009/00051] (19.2%)  Loss=2.6361  cls_loss=1.9130  reg_loss=0.7231  lr_det=3.7e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2189s  iter_time=13.718s
2025-08-06 10:37:09 Train INFO: [Train]: [030][00010/00051] (21.2%)  Loss=2.5383  cls_loss=1.8369  reg_loss=0.7014  lr_det=3.5e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1955s  iter_time=3.397s
2025-08-06 10:37:12 Train INFO: [Train]: [030][00011/00051] (23.1%)  Loss=2.4628  cls_loss=1.7775  reg_loss=0.6853  lr_det=3.4e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1757s  iter_time=2.595s
2025-08-06 10:39:47 Train INFO: [Train]: [030][00012/00051] (25.0%)  Loss=2.3902  cls_loss=1.7223  reg_loss=0.6679  lr_det=3.2e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=2049s  iter_time=155.793s
2025-08-06 10:40:04 Train INFO: [Train]: [030][00013/00051] (26.9%)  Loss=2.3271  cls_loss=1.6754  reg_loss=0.6518  lr_det=3.1e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1898s  iter_time=16.312s
2025-08-06 10:40:06 Train INFO: [Train]: [030][00014/00051] (28.8%)  Loss=2.2691  cls_loss=1.6308  reg_loss=0.6383  lr_det=2.9e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1731s  iter_time=2.470s
2025-08-06 10:40:09 Train INFO: [Train]: [030][00015/00051] (30.8%)  Loss=2.2131  cls_loss=1.5887  reg_loss=0.6244  lr_det=2.8e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1585s  iter_time=2.469s
2025-08-06 10:42:46 Train INFO: [Train]: [030][00016/00051] (32.7%)  Loss=2.1631  cls_loss=1.5516  reg_loss=0.6115  lr_det=2.7e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1774s  iter_time=157.442s
2025-08-06 10:42:54 Train INFO: [Train]: [030][00017/00051] (34.6%)  Loss=2.1198  cls_loss=1.5178  reg_loss=0.6020  lr_det=2.5e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1642s  iter_time=7.452s
2025-08-06 10:42:56 Train INFO: [Train]: [030][00018/00051] (36.5%)  Loss=2.0722  cls_loss=1.4822  reg_loss=0.5900  lr_det=2.4e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1514s  iter_time=2.275s
2025-08-06 10:42:58 Train INFO: [Train]: [030][00019/00051] (38.5%)  Loss=2.0341  cls_loss=1.4529  reg_loss=0.5813  lr_det=2.3e-05  GPU=1428MB(alloc)/11036MB(reserved)/9268MB(max)  ETA=1398s  iter_time=2.235s
2025-08-06 10:51:45 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-06 10:51:45 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 64
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=2,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=2,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=2,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 2
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=5,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-06 10:51:46 Train INFO: training subset: 831 videos
2025-08-06 10:51:46 Train INFO: validation subset: 111 videos, truncated as 968 windows.
2025-08-06 10:51:46 Train INFO: testing subset: 132 videos, truncated as 1158 windows.
2025-08-06 10:51:46 Train INFO: Using single GPU training...
2025-08-06 10:51:46 Train INFO: Using Model EMA...
2025-08-06 10:51:46 Train INFO: Using Automatic Mixed Precision...
2025-08-06 10:51:46 Train INFO: GPU Memory: 24.0 GB
2025-08-06 10:51:46 Train INFO: Freeze the backbone...
2025-08-06 10:51:46 Train INFO: Training Starts...

2025-08-06 10:51:46 Train INFO: Running sanity check with 1 validation steps...
2025-08-06 10:51:46 Train INFO: [Val]: Epoch -1 Loss
2025-08-06 10:54:18 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-06 10:54:18 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 64
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=2,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=2,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=2,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 2
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=5,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-06 10:54:19 Train INFO: training subset: 831 videos
2025-08-06 10:54:19 Train INFO: validation subset: 111 videos, truncated as 968 windows.
2025-08-06 10:54:19 Train INFO: testing subset: 132 videos, truncated as 1158 windows.
2025-08-06 10:54:19 Train INFO: Using single GPU training...
2025-08-06 10:54:19 Train INFO: Using Model EMA...
2025-08-06 10:54:19 Train INFO: Using Automatic Mixed Precision...
2025-08-06 10:54:19 Train INFO: GPU Memory: 24.0 GB
2025-08-06 10:54:19 Train INFO: Freeze the backbone...
2025-08-06 10:54:20 Train INFO: Training Starts...

2025-08-06 10:54:20 Train INFO: Running sanity check with 1 validation steps...
2025-08-06 10:54:20 Train INFO: [Val]: Epoch -1 Loss
2025-08-06 10:57:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-06 10:57:44 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 64
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=2,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=2,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=2,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=2,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 2
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=5,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-06 10:57:44 Train INFO: training subset: 831 videos
2025-08-06 10:57:44 Train INFO: validation subset: 111 videos, truncated as 968 windows.
2025-08-06 10:57:44 Train INFO: testing subset: 132 videos, truncated as 1158 windows.
2025-08-06 10:57:45 Train INFO: Using single GPU training...
2025-08-06 10:57:45 Train INFO: Using Model EMA...
2025-08-06 10:57:45 Train INFO: Using Automatic Mixed Precision...
2025-08-06 10:57:45 Train INFO: GPU Memory: 24.0 GB
2025-08-06 10:57:45 Train INFO: Freeze the backbone...
2025-08-06 10:57:45 Train INFO: Training Starts...

2025-08-06 10:57:45 Train INFO: Running sanity check with 1 validation steps...
2025-08-06 10:57:45 Train INFO: [Val]: Epoch -1 Loss
2025-08-06 11:05:32 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-06 11:05:33 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=5,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-06 11:05:33 Train INFO: training subset: 831 videos
2025-08-06 11:05:33 Train INFO: validation subset: 111 videos, truncated as 1958 windows.
2025-08-06 11:05:33 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-08-06 11:05:34 Train INFO: Using single GPU training...
2025-08-06 11:05:34 Train INFO: Using Model EMA...
2025-08-06 11:05:34 Train INFO: Using Automatic Mixed Precision...
2025-08-06 11:05:34 Train INFO: GPU Memory: 24.0 GB
2025-08-06 11:05:34 Train INFO: Freeze the backbone...
2025-08-06 11:05:34 Train INFO: Training Starts...

2025-08-06 11:05:34 Train INFO: Running sanity check with 1 validation steps...
2025-08-06 11:05:34 Train INFO: [Val]: Epoch -1 Loss
2025-08-06 11:07:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-08-06 11:07:05 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    prediction_filename=
    'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/predictions_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=1)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=5,
    end_epoch=120,
    logging_interval=5,
    num_sanity_check=1,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=5)

2025-08-06 11:07:06 Train INFO: training subset: 831 videos
2025-08-06 11:07:06 Train INFO: validation subset: 111 videos, truncated as 1958 windows.
2025-08-06 11:07:06 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-08-06 11:07:06 Train INFO: Using single GPU training...
2025-08-06 11:07:06 Train INFO: Using Model EMA...
2025-08-06 11:07:06 Train INFO: Using Automatic Mixed Precision...
2025-08-06 11:07:06 Train INFO: GPU Memory: 24.0 GB
2025-08-06 11:07:06 Train INFO: Freeze the backbone...
2025-08-06 11:07:06 Train INFO: Training Starts...

2025-08-06 11:07:06 Train INFO: Running sanity check with 1 validation steps...
2025-08-06 11:07:06 Train INFO: [Val]: Epoch -1 Loss
2025-08-06 11:57:59 Train INFO: [Val]: [-01]  Loss=2.1370  cls_loss=1.1736  reg_loss=0.9634  Average-mAP=0.00%
2025-08-06 11:58:00 Train INFO: Sanity check completed.

2025-08-06 11:58:00 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-08-06 12:01:37 Train INFO: [Train]: [000][00005/00051] (11.5%)  Loss=1.7497  cls_loss=0.9690  reg_loss=0.7807  lr_det=9.8e-06  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1658s  iter_time=216.279s  fwd=2.112s/bwd=0.039s/opt=0.009s
2025-08-06 12:03:17 Train INFO: [Train]: [000][00010/00051] (21.2%)  Loss=1.6168  cls_loss=0.9690  reg_loss=0.6478  lr_det=2.0e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1182s  iter_time=100.743s
2025-08-06 12:05:02 Train INFO: [Train]: [000][00015/00051] (30.8%)  Loss=1.5485  cls_loss=0.9809  reg_loss=0.5676  lr_det=2.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=948s  iter_time=104.194s
2025-08-06 12:07:50 Train INFO: [Train]: [000][00020/00051] (40.4%)  Loss=1.4954  cls_loss=0.9724  reg_loss=0.5230  lr_det=3.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=870s  iter_time=168.223s
2025-08-06 12:09:23 Train INFO: [Train]: [000][00025/00051] (50.0%)  Loss=1.4741  cls_loss=0.9762  reg_loss=0.4979  lr_det=4.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=682s  iter_time=92.959s
2025-08-06 12:11:11 Train INFO: [Train]: [000][00030/00051] (59.6%)  Loss=1.4545  cls_loss=0.9763  reg_loss=0.4782  lr_det=5.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=536s  iter_time=108.122s
2025-08-06 12:12:47 Train INFO: [Train]: [000][00035/00051] (69.2%)  Loss=1.4398  cls_loss=0.9747  reg_loss=0.4651  lr_det=6.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=394s  iter_time=96.529s
2025-08-06 12:15:47 Train INFO: [Train]: [000][00040/00051] (78.8%)  Loss=1.4223  cls_loss=0.9698  reg_loss=0.4524  lr_det=7.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=286s  iter_time=179.485s
2025-08-06 12:17:35 Train INFO: [Train]: [000][00045/00051] (88.5%)  Loss=1.4084  cls_loss=0.9657  reg_loss=0.4427  lr_det=8.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=153s  iter_time=107.951s
2025-08-06 12:19:14 Train INFO: [Train]: [000][00050/00051] (98.1%)  Loss=1.3950  cls_loss=0.9615  reg_loss=0.4336  lr_det=9.8e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=25s  iter_time=98.725s
2025-08-06 12:19:18 Train INFO: [Train]: [000][00051/00051] (100.0%)  Loss=1.3919  cls_loss=0.9600  reg_loss=0.4318  lr_det=1.0e-04  GPU=1393MB(alloc)/4588MB(reserved)/9292MB(max)  ETA=0s  iter_time=4.419s
2025-08-06 12:19:19 Train INFO: [Train]: Epoch 0 completed in 1278.5s (avg 24.586s/iter)
2025-08-06 12:19:19 Train INFO: [Train]: Final Loss=1.3919
2025-08-06 12:19:19 Train INFO: [Train]: Epoch 1 started (Total iterations: 52)
2025-08-06 12:22:48 Train INFO: [Train]: [001][00005/00051] (11.5%)  Loss=1.3147  cls_loss=0.9459  reg_loss=0.3688  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1606s  iter_time=209.511s  fwd=2.216s/bwd=0.086s/opt=0.012s
2025-08-06 12:24:28 Train INFO: [Train]: [001][00010/00051] (21.2%)  Loss=1.3617  cls_loss=0.9802  reg_loss=0.3815  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1154s  iter_time=100.076s
2025-08-06 12:26:04 Train INFO: [Train]: [001][00015/00051] (30.8%)  Loss=1.3497  cls_loss=0.9734  reg_loss=0.3764  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=911s  iter_time=95.365s
2025-08-06 12:29:06 Train INFO: [Train]: [001][00020/00051] (40.4%)  Loss=1.3331  cls_loss=0.9595  reg_loss=0.3736  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=866s  iter_time=181.899s
2025-08-06 12:30:43 Train INFO: [Train]: [001][00025/00051] (50.0%)  Loss=1.3286  cls_loss=0.9575  reg_loss=0.3711  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=684s  iter_time=97.528s
2025-08-06 12:32:20 Train INFO: [Train]: [001][00030/00051] (59.6%)  Loss=1.3211  cls_loss=0.9522  reg_loss=0.3689  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=529s  iter_time=96.406s
2025-08-06 12:33:52 Train INFO: [Train]: [001][00035/00051] (69.2%)  Loss=1.3248  cls_loss=0.9549  reg_loss=0.3698  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=388s  iter_time=92.256s
2025-08-06 12:36:53 Train INFO: [Train]: [001][00040/00051] (78.8%)  Loss=1.3247  cls_loss=0.9546  reg_loss=0.3701  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=283s  iter_time=180.704s
2025-08-06 12:38:30 Train INFO: [Train]: [001][00045/00051] (88.5%)  Loss=1.3128  cls_loss=0.9454  reg_loss=0.3674  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=150s  iter_time=97.612s
2025-08-06 12:40:00 Train INFO: [Train]: [001][00050/00051] (98.1%)  Loss=1.3051  cls_loss=0.9405  reg_loss=0.3647  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=24s  iter_time=89.873s
2025-08-06 12:40:02 Train INFO: [Train]: [001][00051/00051] (100.0%)  Loss=1.3033  cls_loss=0.9390  reg_loss=0.3643  lr_det=1.0e-04  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.020s
2025-08-06 12:40:03 Train INFO: [Train]: Epoch 1 completed in 1244.1s (avg 23.925s/iter)
2025-08-06 12:40:03 Train INFO: [Train]: Final Loss=1.3033
2025-08-06 12:40:03 Train INFO: [Train]: Epoch 2 started (Total iterations: 52)
2025-08-06 12:43:37 Train INFO: [Train]: [002][00005/00051] (11.5%)  Loss=1.2133  cls_loss=0.8610  reg_loss=0.3522  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1638s  iter_time=213.698s  fwd=2.259s/bwd=0.073s/opt=0.012s
2025-08-06 12:45:23 Train INFO: [Train]: [002][00010/00051] (21.2%)  Loss=1.2317  cls_loss=0.8655  reg_loss=0.3662  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1191s  iter_time=105.948s
2025-08-06 12:47:06 Train INFO: [Train]: [002][00015/00051] (30.8%)  Loss=1.2017  cls_loss=0.8422  reg_loss=0.3596  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=952s  iter_time=103.359s
2025-08-06 12:50:04 Train INFO: [Train]: [002][00020/00051] (40.4%)  Loss=1.1887  cls_loss=0.8297  reg_loss=0.3590  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=887s  iter_time=177.746s
2025-08-06 12:51:39 Train INFO: [Train]: [002][00025/00051] (50.0%)  Loss=1.1776  cls_loss=0.8191  reg_loss=0.3585  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=696s  iter_time=95.594s
2025-08-06 12:53:23 Train INFO: [Train]: [002][00030/00051] (59.6%)  Loss=1.1716  cls_loss=0.8137  reg_loss=0.3579  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=542s  iter_time=103.298s
2025-08-06 12:54:58 Train INFO: [Train]: [002][00035/00051] (69.2%)  Loss=1.1720  cls_loss=0.8105  reg_loss=0.3615  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=398s  iter_time=95.124s
2025-08-06 12:57:51 Train INFO: [Train]: [002][00040/00051] (78.8%)  Loss=1.1670  cls_loss=0.8055  reg_loss=0.3614  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=287s  iter_time=173.503s
2025-08-06 12:59:39 Train INFO: [Train]: [002][00045/00051] (88.5%)  Loss=1.1584  cls_loss=0.7981  reg_loss=0.3602  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=153s  iter_time=108.021s
2025-08-06 13:01:20 Train INFO: [Train]: [002][00050/00051] (98.1%)  Loss=1.1534  cls_loss=0.7930  reg_loss=0.3604  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=25s  iter_time=100.878s
2025-08-06 13:01:22 Train INFO: [Train]: [002][00051/00051] (100.0%)  Loss=1.1507  cls_loss=0.7911  reg_loss=0.3597  lr_det=1.0e-04  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.028s
2025-08-06 13:01:23 Train INFO: [Train]: Epoch 2 completed in 1279.9s (avg 24.613s/iter)
2025-08-06 13:01:23 Train INFO: [Train]: Final Loss=1.1507
2025-08-06 13:01:23 Train INFO: [Train]: Epoch 3 started (Total iterations: 52)
2025-08-06 13:05:07 Train INFO: [Train]: [003][00005/00051] (11.5%)  Loss=1.0829  cls_loss=0.7295  reg_loss=0.3534  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1718s  iter_time=224.147s  fwd=2.255s/bwd=0.092s/opt=0.013s
2025-08-06 13:06:45 Train INFO: [Train]: [003][00010/00051] (21.2%)  Loss=1.0873  cls_loss=0.7388  reg_loss=0.3485  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1201s  iter_time=98.041s
2025-08-06 13:08:29 Train INFO: [Train]: [003][00015/00051] (30.8%)  Loss=1.0910  cls_loss=0.7439  reg_loss=0.3471  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=959s  iter_time=103.833s
2025-08-06 13:11:18 Train INFO: [Train]: [003][00020/00051] (40.4%)  Loss=1.1106  cls_loss=0.7553  reg_loss=0.3553  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=879s  iter_time=169.426s
2025-08-06 13:13:06 Train INFO: [Train]: [003][00025/00051] (50.0%)  Loss=1.1010  cls_loss=0.7484  reg_loss=0.3526  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=703s  iter_time=107.365s
2025-08-06 13:14:47 Train INFO: [Train]: [003][00030/00051] (59.6%)  Loss=1.1044  cls_loss=0.7502  reg_loss=0.3542  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=545s  iter_time=101.533s
2025-08-06 13:16:26 Train INFO: [Train]: [003][00035/00051] (69.2%)  Loss=1.1067  cls_loss=0.7505  reg_loss=0.3561  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=401s  iter_time=98.577s
2025-08-06 13:19:07 Train INFO: [Train]: [003][00040/00051] (78.8%)  Loss=1.0980  cls_loss=0.7448  reg_loss=0.3532  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=285s  iter_time=160.938s
2025-08-06 13:21:04 Train INFO: [Train]: [003][00045/00051] (88.5%)  Loss=1.0978  cls_loss=0.7447  reg_loss=0.3531  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=154s  iter_time=117.348s
2025-08-06 13:22:37 Train INFO: [Train]: [003][00050/00051] (98.1%)  Loss=1.0965  cls_loss=0.7433  reg_loss=0.3532  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=25s  iter_time=93.137s
2025-08-06 13:22:39 Train INFO: [Train]: [003][00051/00051] (100.0%)  Loss=1.0934  cls_loss=0.7410  reg_loss=0.3524  lr_det=1.0e-04  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.028s
2025-08-06 13:22:40 Train INFO: [Train]: Epoch 3 completed in 1277.1s (avg 24.560s/iter)
2025-08-06 13:22:40 Train INFO: [Train]: Final Loss=1.0934
2025-08-06 13:22:40 Train INFO: [Train]: Epoch 4 started (Total iterations: 52)
2025-08-06 13:26:21 Train INFO: [Train]: [004][00005/00051] (11.5%)  Loss=1.1048  cls_loss=0.7539  reg_loss=0.3510  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1698s  iter_time=221.533s  fwd=2.108s/bwd=0.037s/opt=0.009s
2025-08-06 13:28:07 Train INFO: [Train]: [004][00010/00051] (21.2%)  Loss=1.0713  cls_loss=0.7337  reg_loss=0.3376  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1219s  iter_time=105.497s
2025-08-06 13:29:45 Train INFO: [Train]: [004][00015/00051] (30.8%)  Loss=1.0850  cls_loss=0.7371  reg_loss=0.3479  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=956s  iter_time=97.878s
2025-08-06 13:32:53 Train INFO: [Train]: [004][00020/00051] (40.4%)  Loss=1.0898  cls_loss=0.7401  reg_loss=0.3496  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=905s  iter_time=188.046s
2025-08-06 13:35:55 Train INFO: [Train]: [004][00025/00051] (50.0%)  Loss=1.0853  cls_loss=0.7368  reg_loss=0.3486  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=795s  iter_time=182.058s
2025-08-06 13:39:32 Train INFO: [Train]: [004][00030/00051] (59.6%)  Loss=1.0830  cls_loss=0.7359  reg_loss=0.3471  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=686s  iter_time=217.501s
2025-08-06 13:41:15 Train INFO: [Train]: [004][00035/00051] (69.2%)  Loss=1.0886  cls_loss=0.7383  reg_loss=0.3503  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=496s  iter_time=102.553s
2025-08-06 13:44:31 Train INFO: [Train]: [004][00040/00051] (78.8%)  Loss=1.0900  cls_loss=0.7393  reg_loss=0.3507  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=352s  iter_time=195.943s
2025-08-06 13:46:18 Train INFO: [Train]: [004][00045/00051] (88.5%)  Loss=1.0894  cls_loss=0.7381  reg_loss=0.3513  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=185s  iter_time=107.181s
2025-08-06 13:48:11 Train INFO: [Train]: [004][00050/00051] (98.1%)  Loss=1.0924  cls_loss=0.7404  reg_loss=0.3520  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=30s  iter_time=112.829s
2025-08-06 13:48:13 Train INFO: [Train]: [004][00051/00051] (100.0%)  Loss=1.0920  cls_loss=0.7400  reg_loss=0.3520  lr_det=1.0e-04  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.163s
2025-08-06 13:48:14 Train INFO: [Train]: Epoch 4 completed in 1533.9s (avg 29.499s/iter)
2025-08-06 13:48:14 Train INFO: [Train]: Final Loss=1.0920
2025-08-06 13:48:15 Train INFO: Checkpoint saved at epoch 4
2025-08-06 13:48:15 Train INFO: [Train]: Epoch 5 started (Total iterations: 52)
2025-08-06 13:52:02 Train INFO: [Train]: [005][00005/00051] (11.5%)  Loss=1.0718  cls_loss=0.7245  reg_loss=0.3473  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1742s  iter_time=227.266s  fwd=2.396s/bwd=0.078s/opt=0.013s
2025-08-06 13:53:42 Train INFO: [Train]: [005][00010/00051] (21.2%)  Loss=1.0592  cls_loss=0.7139  reg_loss=0.3453  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1221s  iter_time=100.254s
2025-08-06 13:55:22 Train INFO: [Train]: [005][00015/00051] (30.8%)  Loss=1.0915  cls_loss=0.7353  reg_loss=0.3562  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=962s  iter_time=100.176s
2025-08-06 13:58:37 Train INFO: [Train]: [005][00020/00051] (40.4%)  Loss=1.0822  cls_loss=0.7303  reg_loss=0.3519  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=919s  iter_time=194.514s
2025-08-06 14:00:25 Train INFO: [Train]: [005][00025/00051] (50.0%)  Loss=1.0856  cls_loss=0.7342  reg_loss=0.3514  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=730s  iter_time=107.910s
2025-08-06 14:02:07 Train INFO: [Train]: [005][00030/00051] (59.6%)  Loss=1.0766  cls_loss=0.7288  reg_loss=0.3478  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=564s  iter_time=102.574s
2025-08-06 14:03:47 Train INFO: [Train]: [005][00035/00051] (69.2%)  Loss=1.0764  cls_loss=0.7282  reg_loss=0.3482  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=415s  iter_time=99.926s
2025-08-06 14:06:49 Train INFO: [Train]: [005][00040/00051] (78.8%)  Loss=1.0716  cls_loss=0.7255  reg_loss=0.3462  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=299s  iter_time=182.107s
2025-08-06 14:08:32 Train INFO: [Train]: [005][00045/00051] (88.5%)  Loss=1.0831  cls_loss=0.7341  reg_loss=0.3490  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=159s  iter_time=102.665s
2025-08-06 14:10:05 Train INFO: [Train]: [005][00050/00051] (98.1%)  Loss=1.0844  cls_loss=0.7349  reg_loss=0.3495  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=26s  iter_time=92.459s
2025-08-06 14:10:07 Train INFO: [Train]: [005][00051/00051] (100.0%)  Loss=1.0820  cls_loss=0.7333  reg_loss=0.3487  lr_det=1.0e-04  GPU=1393MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=0s  iter_time=2.051s
2025-08-06 14:10:07 Train INFO: [Train]: Epoch 5 completed in 1312.6s (avg 25.242s/iter)
2025-08-06 14:10:07 Train INFO: [Train]: Final Loss=1.0820
2025-08-06 14:10:07 Train INFO: [Train]: Epoch 6 started (Total iterations: 52)
2025-08-06 14:13:46 Train INFO: [Train]: [006][00005/00051] (11.5%)  Loss=1.0751  cls_loss=0.7316  reg_loss=0.3435  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1676s  iter_time=218.670s  fwd=2.292s/bwd=0.083s/opt=0.012s
2025-08-06 14:15:20 Train INFO: [Train]: [006][00010/00051] (21.2%)  Loss=1.0892  cls_loss=0.7400  reg_loss=0.3492  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=1166s  iter_time=94.286s
2025-08-06 14:16:58 Train INFO: [Train]: [006][00015/00051] (30.8%)  Loss=1.0842  cls_loss=0.7352  reg_loss=0.3490  lr_det=1.0e-04  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=925s  iter_time=97.968s
2025-08-06 14:19:59 Train INFO: [Train]: [006][00020/00051] (40.4%)  Loss=1.0816  cls_loss=0.7318  reg_loss=0.3498  lr_det=9.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=874s  iter_time=180.970s
2025-08-06 14:21:36 Train INFO: [Train]: [006][00025/00051] (50.0%)  Loss=1.0784  cls_loss=0.7294  reg_loss=0.3491  lr_det=9.9e-05  GPU=1430MB(alloc)/11824MB(reserved)/9292MB(max)  ETA=689s  iter_time=97.293s
