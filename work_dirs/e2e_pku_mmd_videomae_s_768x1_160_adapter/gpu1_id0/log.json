2025-07-23 16:56:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 16:56:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 16:58:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 16:58:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:00:26 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:00:26 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:02:30 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:02:30 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:03:04 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:03:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:04:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:04:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:04:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:04:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:09:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:09:26 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:11:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:11:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
annotation_path = 'data/activitynet-1.3/annotations/activity_net.v1-3.min.json'
block_list = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt'
class_map = 'data/activitynet-1.3/annotations/category_idx.txt'
data_path = 'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='validation',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        subset_name='training',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        ann_file='data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
        block_list=
        'data/activitynet-1.3/raw_data/Anet_videos_15fps_short256/missing_files.txt',
        class_agnostic=True,
        class_map='data/activitynet-1.3/annotations/category_idx.txt',
        data_path='data/activitynet-1.3/raw_data/Anet_videos_15fps_short256',
        filter_gt=True,
        pipeline=[
            dict(format='mp4', prefix='v_', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(method='resize', num_clips=1, type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                224,
            ), type='mmaction.Resize'),
            dict(crop_size=224, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        resize_length=768,
        subset_name='validation',
        type='AnetResizeDataset'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    blocked_videos='data/activitynet-1.3/annotations/blocked.json',
    ground_truth_filename=
    'data/activitynet-1.3/annotations/activity_net.v1-3.min.json',
    subset='validation',
    tiou_thresholds=[
        0.5,
        0.55,
        0.6,
        0.65,
        0.7,
        0.75,
        0.8,
        0.85,
        0.9,
        0.95,
    ],
    type='mAP')
model = dict(
    backbone=dict(
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
    ),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=20,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:13:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:13:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:14:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:14:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:21:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:21:44 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:23:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:23:45 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:24:18 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:24:19 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:40:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:40:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:43:58 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:43:59 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:44:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:44:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:45:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:45:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:46:31 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:46:31 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:47:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:47:37 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:47:48 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:47:48 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:49:02 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:49:03 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:50:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:51:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:52:17 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:52:17 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:52:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:52:58 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:53:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:53:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:54:46 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:54:46 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:55:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:55:11 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:55:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:55:51 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:56:18 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:56:18 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:57:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:57:44 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:58:32 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:58:33 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 17:59:06 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 17:59:07 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:03:13 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:03:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
resize_length = 768
solver = dict(
    train=dict(
        grad_clip=dict(max_norm=40, norm_type=2),
        lr_scheduler=dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR'),
        optimizer=dict(
            betas=(
                0.9,
                0.999,
            ), lr=0.0001, type='AdamW', weight_decay=0.05),
        warmup=dict(start_factor=0.1, total_iters=5, type='LinearLR')))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:03:59 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:03:59 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(
        batch_size=8,
        grad_clip=dict(max_norm=40, norm_type=2),
        lr_scheduler=dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR'),
        num_workers=4,
        optimizer=dict(
            betas=(
                0.9,
                0.999,
            ), lr=0.0001, type='AdamW', weight_decay=0.05),
        warmup=dict(start_factor=0.1, total_iters=5, type='LinearLR')),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:05:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:05:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ), lr=0.0001, type='AdamW', weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:05:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:05:44 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VideoMAE',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ), lr=0.0001, type='AdamW', weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:11:47 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:11:47 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ), lr=0.0001, type='AdamW', weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:11:48 Train INFO: Using single GPU training...
2025-07-23 18:13:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:13:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
lr_scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:13:09 Train INFO: Using single GPU training...
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:13:09 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:14:10 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:14:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(T_max=60, eta_min=1e-06, type='CosineAnnealingLR')
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:14:11 Train INFO: Using single GPU training...
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:14:11 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:14:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:14:43 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    T_max=60, eta_min=1e-06, max_epoch=60, type='CosineAnnealingLR')
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:14:44 Train INFO: Using single GPU training...
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:14:44 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:15:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:15:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    T_max=60, eta_min=1e-06, max_epoch=60, type='CosineAnnealingLR')
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:15:56 Train INFO: Using single GPU training...
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:15:56 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:16:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:16:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    eta_min=1e-06,
    max_epoch=60,
    type='LinearWarmupCosineAnnealingLR',
    warmup_epoch=5)
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'

2025-07-23 18:16:10 Train INFO: Using single GPU training...
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:16:10 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:16:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-23 18:16:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
grad_clip = dict(max_norm=40, norm_type=2)
img_size = 160
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
resize_length = 768
scheduler = dict(
    eta_min=1e-06,
    max_epoch=60,
    type='LinearWarmupCosineAnnealingLR',
    warmup_epoch=5)
solver = dict(
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
warmup = dict(start_factor=0.1, total_iters=5, type='LinearLR')
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=0)

2025-07-23 18:16:54 Train INFO: Using single GPU training...
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-23 18:16:54 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-23 18:16:54 Train INFO: Training Starts...

2025-07-24 09:44:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:44:00 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:47:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:47:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:49:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:49:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:49:45 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:49:46 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:49:47 Train INFO: Using single GPU training...
2025-07-24 09:49:47 Train INFO: Using Model EMA...
2025-07-24 09:49:47 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:49:47 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:49:47 Train INFO: Training Starts...

2025-07-24 09:49:47 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:50:30 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:50:30 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:50:32 Train INFO: Using single GPU training...
2025-07-24 09:50:32 Train INFO: Using Model EMA...
2025-07-24 09:50:32 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:50:32 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:50:32 Train INFO: Training Starts...

2025-07-24 09:50:32 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:51:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:51:24 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:51:26 Train INFO: Using single GPU training...
2025-07-24 09:51:26 Train INFO: Using Model EMA...
2025-07-24 09:51:26 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:51:26 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:51:26 Train INFO: Training Starts...

2025-07-24 09:51:26 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:52:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:52:11 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:52:13 Train INFO: Using single GPU training...
2025-07-24 09:52:13 Train INFO: Using Model EMA...
2025-07-24 09:52:13 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:52:13 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:52:13 Train INFO: Training Starts...

2025-07-24 09:52:13 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:53:02 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:53:03 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:53:04 Train INFO: Using single GPU training...
2025-07-24 09:53:04 Train INFO: Using Model EMA...
2025-07-24 09:53:04 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:53:04 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:53:04 Train INFO: Training Starts...

2025-07-24 09:53:04 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:53:54 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:53:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=2048,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:53:57 Train INFO: Using single GPU training...
2025-07-24 09:53:57 Train INFO: Using Model EMA...
2025-07-24 09:53:57 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:53:57 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:53:57 Train INFO: Training Starts...

2025-07-24 09:53:57 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:55:43 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:55:43 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:55:45 Train INFO: Using single GPU training...
2025-07-24 09:55:45 Train INFO: Using Model EMA...
2025-07-24 09:55:45 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:55:45 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:55:45 Train INFO: Training Starts...

2025-07-24 09:55:45 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:56:39 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:56:39 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:56:41 Train INFO: Using single GPU training...
2025-07-24 09:56:41 Train INFO: Using Model EMA...
2025-07-24 09:56:41 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:56:41 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:56:41 Train INFO: Training Starts...

2025-07-24 09:56:41 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:57:54 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:57:55 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:57:57 Train INFO: Using single GPU training...
2025-07-24 09:57:57 Train INFO: Using Model EMA...
2025-07-24 09:57:57 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:57:57 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:57:57 Train INFO: Training Starts...

2025-07-24 09:57:57 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:58:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:58:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:58:55 Train INFO: Using single GPU training...
2025-07-24 09:58:55 Train INFO: Using Model EMA...
2025-07-24 09:58:55 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:58:55 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:58:55 Train INFO: Training Starts...

2025-07-24 09:58:55 Train INFO: [Train]: Epoch 0 started
2025-07-24 09:59:53 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 09:59:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 09:59:55 Train INFO: Using single GPU training...
2025-07-24 09:59:55 Train INFO: Using Model EMA...
2025-07-24 09:59:55 Train INFO: Using Automatic Mixed Precision...
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 09:59:55 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 09:59:55 Train INFO: Training Starts...

2025-07-24 09:59:55 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:00:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:00:58 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:01:00 Train INFO: Using single GPU training...
2025-07-24 10:01:00 Train INFO: Using Model EMA...
2025-07-24 10:01:00 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:01:00 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:01:00 Train INFO: Training Starts...

2025-07-24 10:01:00 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:04:46 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:04:47 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:04:49 Train INFO: Using single GPU training...
2025-07-24 10:04:49 Train INFO: Using Model EMA...
2025-07-24 10:04:49 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:04:49 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:04:49 Train INFO: Training Starts...

2025-07-24 10:04:49 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:06:04 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:06:04 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:06:06 Train INFO: Using single GPU training...
2025-07-24 10:06:06 Train INFO: Using Model EMA...
2025-07-24 10:06:06 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:06:06 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:06:06 Train INFO: Training Starts...

2025-07-24 10:06:06 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:07:32 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:07:33 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:07:35 Train INFO: Using single GPU training...
2025-07-24 10:07:35 Train INFO: Using Model EMA...
2025-07-24 10:07:35 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:07:35 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:07:35 Train INFO: Training Starts...

2025-07-24 10:07:35 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:09:22 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:09:22 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:09:24 Train INFO: Using single GPU training...
2025-07-24 10:09:24 Train INFO: Using Model EMA...
2025-07-24 10:09:24 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:09:24 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:09:24 Train INFO: Training Starts...

2025-07-24 10:09:24 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:10:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:10:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:10:22 Train INFO: Using single GPU training...
2025-07-24 10:10:22 Train INFO: Using Model EMA...
2025-07-24 10:10:22 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:10:22 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:10:22 Train INFO: Training Starts...

2025-07-24 10:10:22 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:12:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:12:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dim=768,
        img_size=160,
        init_values=0.1,
        mlp_ratio=4.0,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        type='VisionTransformerAdapter',
        use_abs_pos_emb=True,
        use_mean_pooling=True,
        use_rel_pos_bias=False),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
resize_length = 768
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:12:12 Train INFO: Using single GPU training...
2025-07-24 10:12:12 Train INFO: Using Model EMA...
2025-07-24 10:12:12 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:12:12 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:12:12 Train INFO: Training Starts...

2025-07-24 10:12:12 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:17:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:17:29 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:18:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:18:50 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:18:52 Train INFO: Using single GPU training...
2025-07-24 10:18:52 Train INFO: Using Model EMA...
2025-07-24 10:18:52 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:18:52 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:18:52 Train INFO: Training Starts...

2025-07-24 10:18:52 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:20:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:20:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:20:03 Train INFO: Using single GPU training...
2025-07-24 10:20:03 Train INFO: Using Model EMA...
2025-07-24 10:20:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:20:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:20:03 Train INFO: Training Starts...

2025-07-24 10:20:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:20:55 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:20:56 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:20:58 Train INFO: Using single GPU training...
2025-07-24 10:20:58 Train INFO: Using Model EMA...
2025-07-24 10:20:58 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:20:58 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:20:58 Train INFO: Training Starts...

2025-07-24 10:20:58 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:23:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:23:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:23:12 Train INFO: Using single GPU training...
2025-07-24 10:23:12 Train INFO: Using Model EMA...
2025-07-24 10:23:12 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:23:12 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:23:12 Train INFO: Training Starts...

2025-07-24 10:23:12 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:23:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:23:45 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:23:47 Train INFO: Using single GPU training...
2025-07-24 10:23:47 Train INFO: Using Model EMA...
2025-07-24 10:23:47 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:23:47 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:23:47 Train INFO: Training Starts...

2025-07-24 10:23:47 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:28:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:28:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:28:10 Train INFO: Using single GPU training...
2025-07-24 10:28:10 Train INFO: Using Model EMA...
2025-07-24 10:28:10 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:28:10 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:28:10 Train INFO: Training Starts...

2025-07-24 10:28:10 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:29:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:29:29 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:29:31 Train INFO: Using single GPU training...
2025-07-24 10:29:31 Train INFO: Using Model EMA...
2025-07-24 10:29:31 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:29:31 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:29:31 Train INFO: Training Starts...

2025-07-24 10:29:31 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:31:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:31:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:31:03 Train INFO: Using single GPU training...
2025-07-24 10:31:03 Train INFO: Using Model EMA...
2025-07-24 10:31:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:31:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:31:03 Train INFO: Training Starts...

2025-07-24 10:31:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:31:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:31:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:31:59 Train INFO: Using single GPU training...
2025-07-24 10:31:59 Train INFO: Using Model EMA...
2025-07-24 10:31:59 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:31:59 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:31:59 Train INFO: Training Starts...

2025-07-24 10:31:59 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:33:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:33:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:33:17 Train INFO: Using single GPU training...
2025-07-24 10:33:17 Train INFO: Using Model EMA...
2025-07-24 10:33:17 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:33:17 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:33:17 Train INFO: Training Starts...

2025-07-24 10:33:17 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:33:51 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:33:51 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:33:53 Train INFO: Using single GPU training...
2025-07-24 10:33:53 Train INFO: Using Model EMA...
2025-07-24 10:33:53 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:33:53 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:33:53 Train INFO: Training Starts...

2025-07-24 10:33:53 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:35:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:35:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:35:03 Train INFO: Using single GPU training...
2025-07-24 10:35:03 Train INFO: Using Model EMA...
2025-07-24 10:35:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:35:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:35:03 Train INFO: Training Starts...

2025-07-24 10:35:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:36:08 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:36:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:36:11 Train INFO: Using single GPU training...
2025-07-24 10:36:11 Train INFO: Using Model EMA...
2025-07-24 10:36:11 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:36:11 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:36:11 Train INFO: Training Starts...

2025-07-24 10:36:11 Train INFO: [Train]: Epoch 0 started
2025-07-24 10:37:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 10:37:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 10:37:59 Train INFO: Using single GPU training...
2025-07-24 10:37:59 Train INFO: Using Model EMA...
2025-07-24 10:37:59 Train INFO: Using Automatic Mixed Precision...
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 10:37:59 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 10:37:59 Train INFO: Training Starts...

2025-07-24 10:37:59 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:04:49 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:04:50 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:04:52 Train INFO: Using single GPU training...
2025-07-24 11:04:52 Train INFO: Using Model EMA...
2025-07-24 11:04:52 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:04:52 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:04:52 Train INFO: Training Starts...

2025-07-24 11:04:52 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:06:13 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:06:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:06:16 Train INFO: Using single GPU training...
2025-07-24 11:06:16 Train INFO: Using Model EMA...
2025-07-24 11:06:16 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:06:16 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:06:16 Train INFO: Training Starts...

2025-07-24 11:06:16 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:07:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:07:36 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:07:38 Train INFO: Using single GPU training...
2025-07-24 11:07:38 Train INFO: Using Model EMA...
2025-07-24 11:07:38 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:07:38 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:07:38 Train INFO: Training Starts...

2025-07-24 11:07:38 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:11:50 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:11:50 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:11:52 Train INFO: Using single GPU training...
2025-07-24 11:11:52 Train INFO: Using Model EMA...
2025-07-24 11:11:52 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:11:52 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:11:52 Train INFO: Training Starts...

2025-07-24 11:11:52 Train INFO: [Train]: Epoch 0 started
2025-07-24 11:13:34 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 11:13:35 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 11:13:37 Train INFO: Using single GPU training...
2025-07-24 11:13:37 Train INFO: Using Model EMA...
2025-07-24 11:13:37 Train INFO: Using Automatic Mixed Precision...
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 11:13:37 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 11:13:37 Train INFO: Training Starts...

2025-07-24 11:13:37 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:06:13 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:06:13 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:06:15 Train INFO: Using single GPU training...
2025-07-24 13:06:15 Train INFO: Using Model EMA...
2025-07-24 13:06:15 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:06:15 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:06:15 Train INFO: Training Starts...

2025-07-24 13:06:15 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:08:39 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:08:39 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=768,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=12,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=True,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=True,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t h w -> b c t',
                    reduction='mean',
                    type='Reduce'),
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=768, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:10:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:10:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=True,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=True,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t h w -> b c t',
                    reduction='mean',
                    type='Reduce'),
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:10:09 Train INFO: Using single GPU training...
2025-07-24 13:10:09 Train INFO: Using Model EMA...
2025-07-24 13:10:09 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:10:09 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:10:09 Train INFO: Training Starts...

2025-07-24 13:10:09 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:12:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:12:28 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=True,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=True,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:12:30 Train INFO: Using single GPU training...
2025-07-24 13:12:30 Train INFO: Using Model EMA...
2025-07-24 13:12:30 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:12:30 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:12:30 Train INFO: Training Starts...

2025-07-24 13:12:30 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:13:47 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:13:48 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=False,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=False,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) n c -> b c (t1 n)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:13:49 Train INFO: Using single GPU training...
2025-07-24 13:13:49 Train INFO: Using Model EMA...
2025-07-24 13:13:49 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:13:49 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:13:49 Train INFO: Training Starts...

2025-07-24 13:13:49 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:16:56 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:16:57 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=False,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=False,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) n c -> b c (t1 n)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:16:59 Train INFO: Using single GPU training...
2025-07-24 13:16:59 Train INFO: Using Model EMA...
2025-07-24 13:16:59 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:16:59 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:16:59 Train INFO: Training Starts...

2025-07-24 13:16:59 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:19:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:19:52 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        backbone=dict(
            adapter_index=[
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
            ],
            adapter_mlp_ratio=0.25,
            depth=12,
            drop_path_rate=0.1,
            embed_dims=384,
            img_size=160,
            mlp_ratio=4.0,
            num_frames=16,
            num_heads=6,
            patch_size=16,
            pretrained=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
            qkv_bias=True,
            return_feat_map=False,
            total_frames=768,
            tubelet_size=2,
            type='VisionTransformerAdapter',
            use_mean_pooling=False,
            with_cp=True),
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) n c -> b c (t1 n)',
                    t1=48,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=768, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=48,
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        data_preprocessor=dict(
            format_shape='NCTHW',
            mean=[
                123.675,
                116.28,
                103.53,
            ],
            std=[
                58.395,
                57.12,
                57.375,
            ],
            type='mmaction.ActionDataPreprocessor'),
        type='mmaction.Recognizer3D'),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:19:54 Train INFO: Using single GPU training...
2025-07-24 13:19:54 Train INFO: Using Model EMA...
2025-07-24 13:19:54 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.0.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.1.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.2.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.3.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.4.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.5.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.6.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.7.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.8.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.9.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.10.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.gamma
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.dwconv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.conv.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.down_proj.bias
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.weight
2025-07-24 13:19:54 Train INFO: Backbone parameter: model.backbone.blocks.11.adapter.up_proj.bias
2025-07-24 13:19:54 Train INFO: Training Starts...

2025-07-24 13:19:54 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:23:24 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:23:25 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:24:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:24:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:24:03 Train INFO: Using single GPU training...
2025-07-24 13:24:03 Train INFO: Using Model EMA...
2025-07-24 13:24:03 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:24:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:24:03 Train INFO: Training Starts...

2025-07-24 13:24:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:26:03 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:26:03 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:26:22 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:26:22 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:26:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:26:53 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:27:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:27:16 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:27:17 Train INFO: Using single GPU training...
2025-07-24 13:27:17 Train INFO: Using Model EMA...
2025-07-24 13:27:17 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:27:17 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:27:17 Train INFO: Training Starts...

2025-07-24 13:27:17 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:29:10 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:29:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ], ops='b n c t -> b c t', type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:29:12 Train INFO: Using single GPU training...
2025-07-24 13:29:12 Train INFO: Using Model EMA...
2025-07-24 13:29:12 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:29:12 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:29:12 Train INFO: Training Starts...

2025-07-24 13:29:12 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:31:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:31:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ], ops='b n c t -> b c t', type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:31:07 Train INFO: Using single GPU training...
2025-07-24 13:31:07 Train INFO: Using Model EMA...
2025-07-24 13:31:07 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:31:07 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:31:07 Train INFO: Training Starts...

2025-07-24 13:31:07 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:32:20 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:32:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    n=1,
                    ops='b n c t -> b c t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:32:22 Train INFO: Using single GPU training...
2025-07-24 13:32:22 Train INFO: Using Model EMA...
2025-07-24 13:32:22 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:32:22 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:32:22 Train INFO: Training Starts...

2025-07-24 13:32:22 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:35:41 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:35:42 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=19),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:35:45 Train INFO: Using single GPU training...
2025-07-24 13:35:45 Train INFO: Using Model EMA...
2025-07-24 13:35:45 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:35:45 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:35:45 Train INFO: Training Starts...

2025-07-24 13:35:45 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:38:07 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:38:08 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=16),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=2304,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:39:03 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:39:04 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=16),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=384,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:39:47 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:39:48 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=16),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:39:50 Train INFO: Using single GPU training...
2025-07-24 13:39:50 Train INFO: Using Model EMA...
2025-07-24 13:39:50 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:39:50 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:39:50 Train INFO: Training Starts...

2025-07-24 13:39:50 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:41:17 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:41:18 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=32),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:42:03 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:42:04 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=32),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:42:05 Train INFO: Using single GPU training...
2025-07-24 13:42:05 Train INFO: Using Model EMA...
2025-07-24 13:42:05 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:42:05 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:42:05 Train INFO: Training Starts...

2025-07-24 13:42:05 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:43:22 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:43:23 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=384, num_levels=6, out_channels=384, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:43:25 Train INFO: Using single GPU training...
2025-07-24 13:43:25 Train INFO: Using Model EMA...
2025-07-24 13:43:25 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:43:25 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:43:25 Train INFO: Training Starts...

2025-07-24 13:43:25 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:44:34 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:44:35 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=2, num_workers=2),
    val=dict(batch_size=2, num_workers=2))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:44:36 Train INFO: Using single GPU training...
2025-07-24 13:44:36 Train INFO: Using Model EMA...
2025-07-24 13:44:36 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:44:36 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:44:37 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:44:37 Train INFO: Training Starts...

2025-07-24 13:44:37 Train INFO: [Train]: Epoch 0 started
2025-07-24 13:49:04 Train INFO: [Train]: [000][00050/00471]  Loss=1.2492  cls_loss=0.6902  reg_loss=0.5590  lr_backbone=0.0e+00  lr_det=2.1e-06  mem=3756MB
2025-07-24 13:53:17 Train INFO: [Train]: [000][00100/00471]  Loss=1.4273  cls_loss=0.9025  reg_loss=0.5248  lr_backbone=0.0e+00  lr_det=4.2e-06  mem=3756MB
2025-07-24 13:54:08 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 13:54:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=8, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 13:54:10 Train INFO: Using single GPU training...
2025-07-24 13:54:10 Train INFO: Using Model EMA...
2025-07-24 13:54:10 Train INFO: Using Automatic Mixed Precision...
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 13:54:10 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 13:54:10 Train INFO: Training Starts...

2025-07-24 13:54:10 Train INFO: [Train]: Epoch 0 started
2025-07-24 15:19:46 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 15:19:46 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0),
    val=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 15:19:48 Train INFO: Using single GPU training...
2025-07-24 15:19:48 Train INFO: Using Model EMA...
2025-07-24 15:19:48 Train INFO: Using Automatic Mixed Precision...
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 15:19:48 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 15:19:48 Train INFO: Training Starts...

2025-07-24 15:19:48 Train INFO: [Train]: Epoch 0 started
2025-07-24 16:58:00 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-24 16:58:01 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 48
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=768,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    val=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=768))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=768,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0),
    val=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 768
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=2,
    val_loss_interval=-1,
    val_start_epoch=40)

2025-07-24 16:58:02 Train INFO: Using single GPU training...
2025-07-24 16:58:02 Train INFO: Using Model EMA...
2025-07-24 16:58:02 Train INFO: Using Automatic Mixed Precision...
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.pos_embed
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-24 16:58:02 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-24 16:58:03 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-24 16:58:03 Train INFO: Training Starts...

2025-07-24 16:58:03 Train INFO: [Train]: Epoch 0 started
2025-07-24 17:36:40 Train INFO: [Train]: [000][00050/00117]  Loss=1.5018  cls_loss=0.9450  reg_loss=0.5568  lr_backbone=0.0e+00  lr_det=8.5e-06  mem=12367MB
2025-07-24 18:14:16 Train INFO: [Train]: [000][00100/00117]  Loss=1.4311  cls_loss=0.9842  reg_loss=0.4469  lr_backbone=0.0e+00  lr_det=1.7e-05  mem=12367MB
2025-07-24 18:26:57 Train INFO: [Train]: [000][00117/00117]  Loss=1.4140  cls_loss=0.9843  reg_loss=0.4298  lr_backbone=0.0e+00  lr_det=2.0e-05  mem=12367MB
2025-07-24 18:26:57 Train INFO: [Train]: Epoch 1 started
2025-07-24 19:05:41 Train INFO: [Train]: [001][00050/00117]  Loss=1.3225  cls_loss=0.9905  reg_loss=0.3320  lr_backbone=0.0e+00  lr_det=2.9e-05  mem=12367MB
2025-07-24 19:43:40 Train INFO: [Train]: [001][00100/00117]  Loss=1.3068  cls_loss=0.9798  reg_loss=0.3270  lr_backbone=0.0e+00  lr_det=3.7e-05  mem=12367MB
2025-07-24 19:56:23 Train INFO: [Train]: [001][00117/00117]  Loss=1.3014  cls_loss=0.9753  reg_loss=0.3261  lr_backbone=0.0e+00  lr_det=4.0e-05  mem=12367MB
2025-07-24 19:56:24 Train INFO: [Train]: Epoch 2 started
2025-07-24 20:35:07 Train INFO: [Train]: [002][00050/00117]  Loss=1.2868  cls_loss=0.9547  reg_loss=0.3321  lr_backbone=0.0e+00  lr_det=4.9e-05  mem=12367MB
2025-07-24 21:13:06 Train INFO: [Train]: [002][00100/00117]  Loss=1.1936  cls_loss=0.8645  reg_loss=0.3291  lr_backbone=0.0e+00  lr_det=5.7e-05  mem=12367MB
2025-07-24 21:25:50 Train INFO: [Train]: [002][00117/00117]  Loss=1.1695  cls_loss=0.8415  reg_loss=0.3280  lr_backbone=0.0e+00  lr_det=6.0e-05  mem=12367MB
2025-07-24 21:25:50 Train INFO: [Train]: Epoch 3 started
2025-07-24 22:04:36 Train INFO: [Train]: [003][00050/00117]  Loss=1.0364  cls_loss=0.7124  reg_loss=0.3240  lr_backbone=0.0e+00  lr_det=6.9e-05  mem=12367MB
2025-07-24 22:42:38 Train INFO: [Train]: [003][00100/00117]  Loss=1.0362  cls_loss=0.7123  reg_loss=0.3239  lr_backbone=0.0e+00  lr_det=7.7e-05  mem=12367MB
2025-07-24 22:55:23 Train INFO: [Train]: [003][00117/00117]  Loss=1.0277  cls_loss=0.7065  reg_loss=0.3212  lr_backbone=0.0e+00  lr_det=8.0e-05  mem=12367MB
2025-07-24 22:55:24 Train INFO: [Train]: Epoch 4 started
2025-07-24 23:34:12 Train INFO: [Train]: [004][00050/00117]  Loss=1.0415  cls_loss=0.7155  reg_loss=0.3260  lr_backbone=0.0e+00  lr_det=8.9e-05  mem=12367MB
2025-07-25 00:12:14 Train INFO: [Train]: [004][00100/00117]  Loss=1.0292  cls_loss=0.7061  reg_loss=0.3230  lr_backbone=0.0e+00  lr_det=9.7e-05  mem=12367MB
2025-07-25 00:24:55 Train INFO: [Train]: [004][00117/00117]  Loss=1.0294  cls_loss=0.7059  reg_loss=0.3235  lr_backbone=0.0e+00  lr_det=1.0e-04  mem=12367MB
2025-07-25 00:24:55 Train INFO: [Train]: Epoch 5 started
2025-07-25 01:03:36 Train INFO: [Train]: [005][00050/00117]  Loss=1.0278  cls_loss=0.7066  reg_loss=0.3212  lr_backbone=1.5e-12  lr_det=1.0e-04  mem=12367MB
2025-07-25 01:41:34 Train INFO: [Train]: [005][00100/00117]  Loss=1.0260  cls_loss=0.7045  reg_loss=0.3214  lr_backbone=5.9e-12  lr_det=1.0e-04  mem=12367MB
2025-07-25 01:54:16 Train INFO: [Train]: [005][00117/00117]  Loss=1.0161  cls_loss=0.6970  reg_loss=0.3191  lr_backbone=8.0e-12  lr_det=1.0e-04  mem=12367MB
2025-07-25 01:54:16 Train INFO: [Train]: Epoch 6 started
2025-07-25 02:33:00 Train INFO: [Train]: [006][00050/00117]  Loss=1.0504  cls_loss=0.7190  reg_loss=0.3314  lr_backbone=1.7e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 03:10:54 Train INFO: [Train]: [006][00100/00117]  Loss=1.0282  cls_loss=0.7029  reg_loss=0.3253  lr_backbone=2.8e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 03:23:35 Train INFO: [Train]: [006][00117/00117]  Loss=1.0264  cls_loss=0.7014  reg_loss=0.3250  lr_backbone=3.2e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 03:23:35 Train INFO: [Train]: Epoch 7 started
2025-07-25 04:02:11 Train INFO: [Train]: [007][00050/00117]  Loss=1.0084  cls_loss=0.6882  reg_loss=0.3202  lr_backbone=4.8e-11  lr_det=1.0e-04  mem=12367MB
2025-07-25 04:40:10 Train INFO: [Train]: [007][00100/00117]  Loss=1.0064  cls_loss=0.6866  reg_loss=0.3198  lr_backbone=6.6e-11  lr_det=9.9e-05  mem=12367MB
2025-07-25 04:52:51 Train INFO: [Train]: [007][00117/00117]  Loss=1.0051  cls_loss=0.6854  reg_loss=0.3196  lr_backbone=7.3e-11  lr_det=9.9e-05  mem=12367MB
2025-07-25 04:52:52 Train INFO: [Train]: Epoch 8 started
2025-07-25 05:31:37 Train INFO: [Train]: [008][00050/00117]  Loss=1.0140  cls_loss=0.6951  reg_loss=0.3189  lr_backbone=9.5e-11  lr_det=9.9e-05  mem=12367MB
2025-07-25 06:09:35 Train INFO: [Train]: [008][00100/00117]  Loss=1.0138  cls_loss=0.6925  reg_loss=0.3213  lr_backbone=1.2e-10  lr_det=9.9e-05  mem=12367MB
2025-07-25 06:22:17 Train INFO: [Train]: [008][00117/00117]  Loss=1.0129  cls_loss=0.6909  reg_loss=0.3220  lr_backbone=1.3e-10  lr_det=9.9e-05  mem=12367MB
2025-07-25 06:22:17 Train INFO: [Train]: Epoch 9 started
2025-07-25 07:00:53 Train INFO: [Train]: [009][00050/00117]  Loss=1.0029  cls_loss=0.6846  reg_loss=0.3183  lr_backbone=1.6e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 07:38:54 Train INFO: [Train]: [009][00100/00117]  Loss=1.0179  cls_loss=0.6951  reg_loss=0.3228  lr_backbone=1.9e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 07:51:35 Train INFO: [Train]: [009][00117/00117]  Loss=1.0091  cls_loss=0.6884  reg_loss=0.3207  lr_backbone=2.0e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 07:51:36 Train INFO: [Train]: Epoch 10 started
2025-07-25 08:30:20 Train INFO: [Train]: [010][00050/00117]  Loss=1.0232  cls_loss=0.7021  reg_loss=0.3211  lr_backbone=2.4e-10  lr_det=9.8e-05  mem=12367MB
2025-07-25 09:10:29 Train INFO: [Train]: [010][00100/00117]  Loss=1.0165  cls_loss=0.6940  reg_loss=0.3225  lr_backbone=2.8e-10  lr_det=9.7e-05  mem=12367MB
2025-07-25 09:27:32 Train INFO: [Train]: [010][00117/00117]  Loss=1.0092  cls_loss=0.6891  reg_loss=0.3201  lr_backbone=2.9e-10  lr_det=9.7e-05  mem=12367MB
2025-07-25 09:27:32 Train INFO: [Train]: Epoch 11 started
2025-07-25 10:08:41 Train INFO: [Train]: [011][00050/00117]  Loss=1.0107  cls_loss=0.6923  reg_loss=0.3184  lr_backbone=3.3e-10  lr_det=9.7e-05  mem=12367MB
2025-07-25 10:48:28 Train INFO: [Train]: [011][00100/00117]  Loss=1.0092  cls_loss=0.6910  reg_loss=0.3182  lr_backbone=3.8e-10  lr_det=9.6e-05  mem=12367MB
2025-07-25 11:01:13 Train INFO: [Train]: [011][00117/00117]  Loss=1.0139  cls_loss=0.6936  reg_loss=0.3204  lr_backbone=3.9e-10  lr_det=9.6e-05  mem=12367MB
2025-07-25 11:01:14 Train INFO: [Train]: Epoch 12 started
2025-07-25 11:39:55 Train INFO: [Train]: [012][00050/00117]  Loss=1.0135  cls_loss=0.6928  reg_loss=0.3206  lr_backbone=4.4e-10  lr_det=9.6e-05  mem=12367MB
2025-07-25 12:17:50 Train INFO: [Train]: [012][00100/00117]  Loss=1.0033  cls_loss=0.6849  reg_loss=0.3184  lr_backbone=4.9e-10  lr_det=9.5e-05  mem=12367MB
2025-07-25 12:30:28 Train INFO: [Train]: [012][00117/00117]  Loss=1.0042  cls_loss=0.6848  reg_loss=0.3193  lr_backbone=5.1e-10  lr_det=9.5e-05  mem=12367MB
2025-07-25 12:30:28 Train INFO: [Train]: Epoch 13 started
2025-07-25 13:09:15 Train INFO: [Train]: [013][00050/00117]  Loss=1.0052  cls_loss=0.6869  reg_loss=0.3183  lr_backbone=5.7e-10  lr_det=9.4e-05  mem=12367MB
2025-07-25 13:51:53 Train INFO: [Train]: [013][00100/00117]  Loss=1.0047  cls_loss=0.6857  reg_loss=0.3190  lr_backbone=6.3e-10  lr_det=9.4e-05  mem=12367MB
2025-07-25 14:07:46 Train INFO: [Train]: [013][00117/00117]  Loss=1.0069  cls_loss=0.6873  reg_loss=0.3196  lr_backbone=6.5e-10  lr_det=9.4e-05  mem=12367MB
2025-07-25 14:07:46 Train INFO: [Train]: Epoch 14 started
2025-07-25 14:46:38 Train INFO: [Train]: [014][00050/00117]  Loss=1.0091  cls_loss=0.6893  reg_loss=0.3197  lr_backbone=7.1e-10  lr_det=9.3e-05  mem=12367MB
2025-07-25 15:24:41 Train INFO: [Train]: [014][00100/00117]  Loss=1.0122  cls_loss=0.6917  reg_loss=0.3205  lr_backbone=7.7e-10  lr_det=9.2e-05  mem=12367MB
2025-07-25 15:38:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:38:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:39:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:39:06 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:39:08 Train INFO: Using single GPU training...
2025-07-25 15:39:08 Train INFO: Using Model EMA...
2025-07-25 15:39:08 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:39:08 Train INFO: Freeze the backbone...
2025-07-25 15:40:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:40:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:40:07 Train INFO: Using single GPU training...
2025-07-25 15:40:07 Train INFO: Using Model EMA...
2025-07-25 15:40:07 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:40:07 Train INFO: Freeze the backbone...
2025-07-25 15:42:44 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:42:45 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    lr=0.0001,
    paramwise_cfg=dict(
        backbone=dict(
            custom=[
                dict(lr=0.0002, name='adapter', weight_decay=0.05),
            ],
            exclude=[
                'backbone',
            ],
            lr=0,
            weight_decay=0)),
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:42:46 Train INFO: Using single GPU training...
2025-07-25 15:42:46 Train INFO: Using Model EMA...
2025-07-25 15:42:46 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:42:46 Train INFO: Freeze the backbone...
2025-07-25 15:43:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:43:14 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:43:16 Train INFO: Using single GPU training...
2025-07-25 15:43:16 Train INFO: Using Model EMA...
2025-07-25 15:43:16 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:43:16 Train INFO: Freeze the backbone...
2025-07-25 15:44:19 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:44:20 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.75,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=0),
    train=dict(batch_size=8, num_workers=0))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:44:21 Train INFO: Using single GPU training...
2025-07-25 15:44:21 Train INFO: Using Model EMA...
2025-07-25 15:44:21 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:44:21 Train INFO: Freeze the backbone...
2025-07-25 15:44:21 Train INFO: Training Starts...

2025-07-25 15:44:21 Train INFO: [Train]: Epoch 0 started
2025-07-25 15:57:29 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 15:57:30 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='val',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=8),
    train=dict(batch_size=16, num_workers=8))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 15:57:31 Train INFO: Using single GPU training...
2025-07-25 15:57:31 Train INFO: Using Model EMA...
2025-07-25 15:57:31 Train INFO: Using Automatic Mixed Precision...
2025-07-25 15:57:31 Train INFO: Freeze the backbone...
2025-07-25 15:57:31 Train INFO: Training Starts...

2025-07-25 15:57:31 Train INFO: [Train]: Epoch 0 started
2025-07-25 16:20:12 Train INFO: [Train]: [000][00050/00058]  Loss=1.5328  cls_loss=1.0013  reg_loss=0.5315  lr_det=1.7e-05  mem=8731MB
2025-07-25 16:21:39 Train INFO: [Train]: [000][00058/00058]  Loss=1.5141  cls_loss=1.0079  reg_loss=0.5062  lr_det=2.0e-05  mem=8731MB
2025-07-25 16:43:10 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-25 16:43:10 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-25 16:43:12 Train INFO: Using single GPU training...
2025-07-25 16:43:12 Train INFO: Using Model EMA...
2025-07-25 16:43:12 Train INFO: Using Automatic Mixed Precision...
2025-07-25 16:43:12 Train INFO: Freeze the backbone...
2025-07-25 16:43:12 Train INFO: Training Starts...

2025-07-25 16:43:12 Train INFO: [Train]: Epoch 0 started
2025-07-25 17:04:10 Train INFO: [Train]: [000][00050/00058]  Loss=1.5424  cls_loss=1.0041  reg_loss=0.5383  lr_det=1.7e-05  mem=8731MB
2025-07-25 17:06:48 Train INFO: [Train]: [000][00058/00058]  Loss=1.5086  cls_loss=1.0001  reg_loss=0.5085  lr_det=2.0e-05  mem=8731MB
2025-07-25 17:06:49 Train INFO: [Train]: Epoch 1 started
2025-07-25 17:27:36 Train INFO: [Train]: [001][00050/00058]  Loss=1.3082  cls_loss=0.9883  reg_loss=0.3200  lr_det=3.7e-05  mem=8731MB
2025-07-25 17:30:28 Train INFO: [Train]: [001][00058/00058]  Loss=1.2851  cls_loss=0.9721  reg_loss=0.3131  lr_det=4.0e-05  mem=8731MB
2025-07-25 17:30:30 Train INFO: [Train]: Epoch 2 started
2025-07-25 17:51:57 Train INFO: [Train]: [002][00050/00058]  Loss=1.2799  cls_loss=0.9696  reg_loss=0.3103  lr_det=5.7e-05  mem=8731MB
2025-07-25 17:54:32 Train INFO: [Train]: [002][00058/00058]  Loss=1.2811  cls_loss=0.9687  reg_loss=0.3124  lr_det=6.0e-05  mem=8731MB
2025-07-25 17:54:32 Train INFO: [Train]: Epoch 3 started
2025-07-25 18:15:03 Train INFO: [Train]: [003][00050/00058]  Loss=1.1720  cls_loss=0.8587  reg_loss=0.3133  lr_det=7.7e-05  mem=8731MB
2025-07-25 18:17:34 Train INFO: [Train]: [003][00058/00058]  Loss=1.1449  cls_loss=0.8343  reg_loss=0.3106  lr_det=8.0e-05  mem=8731MB
2025-07-25 18:17:36 Train INFO: [Train]: Epoch 4 started
2025-07-25 18:38:25 Train INFO: [Train]: [004][00050/00058]  Loss=1.0178  cls_loss=0.7124  reg_loss=0.3054  lr_det=9.7e-05  mem=8731MB
2025-07-25 18:41:05 Train INFO: [Train]: [004][00058/00058]  Loss=1.0291  cls_loss=0.7193  reg_loss=0.3098  lr_det=1.0e-04  mem=8731MB
2025-07-25 18:41:05 Train INFO: [Train]: Epoch 5 started
2025-07-25 19:01:26 Train INFO: [Train]: [005][00050/00058]  Loss=1.0201  cls_loss=0.7116  reg_loss=0.3085  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:04:04 Train INFO: [Train]: [005][00058/00058]  Loss=1.0152  cls_loss=0.7078  reg_loss=0.3074  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:04:05 Train INFO: [Train]: Epoch 6 started
2025-07-25 19:24:25 Train INFO: [Train]: [006][00050/00058]  Loss=1.0059  cls_loss=0.6958  reg_loss=0.3100  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:27:02 Train INFO: [Train]: [006][00058/00058]  Loss=1.0057  cls_loss=0.6969  reg_loss=0.3088  lr_det=1.0e-04  mem=8731MB
2025-07-25 19:27:03 Train INFO: [Train]: Epoch 7 started
2025-07-25 19:48:01 Train INFO: [Train]: [007][00050/00058]  Loss=1.0088  cls_loss=0.7016  reg_loss=0.3072  lr_det=9.9e-05  mem=8731MB
2025-07-25 19:50:41 Train INFO: [Train]: [007][00058/00058]  Loss=1.0035  cls_loss=0.6986  reg_loss=0.3049  lr_det=9.9e-05  mem=8731MB
2025-07-25 19:50:43 Train INFO: [Train]: Epoch 8 started
2025-07-25 20:11:37 Train INFO: [Train]: [008][00050/00058]  Loss=0.9983  cls_loss=0.6947  reg_loss=0.3037  lr_det=9.9e-05  mem=8731MB
2025-07-25 20:14:04 Train INFO: [Train]: [008][00058/00058]  Loss=0.9983  cls_loss=0.6942  reg_loss=0.3041  lr_det=9.9e-05  mem=8731MB
2025-07-25 20:14:05 Train INFO: [Train]: Epoch 9 started
2025-07-25 20:35:13 Train INFO: [Train]: [009][00050/00058]  Loss=1.0038  cls_loss=0.6940  reg_loss=0.3098  lr_det=9.8e-05  mem=8731MB
2025-07-25 20:37:50 Train INFO: [Train]: [009][00058/00058]  Loss=0.9953  cls_loss=0.6885  reg_loss=0.3068  lr_det=9.8e-05  mem=8731MB
2025-07-25 20:37:52 Train INFO: [Train]: Epoch 10 started
2025-07-25 20:58:50 Train INFO: [Train]: [010][00050/00058]  Loss=1.0131  cls_loss=0.7008  reg_loss=0.3123  lr_det=9.7e-05  mem=8731MB
2025-07-25 21:01:36 Train INFO: [Train]: [010][00058/00058]  Loss=1.0095  cls_loss=0.6986  reg_loss=0.3109  lr_det=9.7e-05  mem=8731MB
2025-07-25 21:01:37 Train INFO: [Train]: Epoch 11 started
2025-07-25 21:22:38 Train INFO: [Train]: [011][00050/00058]  Loss=0.9937  cls_loss=0.6881  reg_loss=0.3056  lr_det=9.6e-05  mem=8731MB
2025-07-25 21:25:12 Train INFO: [Train]: [011][00058/00058]  Loss=0.9873  cls_loss=0.6828  reg_loss=0.3045  lr_det=9.6e-05  mem=8731MB
2025-07-25 21:25:13 Train INFO: [Train]: Epoch 12 started
2025-07-25 21:45:50 Train INFO: [Train]: [012][00050/00058]  Loss=0.9961  cls_loss=0.6898  reg_loss=0.3063  lr_det=9.5e-05  mem=8731MB
2025-07-25 21:48:21 Train INFO: [Train]: [012][00058/00058]  Loss=0.9953  cls_loss=0.6893  reg_loss=0.3060  lr_det=9.5e-05  mem=8731MB
2025-07-25 21:48:21 Train INFO: [Train]: Epoch 13 started
2025-07-25 22:08:40 Train INFO: [Train]: [013][00050/00058]  Loss=0.9984  cls_loss=0.6899  reg_loss=0.3085  lr_det=9.4e-05  mem=8731MB
2025-07-25 22:11:31 Train INFO: [Train]: [013][00058/00058]  Loss=0.9971  cls_loss=0.6898  reg_loss=0.3073  lr_det=9.4e-05  mem=8731MB
2025-07-25 22:11:33 Train INFO: [Train]: Epoch 14 started
2025-07-25 22:32:16 Train INFO: [Train]: [014][00050/00058]  Loss=0.9941  cls_loss=0.6891  reg_loss=0.3050  lr_det=9.2e-05  mem=8731MB
2025-07-25 22:35:05 Train INFO: [Train]: [014][00058/00058]  Loss=0.9882  cls_loss=0.6849  reg_loss=0.3033  lr_det=9.2e-05  mem=8731MB
2025-07-25 22:35:06 Train INFO: [Train]: Epoch 15 started
2025-07-25 22:55:29 Train INFO: [Train]: [015][00050/00058]  Loss=1.0020  cls_loss=0.6946  reg_loss=0.3074  lr_det=9.1e-05  mem=8731MB
2025-07-25 22:58:10 Train INFO: [Train]: [015][00058/00058]  Loss=0.9890  cls_loss=0.6852  reg_loss=0.3038  lr_det=9.0e-05  mem=8731MB
2025-07-25 22:58:11 Train INFO: [Train]: Epoch 16 started
2025-07-25 23:19:30 Train INFO: [Train]: [016][00050/00058]  Loss=1.0061  cls_loss=0.6952  reg_loss=0.3109  lr_det=8.9e-05  mem=8731MB
2025-07-25 23:22:11 Train INFO: [Train]: [016][00058/00058]  Loss=0.9982  cls_loss=0.6898  reg_loss=0.3085  lr_det=8.9e-05  mem=8731MB
2025-07-25 23:22:12 Train INFO: [Train]: Epoch 17 started
2025-07-25 23:43:08 Train INFO: [Train]: [017][00050/00058]  Loss=0.9980  cls_loss=0.6915  reg_loss=0.3065  lr_det=8.7e-05  mem=8731MB
2025-07-25 23:45:47 Train INFO: [Train]: [017][00058/00058]  Loss=0.9851  cls_loss=0.6822  reg_loss=0.3029  lr_det=8.7e-05  mem=8731MB
2025-07-25 23:45:48 Train INFO: [Train]: Epoch 18 started
2025-07-26 00:05:45 Train INFO: [Train]: [018][00050/00058]  Loss=0.9938  cls_loss=0.6958  reg_loss=0.2980  lr_det=8.5e-05  mem=8731MB
2025-07-26 00:08:17 Train INFO: [Train]: [018][00058/00058]  Loss=0.9894  cls_loss=0.6910  reg_loss=0.2984  lr_det=8.5e-05  mem=8731MB
2025-07-26 00:08:18 Train INFO: [Train]: Epoch 19 started
2025-07-26 00:29:14 Train INFO: [Train]: [019][00050/00058]  Loss=0.9833  cls_loss=0.6811  reg_loss=0.3022  lr_det=8.3e-05  mem=8731MB
2025-07-26 00:31:52 Train INFO: [Train]: [019][00058/00058]  Loss=0.9887  cls_loss=0.6853  reg_loss=0.3035  lr_det=8.3e-05  mem=8731MB
2025-07-26 00:31:53 Train INFO: [Train]: Epoch 20 started
2025-07-26 00:51:33 Train INFO: [Train]: [020][00050/00058]  Loss=0.9906  cls_loss=0.6850  reg_loss=0.3056  lr_det=8.1e-05  mem=8731MB
2025-07-26 00:54:20 Train INFO: [Train]: [020][00058/00058]  Loss=0.9931  cls_loss=0.6869  reg_loss=0.3062  lr_det=8.1e-05  mem=8731MB
2025-07-26 00:54:21 Train INFO: [Train]: Epoch 21 started
2025-07-26 01:14:56 Train INFO: [Train]: [021][00050/00058]  Loss=0.9696  cls_loss=0.6722  reg_loss=0.2974  lr_det=7.9e-05  mem=8731MB
2025-07-26 01:17:25 Train INFO: [Train]: [021][00058/00058]  Loss=0.9724  cls_loss=0.6745  reg_loss=0.2979  lr_det=7.8e-05  mem=8731MB
2025-07-26 01:17:27 Train INFO: [Train]: Epoch 22 started
2025-07-26 01:38:10 Train INFO: [Train]: [022][00050/00058]  Loss=0.9821  cls_loss=0.6816  reg_loss=0.3005  lr_det=7.6e-05  mem=8731MB
2025-07-26 01:40:59 Train INFO: [Train]: [022][00058/00058]  Loss=0.9680  cls_loss=0.6722  reg_loss=0.2958  lr_det=7.6e-05  mem=8731MB
2025-07-26 01:41:00 Train INFO: [Train]: Epoch 23 started
2025-07-26 02:01:57 Train INFO: [Train]: [023][00050/00058]  Loss=1.0127  cls_loss=0.7009  reg_loss=0.3118  lr_det=7.4e-05  mem=8731MB
2025-07-26 02:04:45 Train INFO: [Train]: [023][00058/00058]  Loss=1.0072  cls_loss=0.6970  reg_loss=0.3101  lr_det=7.3e-05  mem=8731MB
2025-07-26 02:04:47 Train INFO: [Train]: Epoch 24 started
2025-07-26 02:25:11 Train INFO: [Train]: [024][00050/00058]  Loss=0.9837  cls_loss=0.6818  reg_loss=0.3019  lr_det=7.1e-05  mem=8731MB
2025-07-26 02:27:58 Train INFO: [Train]: [024][00058/00058]  Loss=0.9744  cls_loss=0.6754  reg_loss=0.2991  lr_det=7.1e-05  mem=8731MB
2025-07-26 02:27:58 Train INFO: [Train]: Epoch 25 started
2025-07-26 02:48:34 Train INFO: [Train]: [025][00050/00058]  Loss=0.9745  cls_loss=0.6722  reg_loss=0.3023  lr_det=6.9e-05  mem=8731MB
2025-07-26 02:51:15 Train INFO: [Train]: [025][00058/00058]  Loss=0.9695  cls_loss=0.6695  reg_loss=0.3000  lr_det=6.8e-05  mem=8731MB
2025-07-26 02:51:17 Train INFO: [Train]: Epoch 26 started
2025-07-26 03:11:38 Train INFO: [Train]: [026][00050/00058]  Loss=0.9943  cls_loss=0.6909  reg_loss=0.3033  lr_det=6.6e-05  mem=8731MB
2025-07-26 03:14:30 Train INFO: [Train]: [026][00058/00058]  Loss=0.9936  cls_loss=0.6892  reg_loss=0.3045  lr_det=6.6e-05  mem=8731MB
2025-07-26 03:14:31 Train INFO: [Train]: Epoch 27 started
2025-07-26 03:35:29 Train INFO: [Train]: [027][00050/00058]  Loss=0.9861  cls_loss=0.6811  reg_loss=0.3050  lr_det=6.3e-05  mem=8731MB
2025-07-26 03:38:01 Train INFO: [Train]: [027][00058/00058]  Loss=0.9780  cls_loss=0.6757  reg_loss=0.3024  lr_det=6.3e-05  mem=8731MB
2025-07-26 03:38:02 Train INFO: [Train]: Epoch 28 started
2025-07-26 03:58:30 Train INFO: [Train]: [028][00050/00058]  Loss=0.9960  cls_loss=0.6878  reg_loss=0.3082  lr_det=6.0e-05  mem=8731MB
2025-07-26 04:01:07 Train INFO: [Train]: [028][00058/00058]  Loss=0.9874  cls_loss=0.6817  reg_loss=0.3057  lr_det=6.0e-05  mem=8731MB
2025-07-26 04:01:07 Train INFO: [Train]: Epoch 29 started
2025-07-26 04:21:34 Train INFO: [Train]: [029][00050/00058]  Loss=0.9755  cls_loss=0.6747  reg_loss=0.3008  lr_det=5.8e-05  mem=8731MB
2025-07-26 04:24:22 Train INFO: [Train]: [029][00058/00058]  Loss=0.9707  cls_loss=0.6714  reg_loss=0.2993  lr_det=5.7e-05  mem=8731MB
2025-07-26 04:24:23 Train INFO: [Train]: Epoch 30 started
2025-07-26 04:45:39 Train INFO: [Train]: [030][00050/00058]  Loss=0.9891  cls_loss=0.6880  reg_loss=0.3010  lr_det=5.5e-05  mem=8731MB
2025-07-26 04:48:09 Train INFO: [Train]: [030][00058/00058]  Loss=0.9842  cls_loss=0.6841  reg_loss=0.3000  lr_det=5.4e-05  mem=8731MB
2025-07-26 04:48:10 Train INFO: [Train]: Epoch 31 started
2025-07-26 05:08:41 Train INFO: [Train]: [031][00050/00058]  Loss=0.9758  cls_loss=0.6737  reg_loss=0.3021  lr_det=5.2e-05  mem=8731MB
2025-07-26 05:11:16 Train INFO: [Train]: [031][00058/00058]  Loss=0.9750  cls_loss=0.6743  reg_loss=0.3007  lr_det=5.1e-05  mem=8731MB
2025-07-26 05:11:18 Train INFO: [Train]: Epoch 32 started
2025-07-26 05:32:00 Train INFO: [Train]: [032][00050/00058]  Loss=0.9766  cls_loss=0.6763  reg_loss=0.3003  lr_det=4.9e-05  mem=8731MB
2025-07-26 05:34:45 Train INFO: [Train]: [032][00058/00058]  Loss=0.9761  cls_loss=0.6761  reg_loss=0.3000  lr_det=4.9e-05  mem=8731MB
2025-07-26 05:34:46 Train INFO: [Train]: Epoch 33 started
2025-07-26 05:54:39 Train INFO: [Train]: [033][00050/00058]  Loss=0.9833  cls_loss=0.6776  reg_loss=0.3057  lr_det=4.6e-05  mem=8731MB
2025-07-26 05:57:19 Train INFO: [Train]: [033][00058/00058]  Loss=0.9800  cls_loss=0.6748  reg_loss=0.3051  lr_det=4.6e-05  mem=8731MB
2025-07-26 05:57:20 Train INFO: [Train]: Epoch 34 started
2025-07-26 06:17:44 Train INFO: [Train]: [034][00050/00058]  Loss=0.9778  cls_loss=0.6735  reg_loss=0.3043  lr_det=4.3e-05  mem=8731MB
2025-07-26 06:20:34 Train INFO: [Train]: [034][00058/00058]  Loss=0.9760  cls_loss=0.6744  reg_loss=0.3016  lr_det=4.3e-05  mem=8731MB
2025-07-26 06:20:35 Train INFO: [Train]: Epoch 35 started
2025-07-26 06:41:18 Train INFO: [Train]: [035][00050/00058]  Loss=0.9704  cls_loss=0.6722  reg_loss=0.2983  lr_det=4.1e-05  mem=8731MB
2025-07-26 06:43:59 Train INFO: [Train]: [035][00058/00058]  Loss=0.9648  cls_loss=0.6683  reg_loss=0.2966  lr_det=4.0e-05  mem=8731MB
2025-07-26 06:44:00 Train INFO: [Train]: Epoch 36 started
2025-07-26 07:04:38 Train INFO: [Train]: [036][00050/00058]  Loss=0.9991  cls_loss=0.6901  reg_loss=0.3089  lr_det=3.8e-05  mem=8731MB
2025-07-26 07:07:23 Train INFO: [Train]: [036][00058/00058]  Loss=0.9920  cls_loss=0.6836  reg_loss=0.3084  lr_det=3.7e-05  mem=8731MB
2025-07-26 07:07:24 Train INFO: [Train]: Epoch 37 started
2025-07-26 07:28:20 Train INFO: [Train]: [037][00050/00058]  Loss=0.9756  cls_loss=0.6742  reg_loss=0.3014  lr_det=3.5e-05  mem=8731MB
2025-07-26 07:31:01 Train INFO: [Train]: [037][00058/00058]  Loss=0.9727  cls_loss=0.6727  reg_loss=0.3000  lr_det=3.5e-05  mem=8731MB
2025-07-26 07:31:02 Train INFO: [Train]: Epoch 38 started
2025-07-26 07:51:52 Train INFO: [Train]: [038][00050/00058]  Loss=0.9735  cls_loss=0.6746  reg_loss=0.2988  lr_det=3.2e-05  mem=8731MB
2025-07-26 07:54:34 Train INFO: [Train]: [038][00058/00058]  Loss=0.9665  cls_loss=0.6698  reg_loss=0.2967  lr_det=3.2e-05  mem=8731MB
2025-07-26 07:54:35 Train INFO: [Train]: Epoch 39 started
2025-07-26 08:15:37 Train INFO: [Train]: [039][00050/00058]  Loss=0.9774  cls_loss=0.6789  reg_loss=0.2985  lr_det=3.0e-05  mem=8731MB
2025-07-26 08:18:15 Train INFO: [Train]: [039][00058/00058]  Loss=0.9701  cls_loss=0.6733  reg_loss=0.2967  lr_det=2.9e-05  mem=8731MB
2025-07-26 08:18:17 Train INFO: [Train]: Epoch 40 started
2025-07-26 08:39:12 Train INFO: [Train]: [040][00050/00058]  Loss=0.9707  cls_loss=0.6723  reg_loss=0.2983  lr_det=2.7e-05  mem=8731MB
2025-07-26 08:41:52 Train INFO: [Train]: [040][00058/00058]  Loss=0.9644  cls_loss=0.6676  reg_loss=0.2968  lr_det=2.7e-05  mem=8731MB
2025-07-26 08:41:52 Train INFO: [Train]: Epoch 41 started
2025-07-26 09:02:29 Train INFO: [Train]: [041][00050/00058]  Loss=0.9913  cls_loss=0.6888  reg_loss=0.3025  lr_det=2.5e-05  mem=8731MB
2025-07-26 09:05:15 Train INFO: [Train]: [041][00058/00058]  Loss=0.9913  cls_loss=0.6875  reg_loss=0.3038  lr_det=2.4e-05  mem=8731MB
2025-07-26 09:05:16 Train INFO: [Train]: Epoch 42 started
2025-07-26 09:26:13 Train INFO: [Train]: [042][00050/00058]  Loss=0.9771  cls_loss=0.6741  reg_loss=0.3030  lr_det=2.2e-05  mem=8731MB
2025-07-26 09:28:55 Train INFO: [Train]: [042][00058/00058]  Loss=0.9661  cls_loss=0.6666  reg_loss=0.2995  lr_det=2.2e-05  mem=8731MB
2025-07-26 09:28:56 Train INFO: [Train]: Epoch 43 started
2025-07-26 09:49:23 Train INFO: [Train]: [043][00050/00058]  Loss=0.9821  cls_loss=0.6762  reg_loss=0.3059  lr_det=2.0e-05  mem=8731MB
2025-07-26 09:52:08 Train INFO: [Train]: [043][00058/00058]  Loss=0.9806  cls_loss=0.6758  reg_loss=0.3048  lr_det=2.0e-05  mem=8731MB
2025-07-26 09:52:09 Train INFO: [Train]: Epoch 44 started
2025-07-26 10:12:30 Train INFO: [Train]: [044][00050/00058]  Loss=0.9763  cls_loss=0.6760  reg_loss=0.3004  lr_det=1.8e-05  mem=8731MB
2025-07-26 10:15:12 Train INFO: [Train]: [044][00058/00058]  Loss=0.9756  cls_loss=0.6744  reg_loss=0.3012  lr_det=1.7e-05  mem=8731MB
2025-07-26 10:15:13 Train INFO: [Train]: Epoch 45 started
2025-07-26 10:35:44 Train INFO: [Train]: [045][00050/00058]  Loss=0.9690  cls_loss=0.6712  reg_loss=0.2977  lr_det=1.5e-05  mem=8731MB
2025-07-26 10:38:18 Train INFO: [Train]: [045][00058/00058]  Loss=0.9606  cls_loss=0.6655  reg_loss=0.2951  lr_det=1.5e-05  mem=8731MB
2025-07-26 10:38:19 Train INFO: [Train]: Epoch 46 started
2025-07-26 10:58:47 Train INFO: [Train]: [046][00050/00058]  Loss=0.9847  cls_loss=0.6816  reg_loss=0.3032  lr_det=1.3e-05  mem=8731MB
2025-07-26 11:01:16 Train INFO: [Train]: [046][00058/00058]  Loss=0.9729  cls_loss=0.6742  reg_loss=0.2987  lr_det=1.3e-05  mem=8731MB
2025-07-26 11:01:17 Train INFO: [Train]: Epoch 47 started
2025-07-26 11:21:34 Train INFO: [Train]: [047][00050/00058]  Loss=0.9834  cls_loss=0.6788  reg_loss=0.3046  lr_det=1.2e-05  mem=8731MB
2025-07-26 11:24:11 Train INFO: [Train]: [047][00058/00058]  Loss=0.9785  cls_loss=0.6755  reg_loss=0.3030  lr_det=1.1e-05  mem=8731MB
2025-07-26 11:24:12 Train INFO: [Train]: Epoch 48 started
2025-07-26 11:45:05 Train INFO: [Train]: [048][00050/00058]  Loss=0.9706  cls_loss=0.6709  reg_loss=0.2997  lr_det=9.8e-06  mem=8731MB
2025-07-26 11:47:39 Train INFO: [Train]: [048][00058/00058]  Loss=0.9748  cls_loss=0.6744  reg_loss=0.3004  lr_det=9.6e-06  mem=8731MB
2025-07-26 11:47:39 Train INFO: [Train]: Epoch 49 started
2025-07-26 12:08:27 Train INFO: [Train]: [049][00050/00058]  Loss=0.9660  cls_loss=0.6684  reg_loss=0.2976  lr_det=8.2e-06  mem=8731MB
2025-07-26 12:11:06 Train INFO: [Train]: [049][00058/00058]  Loss=0.9650  cls_loss=0.6678  reg_loss=0.2972  lr_det=8.0e-06  mem=8731MB
2025-07-26 12:11:07 Train INFO: [Train]: Epoch 50 started
2025-07-26 12:32:43 Train INFO: [Train]: [050][00050/00058]  Loss=0.9750  cls_loss=0.6745  reg_loss=0.3005  lr_det=6.7e-06  mem=8731MB
2025-07-26 12:35:25 Train INFO: [Train]: [050][00058/00058]  Loss=0.9673  cls_loss=0.6695  reg_loss=0.2979  lr_det=6.5e-06  mem=8731MB
2025-07-26 12:35:25 Train INFO: [Train]: Epoch 51 started
2025-07-26 12:56:37 Train INFO: [Train]: [051][00050/00058]  Loss=1.0009  cls_loss=0.6919  reg_loss=0.3090  lr_det=5.3e-06  mem=8731MB
2025-07-26 12:59:11 Train INFO: [Train]: [051][00058/00058]  Loss=0.9966  cls_loss=0.6887  reg_loss=0.3080  lr_det=5.2e-06  mem=8731MB
2025-07-26 12:59:12 Train INFO: [Train]: Epoch 52 started
2025-07-26 13:20:09 Train INFO: [Train]: [052][00050/00058]  Loss=0.9538  cls_loss=0.6617  reg_loss=0.2921  lr_det=4.1e-06  mem=8731MB
2025-07-26 13:23:00 Train INFO: [Train]: [052][00058/00058]  Loss=0.9629  cls_loss=0.6674  reg_loss=0.2955  lr_det=4.0e-06  mem=8731MB
2025-07-26 13:23:00 Train INFO: [Train]: Epoch 53 started
2025-07-26 13:44:08 Train INFO: [Train]: [053][00050/00058]  Loss=0.9752  cls_loss=0.6697  reg_loss=0.3055  lr_det=3.1e-06  mem=8731MB
2025-07-26 13:47:07 Train INFO: [Train]: [053][00058/00058]  Loss=0.9717  cls_loss=0.6682  reg_loss=0.3035  lr_det=2.9e-06  mem=8731MB
2025-07-26 13:47:08 Train INFO: [Train]: Epoch 54 started
2025-07-26 14:08:13 Train INFO: [Train]: [054][00050/00058]  Loss=0.9685  cls_loss=0.6683  reg_loss=0.3002  lr_det=2.2e-06  mem=8731MB
2025-07-26 14:11:00 Train INFO: [Train]: [054][00058/00058]  Loss=0.9677  cls_loss=0.6676  reg_loss=0.3001  lr_det=2.0e-06  mem=8731MB
2025-07-26 14:11:00 Train INFO: [Train]: Epoch 55 started
2025-07-26 14:31:31 Train INFO: [Train]: [055][00050/00058]  Loss=0.9859  cls_loss=0.6809  reg_loss=0.3050  lr_det=1.4e-06  mem=8731MB
2025-07-26 14:34:17 Train INFO: [Train]: [055][00058/00058]  Loss=0.9744  cls_loss=0.6727  reg_loss=0.3017  lr_det=1.3e-06  mem=8731MB
2025-07-26 14:34:19 Train INFO: [Train]: Epoch 56 started
2025-07-26 14:54:19 Train INFO: [Train]: [056][00050/00058]  Loss=0.9724  cls_loss=0.6734  reg_loss=0.2991  lr_det=8.2e-07  mem=8731MB
2025-07-26 14:57:09 Train INFO: [Train]: [056][00058/00058]  Loss=0.9670  cls_loss=0.6699  reg_loss=0.2971  lr_det=7.5e-07  mem=8731MB
2025-07-26 14:57:10 Train INFO: [Train]: Epoch 57 started
2025-07-26 15:17:48 Train INFO: [Train]: [057][00050/00058]  Loss=0.9677  cls_loss=0.6712  reg_loss=0.2965  lr_det=3.9e-07  mem=8731MB
2025-07-26 15:20:29 Train INFO: [Train]: [057][00058/00058]  Loss=0.9696  cls_loss=0.6721  reg_loss=0.2975  lr_det=3.4e-07  mem=8731MB
2025-07-26 15:20:30 Train INFO: [Train]: Epoch 58 started
2025-07-26 15:41:17 Train INFO: [Train]: [058][00050/00058]  Loss=0.9780  cls_loss=0.6760  reg_loss=0.3020  lr_det=1.2e-07  mem=8731MB
2025-07-26 15:43:59 Train INFO: [Train]: [058][00058/00058]  Loss=0.9745  cls_loss=0.6734  reg_loss=0.3011  lr_det=9.4e-08  mem=8731MB
2025-07-26 15:44:00 Train INFO: [Train]: Epoch 59 started
2025-07-26 16:04:04 Train INFO: [Train]: [059][00050/00058]  Loss=0.9679  cls_loss=0.6706  reg_loss=0.2973  lr_det=1.2e-08  mem=8731MB
2025-07-26 16:06:51 Train INFO: [Train]: [059][00058/00058]  Loss=0.9637  cls_loss=0.6681  reg_loss=0.2956  lr_det=1.0e-08  mem=8731MB
2025-07-26 16:06:52 Train INFO: Training Over...

2025-07-28 09:23:03 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:23:03 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:23:05 Test INFO: Using single GPU testing...
2025-07-28 09:23:05 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/latest.pth
2025-07-28 09:24:34 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:24:34 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:24:35 Test INFO: Using single GPU testing...
2025-07-28 09:24:35 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:24:36 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:24:36 Test INFO: Using Model EMA...
2025-07-28 09:24:36 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:24:36 Test INFO: Testing Starts...

2025-07-28 09:26:45 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:26:46 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:26:47 Test INFO: Using single GPU testing...
2025-07-28 09:26:47 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:26:47 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:26:47 Test INFO: Using Model EMA...
2025-07-28 09:26:47 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:26:47 Test INFO: Testing Starts...

2025-07-28 09:29:27 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:29:28 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:29:29 Test INFO: Using single GPU testing...
2025-07-28 09:29:29 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:29:29 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:29:29 Test INFO: Using Model EMA...
2025-07-28 09:29:29 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:29:29 Test INFO: Testing Starts...

2025-07-28 09:30:29 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:30:30 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:30:31 Test INFO: Using single GPU testing...
2025-07-28 09:30:31 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:30:31 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:30:31 Test INFO: Using Model EMA...
2025-07-28 09:30:31 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:30:31 Test INFO: Testing Starts...

2025-07-28 09:33:45 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:33:46 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:33:47 Test INFO: Using single GPU testing...
2025-07-28 09:33:47 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:33:47 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:33:47 Test INFO: Using Model EMA...
2025-07-28 09:33:47 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:33:47 Test INFO: Testing Starts...

2025-07-28 09:37:08 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:37:09 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='feats', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:37:10 Test INFO: Using single GPU testing...
2025-07-28 09:37:10 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:37:10 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:37:10 Test INFO: Using Model EMA...
2025-07-28 09:37:10 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:37:10 Test INFO: Testing Starts...

2025-07-28 09:39:13 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:39:14 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:39:15 Test INFO: Using single GPU testing...
2025-07-28 09:39:15 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:39:15 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:39:15 Test INFO: Using Model EMA...
2025-07-28 09:39:15 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:39:15 Test INFO: Testing Starts...

2025-07-28 09:43:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:43:33 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:43:34 Test INFO: Using single GPU testing...
2025-07-28 09:43:34 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:43:35 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:43:35 Test INFO: Using Model EMA...
2025-07-28 09:43:35 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:43:35 Test INFO: Testing Starts...

2025-07-28 09:45:21 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:45:21 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict()
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:45:23 Test INFO: Using single GPU testing...
2025-07-28 09:45:23 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:45:23 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:45:23 Test INFO: Using Model EMA...
2025-07-28 09:45:23 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:45:23 Test INFO: Testing Starts...

2025-07-28 09:51:00 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 09:51:00 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 09:51:01 Test INFO: Using single GPU testing...
2025-07-28 09:51:01 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 09:51:02 Test INFO: Checkpoint is epoch 59.
2025-07-28 09:51:02 Test INFO: Using Model EMA...
2025-07-28 09:51:02 Test INFO: Using Automatic Mixed Precision...
2025-07-28 09:51:02 Test INFO: Testing Starts...

2025-07-28 10:05:08 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:05:09 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:05:10 Test INFO: Using single GPU testing...
2025-07-28 10:05:10 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:05:10 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:05:10 Test INFO: Using Model EMA...
2025-07-28 10:05:10 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:05:10 Test INFO: Testing Starts...

2025-07-28 10:09:04 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:09:04 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:09:05 Test INFO: Using single GPU testing...
2025-07-28 10:09:05 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:09:06 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:09:06 Test INFO: Using Model EMA...
2025-07-28 10:09:06 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:09:06 Test INFO: Testing Starts...

2025-07-28 10:13:47 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:13:47 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(external_cls=None, nms=None, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:13:48 Test INFO: Using single GPU testing...
2025-07-28 10:13:48 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:13:49 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:13:49 Test INFO: Using Model EMA...
2025-07-28 10:13:49 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:13:49 Test INFO: Testing Starts...

2025-07-28 10:17:37 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:17:38 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:17:39 Test INFO: Using single GPU testing...
2025-07-28 10:17:39 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:17:39 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:17:39 Test INFO: Using Model EMA...
2025-07-28 10:17:39 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:17:39 Test INFO: Testing Starts...

2025-07-28 10:19:53 Test INFO: Testing Over...

2025-07-28 10:27:00 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:27:00 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename='data/pku_mmd/annotations/pku_mmd_anno.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.2,
        0.3,
        0.4,
        0.5,
    ],
    top_k=[
        1,
        5,
    ],
    type='mAP')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:27:01 Test INFO: Using single GPU testing...
2025-07-28 10:27:01 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:27:02 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:27:02 Test INFO: Using Model EMA...
2025-07-28 10:27:02 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:27:02 Test INFO: Testing Starts...

2025-07-28 10:32:30 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:32:31 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename=None,
    subset='test',
    tiou_thresholds=[
        0.1,
        0.2,
        0.3,
        0.4,
        0.5,
    ],
    top_k=[
        1,
        5,
    ],
    type='mAP_epic')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:32:32 Test INFO: Using single GPU testing...
2025-07-28 10:32:32 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:32:32 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:32:32 Test INFO: Using Model EMA...
2025-07-28 10:32:32 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:32:32 Test INFO: Testing Starts...

2025-07-28 10:37:51 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:37:52 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename=None,
    subset='test',
    tiou_thresholds=[
        0.1,
        0.2,
        0.3,
        0.4,
        0.5,
    ],
    top_k=[
        1,
        5,
    ],
    type='mAP_EPIC')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:37:53 Test INFO: Using single GPU testing...
2025-07-28 10:37:53 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:37:53 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:37:53 Test INFO: Using Model EMA...
2025-07-28 10:37:53 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:37:53 Test INFO: Testing Starts...

2025-07-28 10:43:23 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:43:24 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    ground_truth_filename=None,
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_EPIC')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:43:25 Test INFO: Using single GPU testing...
2025-07-28 10:43:25 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:43:25 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:43:25 Test INFO: Using Model EMA...
2025-07-28 10:43:25 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:43:25 Test INFO: Testing Starts...

2025-07-28 10:50:06 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:50:06 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:50:07 Test INFO: Using single GPU testing...
2025-07-28 10:50:07 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:50:08 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:50:08 Test INFO: Using Model EMA...
2025-07-28 10:50:08 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:50:08 Test INFO: Testing Starts...

2025-07-28 10:52:34 Test INFO: Evaluation starts...
2025-07-28 10:53:26 Test INFO: Loaded annotations from test subset.
2025-07-28 10:53:26 Test INFO: Number of ground truth instances: 2647
2025-07-28 10:53:26 Test INFO: Number of predictions: 40392
2025-07-28 10:53:26 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 10:53:26 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 10:53:26 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 10:53:26 Test INFO: Testing Over...

2025-07-28 10:54:35 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 10:54:36 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 10:54:37 Test INFO: Using single GPU testing...
2025-07-28 10:54:37 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 10:54:37 Test INFO: Checkpoint is epoch 59.
2025-07-28 10:54:37 Test INFO: Using Model EMA...
2025-07-28 10:54:37 Test INFO: Using Automatic Mixed Precision...
2025-07-28 10:54:37 Test INFO: Testing Starts...

2025-07-28 10:57:07 Test INFO: Evaluation starts...
2025-07-28 10:58:00 Test INFO: Loaded annotations from test subset.
2025-07-28 10:58:00 Test INFO: Number of ground truth instances: 2647
2025-07-28 10:58:00 Test INFO: Number of predictions: 40392
2025-07-28 10:58:00 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 10:58:00 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 10:58:00 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 10:58:00 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 10:58:00 Test INFO: Prediction labels: [51]
2025-07-28 10:58:00 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 10:58:00 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 10:58:00 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 10:58:00 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 10:58:00 Test INFO: Testing Over...

2025-07-28 11:00:53 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 11:00:54 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=False, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 11:00:55 Test INFO: Using single GPU testing...
2025-07-28 11:00:55 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 11:00:55 Test INFO: Checkpoint is epoch 59.
2025-07-28 11:00:55 Test INFO: Using Model EMA...
2025-07-28 11:00:55 Test INFO: Using Automatic Mixed Precision...
2025-07-28 11:00:55 Test INFO: Testing Starts...

2025-07-28 11:03:24 Test INFO: Evaluation starts...
2025-07-28 11:04:17 Test INFO: Loaded annotations from test subset.
2025-07-28 11:04:17 Test INFO: Number of ground truth instances: 2647
2025-07-28 11:04:17 Test INFO: Number of predictions: 40392
2025-07-28 11:04:17 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 11:04:17 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:04:17 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:04:17 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 11:04:17 Test INFO: Prediction labels: [51]
2025-07-28 11:04:17 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 11:04:17 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 11:04:17 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 11:04:17 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 11:04:17 Test INFO: Testing Over...

2025-07-28 11:05:12 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 11:05:13 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 11:05:14 Test INFO: Using single GPU testing...
2025-07-28 11:05:14 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 11:05:14 Test INFO: Checkpoint is epoch 59.
2025-07-28 11:05:14 Test INFO: Using Model EMA...
2025-07-28 11:05:14 Test INFO: Using Automatic Mixed Precision...
2025-07-28 11:05:14 Test INFO: Testing Starts...

2025-07-28 11:08:40 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 11:08:40 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 11:08:41 Test INFO: Using single GPU testing...
2025-07-28 11:08:41 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 11:08:42 Test INFO: Checkpoint is epoch 59.
2025-07-28 11:08:42 Test INFO: Using Model EMA...
2025-07-28 11:08:42 Test INFO: Using Automatic Mixed Precision...
2025-07-28 11:08:42 Test INFO: Testing Starts...

2025-07-28 11:11:03 Test INFO: Evaluation starts...
2025-07-28 11:11:55 Test INFO: Loaded annotations from test subset.
2025-07-28 11:11:55 Test INFO: Number of ground truth instances: 2647
2025-07-28 11:11:55 Test INFO: Number of predictions: 40392
2025-07-28 11:11:55 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 11:11:55 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:11:55 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 11:11:55 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 11:11:55 Test INFO: Prediction labels: [51]
2025-07-28 11:11:55 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 11:11:55 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 11:11:55 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 11:11:55 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 11:11:55 Test INFO: Testing Over...

2025-07-28 13:02:14 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:02:15 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:02:17 Test INFO: Using single GPU testing...
2025-07-28 13:02:17 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/latest.pth
2025-07-28 13:02:29 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:02:29 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:02:30 Test INFO: Using single GPU testing...
2025-07-28 13:02:30 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-28 13:02:31 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:02:31 Test INFO: Using Model EMA...
2025-07-28 13:02:31 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:02:31 Test INFO: Testing Starts...

2025-07-28 13:05:24 Test INFO: Evaluation starts...
2025-07-28 13:06:22 Test INFO: Loaded annotations from test subset.
2025-07-28 13:06:22 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:06:22 Test INFO: Number of predictions: 40392
2025-07-28 13:06:22 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:06:22 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:06:22 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:06:22 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:06:22 Test INFO: Prediction labels: [51]
2025-07-28 13:06:22 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:06:22 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 13:06:22 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:06:22 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:06:23 Test INFO: Testing Over...

2025-07-28 13:07:36 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:07:37 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:07:38 Test INFO: Using single GPU testing...
2025-07-28 13:07:38 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/latest.pth
2025-07-28 13:08:16 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:08:16 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:08:17 Test INFO: Using single GPU testing...
2025-07-28 13:08:17 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:08:18 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:08:18 Test INFO: Using Model EMA...
2025-07-28 13:08:18 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:08:18 Test INFO: Testing Starts...

2025-07-28 13:10:56 Test INFO: Evaluation starts...
2025-07-28 13:11:49 Test INFO: Loaded annotations from test subset.
2025-07-28 13:11:49 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:11:49 Test INFO: Number of predictions: 40392
2025-07-28 13:11:49 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:11:49 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:11:49 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:11:49 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:11:49 Test INFO: Prediction labels: [51]
2025-07-28 13:11:49 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:11:49 Test INFO: Prediction time range: 0.00 - 0.00
2025-07-28 13:11:49 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:11:49 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:11:50 Test INFO: Testing Over...

2025-07-28 13:18:56 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:18:57 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:18:58 Test INFO: Using single GPU testing...
2025-07-28 13:18:58 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:18:58 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:18:58 Test INFO: Using Model EMA...
2025-07-28 13:18:58 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:18:58 Test INFO: Testing Starts...

2025-07-28 13:21:29 Test INFO: Evaluation starts...
2025-07-28 13:22:22 Test INFO: Loaded annotations from test subset.
2025-07-28 13:22:22 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:22:22 Test INFO: Number of predictions: 40392
2025-07-28 13:22:22 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:22:22 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:22:22 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:22:22 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:22:22 Test INFO: Prediction labels: [51]
2025-07-28 13:22:22 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:22:22 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:22:22 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:22:23 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:22:23 Test INFO: Testing Over...

2025-07-28 13:23:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:23:34 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:23:35 Test INFO: Using single GPU testing...
2025-07-28 13:23:35 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:23:35 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:23:35 Test INFO: Using Model EMA...
2025-07-28 13:23:35 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:23:35 Test INFO: Testing Starts...

2025-07-28 13:26:09 Test INFO: Evaluation starts...
2025-07-28 13:27:01 Test INFO: Loaded annotations from test subset.
2025-07-28 13:27:01 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:27:01 Test INFO: Number of predictions: 40392
2025-07-28 13:27:01 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:27:01 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:27:01 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:27:01 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:27:01 Test INFO: Prediction labels: [51]
2025-07-28 13:27:01 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:27:01 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:27:01 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:27:01 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:27:01 Test INFO: Testing Over...

2025-07-28 13:27:54 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:27:55 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:27:56 Test INFO: Using single GPU testing...
2025-07-28 13:27:56 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:27:56 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:27:56 Test INFO: Using Model EMA...
2025-07-28 13:27:56 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:27:56 Test INFO: Testing Starts...

2025-07-28 13:30:33 Test INFO: Evaluation starts...
2025-07-28 13:31:25 Test INFO: Loaded annotations from test subset.
2025-07-28 13:31:25 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:31:25 Test INFO: Number of predictions: 40392
2025-07-28 13:31:25 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:31:25 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:31:25 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:31:25 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:31:25 Test INFO: Prediction labels: [51]
2025-07-28 13:31:25 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:31:25 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:31:25 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:31:25 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:31:25 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:31:26 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:31:26 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:31:26 Test INFO: Testing Over...

2025-07-28 13:33:47 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:33:48 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:33:49 Test INFO: Using single GPU testing...
2025-07-28 13:33:49 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:33:49 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:33:49 Test INFO: Using Model EMA...
2025-07-28 13:33:49 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:33:49 Test INFO: Testing Starts...

2025-07-28 13:36:23 Test INFO: Evaluation starts...
2025-07-28 13:37:16 Test INFO: Loaded annotations from test subset.
2025-07-28 13:37:16 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:37:16 Test INFO: Number of predictions: 40392
2025-07-28 13:37:16 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:37:16 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:37:16 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:37:16 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:37:16 Test INFO: Prediction labels: [51]
2025-07-28 13:37:16 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:37:16 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:37:16 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:37:16 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:37:17 Test INFO: Testing Over...

2025-07-28 13:40:49 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:40:49 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:40:50 Test INFO: Using single GPU testing...
2025-07-28 13:40:50 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:40:51 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:40:51 Test INFO: Using Model EMA...
2025-07-28 13:40:51 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:40:51 Test INFO: Testing Starts...

2025-07-28 13:43:22 Test INFO: Evaluation starts...
2025-07-28 13:44:16 Test INFO: Loaded annotations from test subset.
2025-07-28 13:44:16 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:44:16 Test INFO: Number of predictions: 40392
2025-07-28 13:44:16 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:44:16 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:44:16 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:44:16 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:44:16 Test INFO: Prediction labels: [29 50 17 38  7  4 44  8 45 27]
2025-07-28 13:44:16 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:44:16 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:44:16 Test INFO: Average-mAP:  nan (%)
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.10 is  nan%
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.30 is  nan%
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.50 is  nan%
2025-07-28 13:44:16 Test INFO: mAP at tIoU 0.70 is  nan%
2025-07-28 13:44:16 Test INFO: Testing Over...

2025-07-28 13:46:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 13:46:34 Test INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0004, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 13:46:35 Test INFO: Using single GPU testing...
2025-07-28 13:46:35 Test INFO: Loading checkpoint from: .\work_dirs\e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0\checkpoint\epoch_59.pth
2025-07-28 13:46:35 Test INFO: Checkpoint is epoch 59.
2025-07-28 13:46:35 Test INFO: Using Model EMA...
2025-07-28 13:46:35 Test INFO: Using Automatic Mixed Precision...
2025-07-28 13:46:35 Test INFO: Testing Starts...

2025-07-28 13:49:06 Test INFO: Evaluation starts...
2025-07-28 13:50:00 Test INFO: Loaded annotations from test subset.
2025-07-28 13:50:00 Test INFO: Number of ground truth instances: 2647
2025-07-28 13:50:00 Test INFO: Number of predictions: 40392
2025-07-28 13:50:00 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7]
2025-07-28 13:50:00 Test INFO: Ground truth video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:50:00 Test INFO: Prediction video IDs: ['0291-L' '0291-M' '0291-R' '0292-L' '0292-M' '0292-R' '0293-L' '0293-M'
 '0293-R' '0294-L']
2025-07-28 13:50:00 Test INFO: Ground truth labels: [28  7 15 38 39 29 13 11 47  3]
2025-07-28 13:50:00 Test INFO: Prediction labels: [29 50 17 38  7  4 44  8 45 27]
2025-07-28 13:50:00 Test INFO: Ground truth time range: 2.27 - 246.60
2025-07-28 13:50:00 Test INFO: Prediction time range: -0.00 - 1.10
2025-07-28 13:50:00 Test INFO: Average-mAP: 0.00 (%)
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.10 is 0.00%
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.30 is 0.00%
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.50 is 0.00%
2025-07-28 13:50:00 Test INFO: mAP at tIoU 0.70 is 0.00%
2025-07-28 13:50:01 Test INFO: Testing Over...

2025-07-28 14:11:57 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 14:11:58 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=32, num_workers=8),
    train=dict(batch_size=32, num_workers=8))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 14:12:00 Train INFO: Using single GPU training...
2025-07-28 14:12:00 Train INFO: Using Model EMA...
2025-07-28 14:12:00 Train INFO: Using Automatic Mixed Precision...
2025-07-28 14:12:00 Train INFO: Freeze the backbone...
2025-07-28 14:12:00 Train INFO: Training Starts...

2025-07-28 14:12:00 Train INFO: [Train]: Epoch 0 started
2025-07-28 14:24:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 14:24:26 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=32, num_workers=6),
    train=dict(batch_size=32, num_workers=6))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 14:24:29 Train INFO: Using single GPU training...
2025-07-28 14:24:29 Train INFO: Using Model EMA...
2025-07-28 14:24:29 Train INFO: Using Automatic Mixed Precision...
2025-07-28 14:24:29 Train INFO: Freeze the backbone...
2025-07-28 14:24:29 Train INFO: Training Starts...

2025-07-28 14:24:29 Train INFO: [Train]: Epoch 0 started
2025-07-28 14:49:00 Train INFO: [Train]: [000][00029/00029]  Loss=1.9804  cls_loss=1.1638  reg_loss=0.8167  lr_det=9.7e-06  mem=16326MB
2025-07-28 14:49:01 Train INFO: [Train]: Epoch 1 started
2025-07-28 15:13:31 Train INFO: [Train]: [001][00029/00029]  Loss=1.3897  cls_loss=1.0406  reg_loss=0.3491  lr_det=2.0e-05  mem=16326MB
2025-07-28 15:13:33 Train INFO: [Train]: Epoch 2 started
2025-07-28 15:31:36 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:31:37 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                stride=256,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:32:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:32:05 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        fps=30,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb'))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None, nms=None, save_dict=True, sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:32:07 Train INFO: Using single GPU training...
2025-07-28 15:32:07 Train INFO: Using Model EMA...
2025-07-28 15:32:07 Train INFO: Using Automatic Mixed Precision...
2025-07-28 15:32:07 Train INFO: Freeze the backbone...
2025-07-28 15:32:07 Train INFO: Training Starts...

2025-07-28 15:32:07 Train INFO: [Train]: Epoch 0 started
2025-07-28 15:44:14 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:44:15 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:50:08 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 15:50:09 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=30,
        ioa_thresh=0.75,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.25))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 15:50:10 Train INFO: Using single GPU training...
2025-07-28 15:50:10 Train INFO: Using Model EMA...
2025-07-28 15:50:10 Train INFO: Using Automatic Mixed Precision...
2025-07-28 15:50:10 Train INFO: Freeze the backbone...
2025-07-28 15:50:10 Train INFO: Training Starts...

2025-07-28 15:50:10 Train INFO: [Train]: Epoch 0 started
2025-07-28 16:10:29 Train INFO: [Train]: [000][00050/00688]  Loss=1.9508  cls_loss=1.0481  reg_loss=0.9027  lr_det=7.3e-07  mem=8731MB
2025-07-28 16:30:31 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 16:30:31 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                frame_interval=3,
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                frame_interval=3,
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 16:31:12 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 16:31:13 Train INFO: Config: 
actions_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx'
anno_dir = 'F:/dataset/pku-mmd/Train_Label_PKU_final'
chunk_num = 32
dataset = dict(
    test=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        split='test',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        test_mode=True,
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        actions_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
        anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
        feature_stride=1,
        fps=10,
        ioa_thresh=0.5,
        offset_frames=0,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        split='train',
        split_file=
        'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
        type='PKUMMDLoader',
        video_dir='F:/dataset/pku-mmd/rgb',
        window_overlap_ratio=0.5))
dataset_type = 'PKUMMDLoader'
evaluation = dict(
    actions_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/Actions.xlsx',
    anno_dir='F:/dataset/pku-mmd/Train_Label_PKU_final',
    ground_truth_filename=None,
    split_file=
    'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
    ],
    type='mAP_PKU_MMD')
img_size = 160
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b (n c) t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=None,
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=False,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=1024,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
num_classes = 51
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=5e-05,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    external_cls=None,
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True,
    sliding_window=False)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
split_file = 'F:/dataset/pku-mmd/Split-20250716T021010Z-1-001/Split/cross-subject.txt'
train_cfg = dict(max_epochs=60)
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = './work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(checkpoint_interval=2, end_epoch=60, logging_interval=50)

2025-07-28 16:31:15 Train INFO: Using single GPU training...
2025-07-28 16:31:15 Train INFO: Using Model EMA...
2025-07-28 16:31:15 Train INFO: Using Automatic Mixed Precision...
2025-07-28 16:31:15 Train INFO: Freeze the backbone...
2025-07-28 16:31:15 Train INFO: Training Starts...

2025-07-28 16:31:15 Train INFO: [Train]: Epoch 0 started
2025-07-28 16:51:59 Train INFO: [Train]: [000][00050/01068]  Loss=1.9783  cls_loss=1.0587  reg_loss=0.9197  lr_det=4.7e-07  mem=8731MB
2025-07-28 17:12:10 Train INFO: [Train]: [000][00100/01068]  Loss=2.0043  cls_loss=1.1061  reg_loss=0.8982  lr_det=9.4e-07  mem=8731MB
2025-07-28 17:27:29 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:27:30 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=6),
    train=dict(batch_size=2, num_workers=6),
    val=dict(batch_size=2, num_workers=6))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=60)

2025-07-28 17:28:42 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:28:42 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=6),
    train=dict(batch_size=2, num_workers=6),
    val=dict(batch_size=2, num_workers=6))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=50,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=60)

2025-07-28 17:28:43 Train INFO: training subset: 942 videos
2025-07-28 17:28:43 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:28:44 Train INFO: Using single GPU training...
2025-07-28 17:28:44 Train INFO: Using Model EMA...
2025-07-28 17:28:44 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:28:44 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:28:44 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:28:44 Train INFO: Training Starts...

2025-07-28 17:28:44 Train INFO: [Train]: Epoch 0 started (Total iterations: 471)
2025-07-28 17:31:26 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:31:26 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=6),
    train=dict(batch_size=2, num_workers=6))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:31:27 Train INFO: training subset: 942 videos
2025-07-28 17:31:27 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:31:28 Train INFO: Using single GPU training...
2025-07-28 17:31:28 Train INFO: Using Model EMA...
2025-07-28 17:31:28 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:31:28 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:31:28 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:31:28 Train INFO: Training Starts...

2025-07-28 17:31:28 Train INFO: [Train]: Epoch 0 started (Total iterations: 471)
2025-07-28 17:32:25 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:32:25 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=False)
scale_factor = 1
scheduler = dict(
    max_epoch=100, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:32:26 Train INFO: training subset: 942 videos
2025-07-28 17:32:26 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:32:26 Train INFO: Using single GPU training...
2025-07-28 17:32:26 Train INFO: Using Model EMA...
2025-07-28 17:32:27 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:32:27 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:32:27 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:32:27 Train INFO: Training Starts...

2025-07-28 17:32:27 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 17:48:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:48:10 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=False,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t h w -> b c t',
                    reduction='mean',
                    type='Reduce'),
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='(b t1) c t -> b c (t1 t)',
                    t1=32,
                    type='Rearrange'),
                dict(keys=[
                    'feats',
                ], size=512, type='Interpolate'),
            ],
            pre_processing_pipeline=[
                dict(
                    keys=[
                        'frames',
                    ],
                    ops='b n c (t1 t) h w -> (b t1) n c t h w',
                    t1=32,
                    type='Rearrange'),
            ]),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:48:10 Train INFO: training subset: 942 videos
2025-07-28 17:48:10 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:48:11 Train INFO: Using single GPU training...
2025-07-28 17:48:11 Train INFO: Using Model EMA...
2025-07-28 17:48:11 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:48:11 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.pos_embed
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.patch_embed.projection.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.patch_embed.projection.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.0.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.1.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.2.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.3.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.4.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.5.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.6.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.7.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.8.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.9.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.10.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.q_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.v_bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.qkv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.attn.proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm2.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.norm2.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.0.0.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.mlp.layers.1.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.gamma
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.dwconv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.conv.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.down_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.blocks.11.adapter.up_proj.bias
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.fc_norm.weight
2025-07-28 17:48:11 Train INFO: Backbone parameter: model.fc_norm.bias
2025-07-28 17:48:11 Train INFO: Training Starts...

2025-07-28 17:48:11 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 17:54:28 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 17:54:28 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    n=1,
                    ops='(b n) c t -> b n c t',
                    type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 17:54:29 Train INFO: training subset: 942 videos
2025-07-28 17:54:29 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 17:54:30 Train INFO: Using single GPU training...
2025-07-28 17:54:30 Train INFO: Using Model EMA...
2025-07-28 17:54:30 Train INFO: Using Automatic Mixed Precision...
2025-07-28 17:54:30 Train INFO: GPU Memory: 24.0 GB
2025-07-28 17:54:30 Train INFO: Freeze the backbone...
2025-07-28 17:54:30 Train INFO: Training Starts...

2025-07-28 17:54:30 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:05:30 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 18:05:31 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ], ops='b n c t -> b c t', type='Rearrange'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 18:05:31 Train INFO: training subset: 942 videos
2025-07-28 18:05:31 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 18:05:32 Train INFO: Using single GPU training...
2025-07-28 18:05:32 Train INFO: Using Model EMA...
2025-07-28 18:05:32 Train INFO: Using Automatic Mixed Precision...
2025-07-28 18:05:32 Train INFO: GPU Memory: 24.0 GB
2025-07-28 18:05:32 Train INFO: Freeze the backbone...
2025-07-28 18:05:32 Train INFO: Training Starts...

2025-07-28 18:05:32 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:09:31 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 18:09:32 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 18:09:32 Train INFO: training subset: 942 videos
2025-07-28 18:09:32 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 18:09:33 Train INFO: Using single GPU training...
2025-07-28 18:09:33 Train INFO: Using Model EMA...
2025-07-28 18:09:33 Train INFO: Using Automatic Mixed Precision...
2025-07-28 18:09:33 Train INFO: GPU Memory: 24.0 GB
2025-07-28 18:09:33 Train INFO: Freeze the backbone...
2025-07-28 18:09:33 Train INFO: Training Starts...

2025-07-28 18:09:33 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:12:56 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-28 18:12:57 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-28 18:12:58 Train INFO: training subset: 942 videos
2025-07-28 18:12:58 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-28 18:12:58 Train INFO: Using single GPU training...
2025-07-28 18:12:58 Train INFO: Using Model EMA...
2025-07-28 18:12:58 Train INFO: Using Automatic Mixed Precision...
2025-07-28 18:12:58 Train INFO: GPU Memory: 24.0 GB
2025-07-28 18:12:58 Train INFO: Freeze the backbone...
2025-07-28 18:12:58 Train INFO: Training Starts...

2025-07-28 18:12:58 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-28 18:17:57 Train INFO: [Train]: [000][00010/00058] (18.6%)  Loss=1.6274  cls_loss=0.8716  reg_loss=0.7558  lr_det=3.4e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1305s  iter_time=299.103s  fwd=2.223s/bwd=0.076s/opt=0.007s
2025-07-28 18:22:30 Train INFO: [Train]: [000][00020/00058] (35.6%)  Loss=1.7138  cls_loss=0.9560  reg_loss=0.7577  lr_det=6.8e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=273.047s
2025-07-28 18:25:34 Train INFO: [Train]: [000][00030/00058] (52.5%)  Loss=1.6507  cls_loss=0.9818  reg_loss=0.6690  lr_det=1.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=683s  iter_time=184.199s
2025-07-28 18:30:07 Train INFO: [Train]: [000][00040/00058] (69.5%)  Loss=1.5867  cls_loss=0.9983  reg_loss=0.5884  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=452s  iter_time=272.550s
2025-07-28 18:33:15 Train INFO: [Train]: [000][00050/00058] (86.4%)  Loss=1.5577  cls_loss=1.0158  reg_loss=0.5420  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=191s  iter_time=188.115s
2025-07-28 18:35:54 Train INFO: [Train]: [000][00058/00058] (100.0%)  Loss=1.5216  cls_loss=1.0101  reg_loss=0.5116  lr_det=2.0e-05  GPU=1354MB(alloc)/4212MB(reserved)/9063MB(max)  ETA=0s  iter_time=159.014s
2025-07-28 18:35:55 Train INFO: [Train]: Epoch 0 completed in 1376.8s (avg 23.336s/iter)
2025-07-28 18:35:55 Train INFO: [Train]: Final Loss=1.5216
2025-07-28 18:35:55 Train INFO: [Train]: Epoch 1 started (Total iterations: 59)
2025-07-28 18:40:58 Train INFO: [Train]: [001][00010/00058] (18.6%)  Loss=1.3968  cls_loss=1.0492  reg_loss=0.3476  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1322s  iter_time=302.910s  fwd=2.143s/bwd=0.064s/opt=0.010s
2025-07-28 18:45:15 Train INFO: [Train]: [001][00020/00058] (35.6%)  Loss=1.3410  cls_loss=1.0079  reg_loss=0.3331  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1014s  iter_time=257.690s
2025-07-28 18:48:23 Train INFO: [Train]: [001][00030/00058] (52.5%)  Loss=1.3258  cls_loss=1.0015  reg_loss=0.3242  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=676s  iter_time=187.902s
2025-07-28 18:53:06 Train INFO: [Train]: [001][00040/00058] (69.5%)  Loss=1.3106  cls_loss=0.9910  reg_loss=0.3196  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=453s  iter_time=282.229s
2025-07-28 18:56:43 Train INFO: [Train]: [001][00050/00058] (86.4%)  Loss=1.3027  cls_loss=0.9858  reg_loss=0.3169  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=196s  iter_time=217.130s
2025-07-28 18:59:45 Train INFO: [Train]: [001][00058/00058] (100.0%)  Loss=1.2932  cls_loss=0.9794  reg_loss=0.3139  lr_det=4.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=182.651s
2025-07-28 18:59:46 Train INFO: [Train]: Epoch 1 completed in 1431.7s (avg 24.265s/iter)
2025-07-28 18:59:46 Train INFO: [Train]: Final Loss=1.2932
2025-07-28 18:59:48 Train INFO: Checkpoint saved at epoch 1
2025-07-28 18:59:48 Train INFO: [Train]: Epoch 2 started (Total iterations: 59)
2025-07-28 19:05:15 Train INFO: [Train]: [002][00010/00058] (18.6%)  Loss=1.3129  cls_loss=0.9869  reg_loss=0.3260  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1426s  iter_time=326.903s  fwd=2.206s/bwd=0.075s/opt=0.011s
2025-07-28 19:10:08 Train INFO: [Train]: [002][00020/00058] (35.6%)  Loss=1.2793  cls_loss=0.9679  reg_loss=0.3114  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1121s  iter_time=292.815s
2025-07-28 19:13:32 Train INFO: [Train]: [002][00030/00058] (52.5%)  Loss=1.2876  cls_loss=0.9750  reg_loss=0.3126  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=745s  iter_time=204.785s
2025-07-28 19:18:21 Train INFO: [Train]: [002][00040/00058] (69.5%)  Loss=1.2823  cls_loss=0.9697  reg_loss=0.3126  lr_det=5.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=489s  iter_time=288.194s
2025-07-28 19:21:48 Train INFO: [Train]: [002][00050/00058] (86.4%)  Loss=1.2707  cls_loss=0.9627  reg_loss=0.3080  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=207s  iter_time=207.388s
2025-07-28 19:24:49 Train INFO: [Train]: [002][00058/00058] (100.0%)  Loss=1.2715  cls_loss=0.9616  reg_loss=0.3099  lr_det=6.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=181.019s
2025-07-28 19:24:50 Train INFO: [Train]: Epoch 2 completed in 1502.3s (avg 25.462s/iter)
2025-07-28 19:24:50 Train INFO: [Train]: Final Loss=1.2715
2025-07-28 19:24:50 Train INFO: [Train]: Epoch 3 started (Total iterations: 59)
2025-07-28 19:30:10 Train INFO: [Train]: [003][00010/00058] (18.6%)  Loss=1.2802  cls_loss=0.9583  reg_loss=0.3219  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1396s  iter_time=320.001s  fwd=2.132s/bwd=0.083s/opt=0.009s
2025-07-28 19:34:45 Train INFO: [Train]: [003][00020/00058] (35.6%)  Loss=1.2266  cls_loss=0.9130  reg_loss=0.3136  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1077s  iter_time=275.297s
2025-07-28 19:37:55 Train INFO: [Train]: [003][00030/00058] (52.5%)  Loss=1.2071  cls_loss=0.8926  reg_loss=0.3145  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=709s  iter_time=189.996s
2025-07-28 19:42:39 Train INFO: [Train]: [003][00040/00058] (69.5%)  Loss=1.1858  cls_loss=0.8720  reg_loss=0.3138  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=469s  iter_time=283.612s
2025-07-28 19:45:46 Train INFO: [Train]: [003][00050/00058] (86.4%)  Loss=1.1555  cls_loss=0.8445  reg_loss=0.3110  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=197s  iter_time=187.270s
2025-07-28 19:48:40 Train INFO: [Train]: [003][00058/00058] (100.0%)  Loss=1.1321  cls_loss=0.8241  reg_loss=0.3080  lr_det=8.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=174.118s
2025-07-28 19:48:42 Train INFO: [Train]: Epoch 3 completed in 1431.4s (avg 24.261s/iter)
2025-07-28 19:48:42 Train INFO: [Train]: Final Loss=1.1321
2025-07-28 19:48:43 Train INFO: Checkpoint saved at epoch 3
2025-07-28 19:48:43 Train INFO: [Train]: Epoch 4 started (Total iterations: 59)
2025-07-28 19:53:54 Train INFO: [Train]: [004][00010/00058] (18.6%)  Loss=1.0889  cls_loss=0.7636  reg_loss=0.3252  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1357s  iter_time=311.052s  fwd=2.063s/bwd=0.041s/opt=0.006s
2025-07-28 19:58:27 Train INFO: [Train]: [004][00020/00058] (35.6%)  Loss=1.0705  cls_loss=0.7504  reg_loss=0.3202  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1057s  iter_time=273.225s
2025-07-28 20:01:45 Train INFO: [Train]: [004][00030/00058] (52.5%)  Loss=1.0430  cls_loss=0.7313  reg_loss=0.3117  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=707s  iter_time=198.257s
2025-07-28 20:06:16 Train INFO: [Train]: [004][00040/00058] (69.5%)  Loss=1.0413  cls_loss=0.7292  reg_loss=0.3121  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=462s  iter_time=270.658s
2025-07-28 20:09:35 Train INFO: [Train]: [004][00050/00058] (86.4%)  Loss=1.0225  cls_loss=0.7167  reg_loss=0.3058  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=197s  iter_time=199.713s
2025-07-28 20:12:28 Train INFO: [Train]: [004][00058/00058] (100.0%)  Loss=1.0322  cls_loss=0.7226  reg_loss=0.3096  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=172.213s
2025-07-28 20:12:29 Train INFO: [Train]: Epoch 4 completed in 1426.3s (avg 24.175s/iter)
2025-07-28 20:12:29 Train INFO: [Train]: Final Loss=1.0322
2025-07-28 20:12:29 Train INFO: [Train]: Epoch 5 started (Total iterations: 59)
2025-07-28 20:17:42 Train INFO: [Train]: [005][00010/00058] (18.6%)  Loss=1.0442  cls_loss=0.7311  reg_loss=0.3131  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1367s  iter_time=313.209s  fwd=2.084s/bwd=0.058s/opt=0.006s
2025-07-28 20:22:22 Train INFO: [Train]: [005][00020/00058] (35.6%)  Loss=1.0195  cls_loss=0.7113  reg_loss=0.3082  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1073s  iter_time=279.884s
2025-07-28 20:25:41 Train INFO: [Train]: [005][00030/00058] (52.5%)  Loss=1.0168  cls_loss=0.7108  reg_loss=0.3060  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=715s  iter_time=198.749s
2025-07-28 20:30:19 Train INFO: [Train]: [005][00040/00058] (69.5%)  Loss=1.0156  cls_loss=0.7105  reg_loss=0.3051  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=470s  iter_time=278.657s
2025-07-28 20:33:35 Train INFO: [Train]: [005][00050/00058] (86.4%)  Loss=1.0172  cls_loss=0.7096  reg_loss=0.3076  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=199s  iter_time=196.119s
2025-07-28 20:36:32 Train INFO: [Train]: [005][00058/00058] (100.0%)  Loss=1.0072  cls_loss=0.7022  reg_loss=0.3050  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=176.539s
2025-07-28 20:36:33 Train INFO: [Train]: Epoch 5 completed in 1444.3s (avg 24.480s/iter)
2025-07-28 20:36:33 Train INFO: [Train]: Final Loss=1.0072
2025-07-28 20:36:34 Train INFO: Checkpoint saved at epoch 5
2025-07-28 20:36:34 Train INFO: [Train]: Epoch 6 started (Total iterations: 59)
2025-07-28 20:42:05 Train INFO: [Train]: [006][00010/00058] (18.6%)  Loss=1.0298  cls_loss=0.7141  reg_loss=0.3157  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1443s  iter_time=330.652s  fwd=2.083s/bwd=0.049s/opt=0.007s
2025-07-28 20:46:43 Train INFO: [Train]: [006][00020/00058] (35.6%)  Loss=1.0201  cls_loss=0.7064  reg_loss=0.3137  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1102s  iter_time=278.103s
2025-07-28 20:50:11 Train INFO: [Train]: [006][00030/00058] (52.5%)  Loss=1.0081  cls_loss=0.6983  reg_loss=0.3098  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=737s  iter_time=207.656s
2025-07-28 20:55:33 Train INFO: [Train]: [006][00040/00058] (69.5%)  Loss=1.0220  cls_loss=0.7075  reg_loss=0.3146  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=500s  iter_time=322.006s
2025-07-28 20:59:09 Train INFO: [Train]: [006][00050/00058] (86.4%)  Loss=1.0134  cls_loss=0.7022  reg_loss=0.3112  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=213s  iter_time=216.851s
2025-07-28 21:02:02 Train INFO: [Train]: [006][00058/00058] (100.0%)  Loss=1.0113  cls_loss=0.7017  reg_loss=0.3096  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=172.217s
2025-07-28 21:02:03 Train INFO: [Train]: Epoch 6 completed in 1528.8s (avg 25.912s/iter)
2025-07-28 21:02:03 Train INFO: [Train]: Final Loss=1.0113
2025-07-28 21:02:03 Train INFO: [Train]: Epoch 7 started (Total iterations: 59)
2025-07-28 21:07:50 Train INFO: [Train]: [007][00010/00058] (18.6%)  Loss=1.0124  cls_loss=0.7008  reg_loss=0.3116  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1514s  iter_time=346.903s  fwd=2.171s/bwd=0.101s/opt=0.014s
2025-07-28 21:12:48 Train INFO: [Train]: [007][00020/00058] (35.6%)  Loss=1.0088  cls_loss=0.6999  reg_loss=0.3089  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1167s  iter_time=297.960s
2025-07-28 21:16:21 Train INFO: [Train]: [007][00030/00058] (52.5%)  Loss=1.0154  cls_loss=0.7032  reg_loss=0.3122  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=775s  iter_time=213.560s
2025-07-28 21:21:19 Train INFO: [Train]: [007][00040/00058] (69.5%)  Loss=1.0133  cls_loss=0.7028  reg_loss=0.3105  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=508s  iter_time=298.064s
2025-07-28 21:24:56 Train INFO: [Train]: [007][00050/00058] (86.4%)  Loss=1.0065  cls_loss=0.6991  reg_loss=0.3074  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=215s  iter_time=216.712s
2025-07-28 21:28:11 Train INFO: [Train]: [007][00058/00058] (100.0%)  Loss=0.9987  cls_loss=0.6941  reg_loss=0.3046  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=194.490s
2025-07-28 21:28:12 Train INFO: [Train]: Epoch 7 completed in 1568.9s (avg 26.592s/iter)
2025-07-28 21:28:12 Train INFO: [Train]: Final Loss=0.9987
2025-07-28 21:28:13 Train INFO: Checkpoint saved at epoch 7
2025-07-28 21:28:13 Train INFO: [Train]: Epoch 8 started (Total iterations: 59)
2025-07-28 21:33:53 Train INFO: [Train]: [008][00010/00058] (18.6%)  Loss=1.0550  cls_loss=0.7314  reg_loss=0.3236  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1482s  iter_time=339.543s  fwd=2.140s/bwd=0.081s/opt=0.011s
2025-07-28 21:38:53 Train INFO: [Train]: [008][00020/00058] (35.6%)  Loss=1.0089  cls_loss=0.6990  reg_loss=0.3099  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1159s  iter_time=300.821s
2025-07-28 21:42:15 Train INFO: [Train]: [008][00030/00058] (52.5%)  Loss=1.0086  cls_loss=0.6992  reg_loss=0.3094  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=760s  iter_time=201.454s
2025-07-28 21:47:14 Train INFO: [Train]: [008][00040/00058] (69.5%)  Loss=1.0155  cls_loss=0.7068  reg_loss=0.3088  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=501s  iter_time=299.344s
2025-07-28 21:50:40 Train INFO: [Train]: [008][00050/00058] (86.4%)  Loss=1.0001  cls_loss=0.6959  reg_loss=0.3042  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=211s  iter_time=205.666s
2025-07-28 21:53:54 Train INFO: [Train]: [008][00058/00058] (100.0%)  Loss=1.0030  cls_loss=0.6974  reg_loss=0.3056  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=194.253s
2025-07-28 21:53:55 Train INFO: [Train]: Epoch 8 completed in 1542.3s (avg 26.140s/iter)
2025-07-28 21:53:55 Train INFO: [Train]: Final Loss=1.0030
2025-07-28 21:53:55 Train INFO: [Train]: Epoch 9 started (Total iterations: 59)
2025-07-28 21:59:02 Train INFO: [Train]: [009][00010/00058] (18.6%)  Loss=1.0418  cls_loss=0.7191  reg_loss=0.3227  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1339s  iter_time=306.785s  fwd=2.049s/bwd=0.054s/opt=0.010s
2025-07-28 22:03:22 Train INFO: [Train]: [009][00020/00058] (35.6%)  Loss=1.0042  cls_loss=0.6946  reg_loss=0.3096  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1026s  iter_time=260.077s
2025-07-28 22:06:28 Train INFO: [Train]: [009][00030/00058] (52.5%)  Loss=1.0116  cls_loss=0.6990  reg_loss=0.3127  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=186.119s
2025-07-28 22:10:52 Train INFO: [Train]: [009][00040/00058] (69.5%)  Loss=0.9980  cls_loss=0.6899  reg_loss=0.3081  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=446s  iter_time=263.900s
2025-07-28 22:14:03 Train INFO: [Train]: [009][00050/00058] (86.4%)  Loss=0.9940  cls_loss=0.6872  reg_loss=0.3067  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=190.427s
2025-07-28 22:16:42 Train INFO: [Train]: [009][00058/00058] (100.0%)  Loss=0.9832  cls_loss=0.6794  reg_loss=0.3038  lr_det=9.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=159.338s
2025-07-28 22:16:43 Train INFO: [Train]: Epoch 9 completed in 1367.4s (avg 23.176s/iter)
2025-07-28 22:16:43 Train INFO: [Train]: Final Loss=0.9832
2025-07-28 22:16:43 Train INFO: Checkpoint saved at epoch 9
2025-07-28 22:16:43 Train INFO: [Train]: Epoch 10 started (Total iterations: 59)
2025-07-28 22:21:48 Train INFO: [Train]: [010][00010/00058] (18.6%)  Loss=1.1050  cls_loss=0.7643  reg_loss=0.3406  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1329s  iter_time=304.587s  fwd=2.052s/bwd=0.059s/opt=0.012s
2025-07-28 22:26:01 Train INFO: [Train]: [010][00020/00058] (35.6%)  Loss=1.0485  cls_loss=0.7258  reg_loss=0.3227  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1009s  iter_time=252.956s
2025-07-28 22:29:18 Train INFO: [Train]: [010][00030/00058] (52.5%)  Loss=1.0291  cls_loss=0.7127  reg_loss=0.3163  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=682s  iter_time=197.350s
2025-07-28 22:33:41 Train INFO: [Train]: [010][00040/00058] (69.5%)  Loss=1.0257  cls_loss=0.7094  reg_loss=0.3162  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=262.724s
2025-07-28 22:36:42 Train INFO: [Train]: [010][00050/00058] (86.4%)  Loss=1.0239  cls_loss=0.7086  reg_loss=0.3153  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=188s  iter_time=180.633s
2025-07-28 22:39:22 Train INFO: [Train]: [010][00058/00058] (100.0%)  Loss=1.0161  cls_loss=0.7033  reg_loss=0.3128  lr_det=9.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.714s
2025-07-28 22:39:23 Train INFO: [Train]: Epoch 10 completed in 1359.7s (avg 23.046s/iter)
2025-07-28 22:39:23 Train INFO: [Train]: Final Loss=1.0161
2025-07-28 22:39:23 Train INFO: [Train]: Epoch 11 started (Total iterations: 59)
2025-07-28 22:44:07 Train INFO: [Train]: [011][00010/00058] (18.6%)  Loss=0.9867  cls_loss=0.6844  reg_loss=0.3022  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1238s  iter_time=283.746s  fwd=2.027s/bwd=0.036s/opt=0.007s
2025-07-28 22:48:19 Train INFO: [Train]: [011][00020/00058] (35.6%)  Loss=1.0043  cls_loss=0.6977  reg_loss=0.3066  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=970s  iter_time=252.449s
2025-07-28 22:51:22 Train INFO: [Train]: [011][00030/00058] (52.5%)  Loss=1.0116  cls_loss=0.7008  reg_loss=0.3107  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=650s  iter_time=183.203s
2025-07-28 22:55:45 Train INFO: [Train]: [011][00040/00058] (69.5%)  Loss=0.9948  cls_loss=0.6891  reg_loss=0.3058  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=431s  iter_time=262.447s
2025-07-28 22:58:48 Train INFO: [Train]: [011][00050/00058] (86.4%)  Loss=1.0001  cls_loss=0.6917  reg_loss=0.3084  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=183s  iter_time=183.353s
2025-07-28 23:01:18 Train INFO: [Train]: [011][00058/00058] (100.0%)  Loss=0.9918  cls_loss=0.6853  reg_loss=0.3065  lr_det=9.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=149.602s
2025-07-28 23:01:19 Train INFO: [Train]: Epoch 11 completed in 1315.5s (avg 22.297s/iter)
2025-07-28 23:01:19 Train INFO: [Train]: Final Loss=0.9918
2025-07-28 23:01:19 Train INFO: Checkpoint saved at epoch 11
2025-07-28 23:01:19 Train INFO: [Train]: Epoch 12 started (Total iterations: 59)
2025-07-28 23:06:00 Train INFO: [Train]: [012][00010/00058] (18.6%)  Loss=0.9986  cls_loss=0.6907  reg_loss=0.3078  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1226s  iter_time=280.958s  fwd=2.048s/bwd=0.035s/opt=0.009s
2025-07-28 23:10:10 Train INFO: [Train]: [012][00020/00058] (35.6%)  Loss=1.0041  cls_loss=0.6942  reg_loss=0.3099  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=960s  iter_time=249.813s
2025-07-28 23:13:11 Train INFO: [Train]: [012][00030/00058] (52.5%)  Loss=0.9865  cls_loss=0.6827  reg_loss=0.3037  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=643s  iter_time=181.602s
2025-07-28 23:17:43 Train INFO: [Train]: [012][00040/00058] (69.5%)  Loss=0.9884  cls_loss=0.6847  reg_loss=0.3037  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=432s  iter_time=271.228s
2025-07-28 23:20:44 Train INFO: [Train]: [012][00050/00058] (86.4%)  Loss=0.9912  cls_loss=0.6864  reg_loss=0.3048  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=183s  iter_time=181.097s
2025-07-28 23:23:19 Train INFO: [Train]: [012][00058/00058] (100.0%)  Loss=0.9877  cls_loss=0.6837  reg_loss=0.3040  lr_det=9.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=155.288s
2025-07-28 23:23:20 Train INFO: [Train]: Epoch 12 completed in 1320.7s (avg 22.385s/iter)
2025-07-28 23:23:20 Train INFO: [Train]: Final Loss=0.9877
2025-07-28 23:23:20 Train INFO: [Train]: Epoch 13 started (Total iterations: 59)
2025-07-28 23:28:26 Train INFO: [Train]: [013][00010/00058] (18.6%)  Loss=1.0349  cls_loss=0.7145  reg_loss=0.3204  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1336s  iter_time=306.060s  fwd=2.077s/bwd=0.053s/opt=0.012s
2025-07-28 23:32:42 Train INFO: [Train]: [013][00020/00058] (35.6%)  Loss=1.0047  cls_loss=0.6946  reg_loss=0.3101  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1016s  iter_time=255.659s
2025-07-28 23:35:57 Train INFO: [Train]: [013][00030/00058] (52.5%)  Loss=0.9922  cls_loss=0.6874  reg_loss=0.3049  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=684s  iter_time=195.877s
2025-07-28 23:40:21 Train INFO: [Train]: [013][00040/00058] (69.5%)  Loss=1.0148  cls_loss=0.7016  reg_loss=0.3132  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=448s  iter_time=263.951s
2025-07-28 23:43:26 Train INFO: [Train]: [013][00050/00058] (86.4%)  Loss=1.0084  cls_loss=0.6980  reg_loss=0.3104  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=184.845s
2025-07-28 23:46:15 Train INFO: [Train]: [013][00058/00058] (100.0%)  Loss=1.0086  cls_loss=0.6987  reg_loss=0.3099  lr_det=9.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=168.704s
2025-07-28 23:46:16 Train INFO: [Train]: Epoch 13 completed in 1375.8s (avg 23.319s/iter)
2025-07-28 23:46:16 Train INFO: [Train]: Final Loss=1.0086
2025-07-28 23:46:16 Train INFO: Checkpoint saved at epoch 13
2025-07-28 23:46:16 Train INFO: [Train]: Epoch 14 started (Total iterations: 59)
2025-07-28 23:51:17 Train INFO: [Train]: [014][00010/00058] (18.6%)  Loss=0.9702  cls_loss=0.6773  reg_loss=0.2929  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1313s  iter_time=300.903s  fwd=2.037s/bwd=0.082s/opt=0.010s
2025-07-28 23:55:42 Train INFO: [Train]: [014][00020/00058] (35.6%)  Loss=0.9590  cls_loss=0.6695  reg_loss=0.2896  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1023s  iter_time=264.330s
2025-07-28 23:58:50 Train INFO: [Train]: [014][00030/00058] (52.5%)  Loss=0.9857  cls_loss=0.6859  reg_loss=0.2998  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=681s  iter_time=188.298s
2025-07-29 00:02:56 Train INFO: [Train]: [014][00040/00058] (69.5%)  Loss=0.9838  cls_loss=0.6854  reg_loss=0.2984  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=439s  iter_time=245.730s
2025-07-29 00:06:07 Train INFO: [Train]: [014][00050/00058] (86.4%)  Loss=0.9827  cls_loss=0.6843  reg_loss=0.2985  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=190.995s
2025-07-29 00:08:49 Train INFO: [Train]: [014][00058/00058] (100.0%)  Loss=0.9819  cls_loss=0.6832  reg_loss=0.2987  lr_det=9.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=162.167s
2025-07-29 00:08:49 Train INFO: [Train]: Epoch 14 completed in 1353.2s (avg 22.935s/iter)
2025-07-29 00:08:49 Train INFO: [Train]: Final Loss=0.9819
2025-07-29 00:08:49 Train INFO: [Train]: Epoch 15 started (Total iterations: 59)
2025-07-29 00:13:44 Train INFO: [Train]: [015][00010/00058] (18.6%)  Loss=1.0217  cls_loss=0.7073  reg_loss=0.3144  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1286s  iter_time=294.668s  fwd=2.052s/bwd=0.087s/opt=0.010s
2025-07-29 00:17:55 Train INFO: [Train]: [015][00020/00058] (35.6%)  Loss=1.0292  cls_loss=0.7113  reg_loss=0.3179  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=986s  iter_time=250.481s
2025-07-29 00:21:04 Train INFO: [Train]: [015][00030/00058] (52.5%)  Loss=1.0193  cls_loss=0.7061  reg_loss=0.3132  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=663s  iter_time=188.981s
2025-07-29 00:25:22 Train INFO: [Train]: [015][00040/00058] (69.5%)  Loss=1.0230  cls_loss=0.7085  reg_loss=0.3145  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=436s  iter_time=257.983s
2025-07-29 00:28:32 Train INFO: [Train]: [015][00050/00058] (86.4%)  Loss=1.0065  cls_loss=0.6968  reg_loss=0.3097  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=186s  iter_time=190.494s
2025-07-29 00:31:08 Train INFO: [Train]: [015][00058/00058] (100.0%)  Loss=0.9933  cls_loss=0.6874  reg_loss=0.3059  lr_det=9.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=155.758s
2025-07-29 00:31:09 Train INFO: [Train]: Epoch 15 completed in 1339.1s (avg 22.697s/iter)
2025-07-29 00:31:09 Train INFO: [Train]: Final Loss=0.9933
2025-07-29 00:31:09 Train INFO: Checkpoint saved at epoch 15
2025-07-29 00:31:09 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-29 00:36:12 Train INFO: [Train]: [016][00010/00058] (18.6%)  Loss=1.0779  cls_loss=0.7418  reg_loss=0.3361  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1320s  iter_time=302.546s  fwd=2.066s/bwd=0.044s/opt=0.004s
2025-07-29 00:40:31 Train INFO: [Train]: [016][00020/00058] (35.6%)  Loss=1.0174  cls_loss=0.7011  reg_loss=0.3163  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1016s  iter_time=259.047s
2025-07-29 00:43:52 Train INFO: [Train]: [016][00030/00058] (52.5%)  Loss=1.0021  cls_loss=0.6930  reg_loss=0.3092  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=689s  iter_time=201.156s
2025-07-29 00:48:26 Train INFO: [Train]: [016][00040/00058] (69.5%)  Loss=0.9996  cls_loss=0.6906  reg_loss=0.3091  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=455s  iter_time=274.413s
2025-07-29 00:51:41 Train INFO: [Train]: [016][00050/00058] (86.4%)  Loss=1.0080  cls_loss=0.6960  reg_loss=0.3120  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=193s  iter_time=194.792s
2025-07-29 00:54:20 Train INFO: [Train]: [016][00058/00058] (100.0%)  Loss=1.0004  cls_loss=0.6915  reg_loss=0.3089  lr_det=8.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=158.885s
2025-07-29 00:54:21 Train INFO: [Train]: Epoch 16 completed in 1391.6s (avg 23.586s/iter)
2025-07-29 00:54:21 Train INFO: [Train]: Final Loss=1.0004
2025-07-29 00:54:21 Train INFO: [Train]: Epoch 17 started (Total iterations: 59)
2025-07-29 00:59:16 Train INFO: [Train]: [017][00010/00058] (18.6%)  Loss=1.0374  cls_loss=0.7173  reg_loss=0.3202  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1287s  iter_time=294.993s  fwd=2.075s/bwd=0.065s/opt=0.011s
2025-07-29 01:03:43 Train INFO: [Train]: [017][00020/00058] (35.6%)  Loss=0.9765  cls_loss=0.6779  reg_loss=0.2986  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1017s  iter_time=267.141s
2025-07-29 01:06:49 Train INFO: [Train]: [017][00030/00058] (52.5%)  Loss=0.9774  cls_loss=0.6784  reg_loss=0.2990  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=676s  iter_time=186.058s
2025-07-29 01:11:12 Train INFO: [Train]: [017][00040/00058] (69.5%)  Loss=0.9962  cls_loss=0.6913  reg_loss=0.3049  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=444s  iter_time=263.244s
2025-07-29 01:14:15 Train INFO: [Train]: [017][00050/00058] (86.4%)  Loss=0.9916  cls_loss=0.6875  reg_loss=0.3041  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=183.235s
2025-07-29 01:16:50 Train INFO: [Train]: [017][00058/00058] (100.0%)  Loss=0.9795  cls_loss=0.6786  reg_loss=0.3008  lr_det=8.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=154.633s
2025-07-29 01:16:51 Train INFO: [Train]: Epoch 17 completed in 1350.0s (avg 22.882s/iter)
2025-07-29 01:16:51 Train INFO: [Train]: Final Loss=0.9795
2025-07-29 01:16:51 Train INFO: Checkpoint saved at epoch 17
2025-07-29 01:16:51 Train INFO: [Train]: Epoch 18 started (Total iterations: 59)
2025-07-29 01:21:42 Train INFO: [Train]: [018][00010/00058] (18.6%)  Loss=1.0358  cls_loss=0.7280  reg_loss=0.3077  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1267s  iter_time=290.296s  fwd=2.064s/bwd=0.044s/opt=0.004s
2025-07-29 01:26:17 Train INFO: [Train]: [018][00020/00058] (35.6%)  Loss=1.0281  cls_loss=0.7216  reg_loss=0.3065  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1023s  iter_time=275.019s
2025-07-29 01:29:16 Train INFO: [Train]: [018][00030/00058] (52.5%)  Loss=1.0185  cls_loss=0.7130  reg_loss=0.3055  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=673s  iter_time=179.767s
2025-07-29 01:33:32 Train INFO: [Train]: [018][00040/00058] (69.5%)  Loss=1.0004  cls_loss=0.6989  reg_loss=0.3015  lr_det=8.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=439s  iter_time=255.599s
2025-07-29 01:36:51 Train INFO: [Train]: [018][00050/00058] (86.4%)  Loss=0.9941  cls_loss=0.6955  reg_loss=0.2986  lr_det=8.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=188s  iter_time=198.469s
2025-07-29 01:39:32 Train INFO: [Train]: [018][00058/00058] (100.0%)  Loss=0.9894  cls_loss=0.6906  reg_loss=0.2988  lr_det=8.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=161.447s
2025-07-29 01:39:33 Train INFO: [Train]: Epoch 18 completed in 1361.3s (avg 23.073s/iter)
2025-07-29 01:39:33 Train INFO: [Train]: Final Loss=0.9894
2025-07-29 01:39:33 Train INFO: [Train]: Epoch 19 started (Total iterations: 59)
2025-07-29 01:44:26 Train INFO: [Train]: [019][00010/00058] (18.6%)  Loss=1.0044  cls_loss=0.6977  reg_loss=0.3066  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1282s  iter_time=293.734s  fwd=2.068s/bwd=0.056s/opt=0.004s
2025-07-29 01:49:05 Train INFO: [Train]: [019][00020/00058] (35.6%)  Loss=1.0005  cls_loss=0.6938  reg_loss=0.3067  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=278.365s
2025-07-29 01:52:24 Train INFO: [Train]: [019][00030/00058] (52.5%)  Loss=1.0035  cls_loss=0.6945  reg_loss=0.3090  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=697s  iter_time=199.453s
2025-07-29 01:56:31 Train INFO: [Train]: [019][00040/00058] (69.5%)  Loss=0.9998  cls_loss=0.6912  reg_loss=0.3085  lr_det=8.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=246.476s
2025-07-29 01:59:37 Train INFO: [Train]: [019][00050/00058] (86.4%)  Loss=0.9914  cls_loss=0.6863  reg_loss=0.3051  lr_det=8.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=186.423s
2025-07-29 02:02:18 Train INFO: [Train]: [019][00058/00058] (100.0%)  Loss=0.9911  cls_loss=0.6861  reg_loss=0.3051  lr_det=8.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.903s
2025-07-29 02:02:19 Train INFO: [Train]: Epoch 19 completed in 1366.1s (avg 23.154s/iter)
2025-07-29 02:02:19 Train INFO: [Train]: Final Loss=0.9911
2025-07-29 02:02:19 Train INFO: Checkpoint saved at epoch 19
2025-07-29 02:02:19 Train INFO: [Train]: Epoch 20 started (Total iterations: 59)
2025-07-29 02:07:22 Train INFO: [Train]: [020][00010/00058] (18.6%)  Loss=1.0233  cls_loss=0.7073  reg_loss=0.3161  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1319s  iter_time=302.180s  fwd=2.082s/bwd=0.081s/opt=0.012s
2025-07-29 02:11:36 Train INFO: [Train]: [020][00020/00058] (35.6%)  Loss=0.9905  cls_loss=0.6852  reg_loss=0.3053  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1008s  iter_time=254.622s
2025-07-29 02:14:34 Train INFO: [Train]: [020][00030/00058] (52.5%)  Loss=0.9893  cls_loss=0.6858  reg_loss=0.3035  lr_det=8.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=663s  iter_time=177.607s
2025-07-29 02:19:06 Train INFO: [Train]: [020][00040/00058] (69.5%)  Loss=0.9947  cls_loss=0.6884  reg_loss=0.3063  lr_det=8.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=442s  iter_time=272.393s
2025-07-29 02:22:09 Train INFO: [Train]: [020][00050/00058] (86.4%)  Loss=0.9886  cls_loss=0.6843  reg_loss=0.3044  lr_det=8.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=182.373s
2025-07-29 02:24:49 Train INFO: [Train]: [020][00058/00058] (100.0%)  Loss=0.9920  cls_loss=0.6868  reg_loss=0.3053  lr_det=8.1e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.004s
2025-07-29 02:24:49 Train INFO: [Train]: Epoch 20 completed in 1349.9s (avg 22.880s/iter)
2025-07-29 02:24:49 Train INFO: [Train]: Final Loss=0.9920
2025-07-29 02:24:49 Train INFO: [Train]: Epoch 21 started (Total iterations: 59)
2025-07-29 02:29:52 Train INFO: [Train]: [021][00010/00058] (18.6%)  Loss=0.9807  cls_loss=0.6789  reg_loss=0.3018  lr_det=8.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1319s  iter_time=302.372s  fwd=2.061s/bwd=0.075s/opt=0.007s
2025-07-29 02:34:04 Train INFO: [Train]: [021][00020/00058] (35.6%)  Loss=0.9685  cls_loss=0.6699  reg_loss=0.2986  lr_det=8.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1003s  iter_time=251.956s
2025-07-29 02:37:27 Train INFO: [Train]: [021][00030/00058] (52.5%)  Loss=0.9836  cls_loss=0.6823  reg_loss=0.3013  lr_det=7.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=684s  iter_time=203.128s
2025-07-29 02:41:37 Train INFO: [Train]: [021][00040/00058] (69.5%)  Loss=0.9842  cls_loss=0.6838  reg_loss=0.3004  lr_det=7.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=442s  iter_time=249.697s
2025-07-29 02:44:54 Train INFO: [Train]: [021][00050/00058] (86.4%)  Loss=0.9759  cls_loss=0.6779  reg_loss=0.2979  lr_det=7.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=197.699s
2025-07-29 02:47:20 Train INFO: [Train]: [021][00058/00058] (100.0%)  Loss=0.9758  cls_loss=0.6781  reg_loss=0.2977  lr_det=7.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=146.265s
2025-07-29 02:47:21 Train INFO: [Train]: Epoch 21 completed in 1351.9s (avg 22.913s/iter)
2025-07-29 02:47:21 Train INFO: [Train]: Final Loss=0.9758
2025-07-29 02:47:22 Train INFO: Checkpoint saved at epoch 21
2025-07-29 02:47:22 Train INFO: [Train]: Epoch 22 started (Total iterations: 59)
2025-07-29 02:52:29 Train INFO: [Train]: [022][00010/00058] (18.6%)  Loss=1.0247  cls_loss=0.7063  reg_loss=0.3185  lr_det=7.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1341s  iter_time=307.335s  fwd=2.025s/bwd=0.034s/opt=0.005s
2025-07-29 02:56:32 Train INFO: [Train]: [022][00020/00058] (35.6%)  Loss=0.9934  cls_loss=0.6878  reg_loss=0.3056  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=996s  iter_time=242.937s
2025-07-29 02:59:38 Train INFO: [Train]: [022][00030/00058] (52.5%)  Loss=1.0015  cls_loss=0.6942  reg_loss=0.3074  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=665s  iter_time=185.852s
2025-07-29 03:04:02 Train INFO: [Train]: [022][00040/00058] (69.5%)  Loss=0.9936  cls_loss=0.6884  reg_loss=0.3052  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=439s  iter_time=263.742s
2025-07-29 03:07:16 Train INFO: [Train]: [022][00050/00058] (86.4%)  Loss=0.9940  cls_loss=0.6885  reg_loss=0.3056  lr_det=7.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=193.860s
2025-07-29 03:09:56 Train INFO: [Train]: [022][00058/00058] (100.0%)  Loss=0.9843  cls_loss=0.6816  reg_loss=0.3028  lr_det=7.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.146s
2025-07-29 03:09:56 Train INFO: [Train]: Epoch 22 completed in 1354.6s (avg 22.960s/iter)
2025-07-29 03:09:56 Train INFO: [Train]: Final Loss=0.9843
2025-07-29 03:09:56 Train INFO: [Train]: Epoch 23 started (Total iterations: 59)
2025-07-29 03:15:12 Train INFO: [Train]: [023][00010/00058] (18.6%)  Loss=0.9787  cls_loss=0.6793  reg_loss=0.2994  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1376s  iter_time=315.309s  fwd=2.080s/bwd=0.071s/opt=0.005s
2025-07-29 03:19:28 Train INFO: [Train]: [023][00020/00058] (35.6%)  Loss=0.9750  cls_loss=0.6753  reg_loss=0.2996  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=256.748s
2025-07-29 03:22:38 Train INFO: [Train]: [023][00030/00058] (52.5%)  Loss=0.9913  cls_loss=0.6869  reg_loss=0.3045  lr_det=7.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=688s  iter_time=189.163s
2025-07-29 03:26:54 Train INFO: [Train]: [023][00040/00058] (69.5%)  Loss=0.9996  cls_loss=0.6915  reg_loss=0.3081  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=256.413s
2025-07-29 03:29:59 Train INFO: [Train]: [023][00050/00058] (86.4%)  Loss=0.9915  cls_loss=0.6849  reg_loss=0.3066  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=185.299s
2025-07-29 03:32:41 Train INFO: [Train]: [023][00058/00058] (100.0%)  Loss=0.9890  cls_loss=0.6833  reg_loss=0.3057  lr_det=7.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=161.634s
2025-07-29 03:32:42 Train INFO: [Train]: Epoch 23 completed in 1365.3s (avg 23.141s/iter)
2025-07-29 03:32:42 Train INFO: [Train]: Final Loss=0.9890
2025-07-29 03:32:42 Train INFO: Checkpoint saved at epoch 23
2025-07-29 03:32:42 Train INFO: [Train]: Epoch 24 started (Total iterations: 59)
2025-07-29 03:37:41 Train INFO: [Train]: [024][00010/00058] (18.6%)  Loss=0.9791  cls_loss=0.6820  reg_loss=0.2971  lr_det=7.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1302s  iter_time=298.439s  fwd=2.067s/bwd=0.082s/opt=0.012s
2025-07-29 03:42:09 Train INFO: [Train]: [024][00020/00058] (35.6%)  Loss=0.9741  cls_loss=0.6752  reg_loss=0.2989  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1025s  iter_time=268.202s
2025-07-29 03:45:33 Train INFO: [Train]: [024][00030/00058] (52.5%)  Loss=0.9803  cls_loss=0.6783  reg_loss=0.3021  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=696s  iter_time=204.319s
2025-07-29 03:49:47 Train INFO: [Train]: [024][00040/00058] (69.5%)  Loss=0.9913  cls_loss=0.6887  reg_loss=0.3026  lr_det=7.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=450s  iter_time=253.599s
2025-07-29 03:53:01 Train INFO: [Train]: [024][00050/00058] (86.4%)  Loss=0.9840  cls_loss=0.6841  reg_loss=0.2999  lr_det=7.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=191s  iter_time=194.416s
2025-07-29 03:55:35 Train INFO: [Train]: [024][00058/00058] (100.0%)  Loss=0.9755  cls_loss=0.6778  reg_loss=0.2977  lr_det=7.1e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.484s
2025-07-29 03:55:36 Train INFO: [Train]: Epoch 24 completed in 1373.2s (avg 23.274s/iter)
2025-07-29 03:55:36 Train INFO: [Train]: Final Loss=0.9755
2025-07-29 03:55:36 Train INFO: [Train]: Epoch 25 started (Total iterations: 59)
2025-07-29 04:00:34 Train INFO: [Train]: [025][00010/00058] (18.6%)  Loss=0.9951  cls_loss=0.6896  reg_loss=0.3054  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1304s  iter_time=298.736s  fwd=2.107s/bwd=0.073s/opt=0.011s
2025-07-29 04:04:52 Train INFO: [Train]: [025][00020/00058] (35.6%)  Loss=0.9966  cls_loss=0.6908  reg_loss=0.3058  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1007s  iter_time=257.769s
2025-07-29 04:08:05 Train INFO: [Train]: [025][00030/00058] (52.5%)  Loss=1.0013  cls_loss=0.6933  reg_loss=0.3081  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=677s  iter_time=192.629s
2025-07-29 04:12:11 Train INFO: [Train]: [025][00040/00058] (69.5%)  Loss=0.9865  cls_loss=0.6812  reg_loss=0.3053  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=437s  iter_time=246.652s
2025-07-29 04:15:28 Train INFO: [Train]: [025][00050/00058] (86.4%)  Loss=0.9788  cls_loss=0.6758  reg_loss=0.3030  lr_det=6.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=196.897s
2025-07-29 04:18:08 Train INFO: [Train]: [025][00058/00058] (100.0%)  Loss=0.9771  cls_loss=0.6750  reg_loss=0.3021  lr_det=6.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=160.194s
2025-07-29 04:18:09 Train INFO: [Train]: Epoch 25 completed in 1353.6s (avg 22.943s/iter)
2025-07-29 04:18:09 Train INFO: [Train]: Final Loss=0.9771
2025-07-29 04:18:10 Train INFO: Checkpoint saved at epoch 25
2025-07-29 04:18:10 Train INFO: [Train]: Epoch 26 started (Total iterations: 59)
2025-07-29 04:22:52 Train INFO: [Train]: [026][00010/00058] (18.6%)  Loss=1.0350  cls_loss=0.7116  reg_loss=0.3235  lr_det=6.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1233s  iter_time=282.493s  fwd=2.026s/bwd=0.035s/opt=0.005s
2025-07-29 04:27:10 Train INFO: [Train]: [026][00020/00058] (35.6%)  Loss=1.0049  cls_loss=0.6936  reg_loss=0.3112  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=978s  iter_time=257.729s
2025-07-29 04:30:17 Train INFO: [Train]: [026][00030/00058] (52.5%)  Loss=0.9996  cls_loss=0.6910  reg_loss=0.3086  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=656s  iter_time=186.546s
2025-07-29 04:34:35 Train INFO: [Train]: [026][00040/00058] (69.5%)  Loss=0.9937  cls_loss=0.6883  reg_loss=0.3054  lr_det=6.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=432s  iter_time=258.333s
2025-07-29 04:37:27 Train INFO: [Train]: [026][00050/00058] (86.4%)  Loss=0.9868  cls_loss=0.6847  reg_loss=0.3021  lr_det=6.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=182s  iter_time=172.094s
2025-07-29 04:40:00 Train INFO: [Train]: [026][00058/00058] (100.0%)  Loss=0.9867  cls_loss=0.6835  reg_loss=0.3032  lr_det=6.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.048s
2025-07-29 04:40:01 Train INFO: [Train]: Epoch 26 completed in 1311.0s (avg 22.220s/iter)
2025-07-29 04:40:01 Train INFO: [Train]: Final Loss=0.9867
2025-07-29 04:40:01 Train INFO: [Train]: Epoch 27 started (Total iterations: 59)
2025-07-29 04:44:44 Train INFO: [Train]: [027][00010/00058] (18.6%)  Loss=0.9966  cls_loss=0.6892  reg_loss=0.3075  lr_det=6.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1234s  iter_time=282.863s  fwd=2.041s/bwd=0.056s/opt=0.004s
2025-07-29 04:49:02 Train INFO: [Train]: [027][00020/00058] (35.6%)  Loss=0.9908  cls_loss=0.6877  reg_loss=0.3031  lr_det=6.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=979s  iter_time=257.972s
2025-07-29 04:52:15 Train INFO: [Train]: [027][00030/00058] (52.5%)  Loss=0.9985  cls_loss=0.6931  reg_loss=0.3053  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=663s  iter_time=193.418s
2025-07-29 04:56:35 Train INFO: [Train]: [027][00040/00058] (69.5%)  Loss=1.0023  cls_loss=0.6957  reg_loss=0.3066  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=436s  iter_time=259.827s
2025-07-29 04:59:37 Train INFO: [Train]: [027][00050/00058] (86.4%)  Loss=0.9859  cls_loss=0.6826  reg_loss=0.3032  lr_det=6.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=184s  iter_time=181.802s
2025-07-29 05:02:20 Train INFO: [Train]: [027][00058/00058] (100.0%)  Loss=0.9791  cls_loss=0.6777  reg_loss=0.3014  lr_det=6.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=162.936s
2025-07-29 05:02:20 Train INFO: [Train]: Epoch 27 completed in 1339.6s (avg 22.704s/iter)
2025-07-29 05:02:20 Train INFO: [Train]: Final Loss=0.9791
2025-07-29 05:02:21 Train INFO: Checkpoint saved at epoch 27
2025-07-29 05:02:21 Train INFO: [Train]: Epoch 28 started (Total iterations: 59)
2025-07-29 05:07:07 Train INFO: [Train]: [028][00010/00058] (18.6%)  Loss=0.9995  cls_loss=0.6912  reg_loss=0.3083  lr_det=6.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1249s  iter_time=286.131s  fwd=2.057s/bwd=0.066s/opt=0.012s
2025-07-29 05:11:29 Train INFO: [Train]: [028][00020/00058] (35.6%)  Loss=1.0123  cls_loss=0.7001  reg_loss=0.3122  lr_det=6.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=992s  iter_time=262.340s
2025-07-29 05:14:40 Train INFO: [Train]: [028][00030/00058] (52.5%)  Loss=0.9987  cls_loss=0.6892  reg_loss=0.3095  lr_det=6.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=667s  iter_time=190.341s
2025-07-29 05:18:55 Train INFO: [Train]: [028][00040/00058] (69.5%)  Loss=0.9984  cls_loss=0.6890  reg_loss=0.3094  lr_det=6.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=437s  iter_time=255.490s
2025-07-29 05:22:14 Train INFO: [Train]: [028][00050/00058] (86.4%)  Loss=0.9902  cls_loss=0.6832  reg_loss=0.3069  lr_det=6.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=199.190s
2025-07-29 05:25:02 Train INFO: [Train]: [028][00058/00058] (100.0%)  Loss=0.9841  cls_loss=0.6796  reg_loss=0.3045  lr_det=6.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=167.542s
2025-07-29 05:25:03 Train INFO: [Train]: Epoch 28 completed in 1361.8s (avg 23.081s/iter)
2025-07-29 05:25:03 Train INFO: [Train]: Final Loss=0.9841
2025-07-29 05:25:03 Train INFO: [Train]: Epoch 29 started (Total iterations: 59)
2025-07-29 05:29:53 Train INFO: [Train]: [029][00010/00058] (18.6%)  Loss=0.9851  cls_loss=0.6853  reg_loss=0.2998  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1268s  iter_time=290.697s  fwd=2.066s/bwd=0.055s/opt=0.007s
2025-07-29 05:34:26 Train INFO: [Train]: [029][00020/00058] (35.6%)  Loss=0.9926  cls_loss=0.6891  reg_loss=0.3035  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1019s  iter_time=272.631s
2025-07-29 05:37:30 Train INFO: [Train]: [029][00030/00058] (52.5%)  Loss=0.9706  cls_loss=0.6731  reg_loss=0.2975  lr_det=5.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=675s  iter_time=184.112s
2025-07-29 05:41:50 Train INFO: [Train]: [029][00040/00058] (69.5%)  Loss=0.9717  cls_loss=0.6736  reg_loss=0.2981  lr_det=5.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=442s  iter_time=260.266s
2025-07-29 05:45:03 Train INFO: [Train]: [029][00050/00058] (86.4%)  Loss=0.9697  cls_loss=0.6720  reg_loss=0.2977  lr_det=5.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=188s  iter_time=192.906s
2025-07-29 05:47:37 Train INFO: [Train]: [029][00058/00058] (100.0%)  Loss=0.9636  cls_loss=0.6667  reg_loss=0.2970  lr_det=5.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.961s
2025-07-29 05:47:38 Train INFO: [Train]: Epoch 29 completed in 1355.3s (avg 22.972s/iter)
2025-07-29 05:47:38 Train INFO: [Train]: Final Loss=0.9636
2025-07-29 05:47:39 Train INFO: Checkpoint saved at epoch 29
2025-07-29 05:47:39 Train INFO: [Train]: Epoch 30 started (Total iterations: 59)
2025-07-29 05:52:33 Train INFO: [Train]: [030][00010/00058] (18.6%)  Loss=1.0376  cls_loss=0.7210  reg_loss=0.3166  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1284s  iter_time=294.352s  fwd=2.046s/bwd=0.032s/opt=0.004s
2025-07-29 05:56:45 Train INFO: [Train]: [030][00020/00058] (35.6%)  Loss=1.0262  cls_loss=0.7129  reg_loss=0.3133  lr_det=5.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=989s  iter_time=252.189s
2025-07-29 06:00:09 Train INFO: [Train]: [030][00030/00058] (52.5%)  Loss=1.0075  cls_loss=0.7013  reg_loss=0.3062  lr_det=5.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=678s  iter_time=203.649s
2025-07-29 06:04:47 Train INFO: [Train]: [030][00040/00058] (69.5%)  Loss=0.9992  cls_loss=0.6956  reg_loss=0.3036  lr_det=5.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=452s  iter_time=278.709s
2025-07-29 06:07:52 Train INFO: [Train]: [030][00050/00058] (86.4%)  Loss=0.9925  cls_loss=0.6905  reg_loss=0.3020  lr_det=5.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=190s  iter_time=184.900s
2025-07-29 06:10:42 Train INFO: [Train]: [030][00058/00058] (100.0%)  Loss=0.9888  cls_loss=0.6871  reg_loss=0.3017  lr_det=5.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=169.462s
2025-07-29 06:10:43 Train INFO: [Train]: Epoch 30 completed in 1384.0s (avg 23.458s/iter)
2025-07-29 06:10:43 Train INFO: [Train]: Final Loss=0.9888
2025-07-29 06:10:43 Train INFO: [Train]: Epoch 31 started (Total iterations: 59)
2025-07-29 06:15:28 Train INFO: [Train]: [031][00010/00058] (18.6%)  Loss=0.9857  cls_loss=0.6798  reg_loss=0.3059  lr_det=5.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1244s  iter_time=285.092s  fwd=2.044s/bwd=0.041s/opt=0.005s
2025-07-29 06:19:43 Train INFO: [Train]: [031][00020/00058] (35.6%)  Loss=0.9871  cls_loss=0.6827  reg_loss=0.3044  lr_det=5.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=977s  iter_time=254.881s
2025-07-29 06:22:39 Train INFO: [Train]: [031][00030/00058] (52.5%)  Loss=0.9919  cls_loss=0.6851  reg_loss=0.3068  lr_det=5.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=647s  iter_time=176.413s
2025-07-29 06:26:57 Train INFO: [Train]: [031][00040/00058] (69.5%)  Loss=0.9926  cls_loss=0.6852  reg_loss=0.3074  lr_det=5.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=428s  iter_time=258.363s
2025-07-29 06:30:04 Train INFO: [Train]: [031][00050/00058] (86.4%)  Loss=0.9827  cls_loss=0.6784  reg_loss=0.3043  lr_det=5.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=182s  iter_time=187.046s
2025-07-29 06:32:52 Train INFO: [Train]: [031][00058/00058] (100.0%)  Loss=0.9756  cls_loss=0.6741  reg_loss=0.3015  lr_det=5.1e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=167.317s
2025-07-29 06:32:52 Train INFO: [Train]: Epoch 31 completed in 1329.9s (avg 22.540s/iter)
2025-07-29 06:32:52 Train INFO: [Train]: Final Loss=0.9756
2025-07-29 06:32:53 Train INFO: Checkpoint saved at epoch 31
2025-07-29 06:32:53 Train INFO: [Train]: Epoch 32 started (Total iterations: 59)
2025-07-29 06:37:50 Train INFO: [Train]: [032][00010/00058] (18.6%)  Loss=1.0418  cls_loss=0.7245  reg_loss=0.3173  lr_det=5.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1295s  iter_time=296.820s  fwd=2.079s/bwd=0.065s/opt=0.006s
2025-07-29 06:42:27 Train INFO: [Train]: [032][00020/00058] (35.6%)  Loss=1.0022  cls_loss=0.6969  reg_loss=0.3054  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1038s  iter_time=277.037s
2025-07-29 06:45:36 Train INFO: [Train]: [032][00030/00058] (52.5%)  Loss=0.9865  cls_loss=0.6851  reg_loss=0.3014  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=689s  iter_time=188.725s
2025-07-29 06:49:57 Train INFO: [Train]: [032][00040/00058] (69.5%)  Loss=0.9942  cls_loss=0.6895  reg_loss=0.3047  lr_det=4.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=450s  iter_time=261.725s
2025-07-29 06:53:00 Train INFO: [Train]: [032][00050/00058] (86.4%)  Loss=0.9792  cls_loss=0.6780  reg_loss=0.3012  lr_det=4.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=182.790s
2025-07-29 06:55:21 Train INFO: [Train]: [032][00058/00058] (100.0%)  Loss=0.9780  cls_loss=0.6771  reg_loss=0.3009  lr_det=4.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=140.807s
2025-07-29 06:55:22 Train INFO: [Train]: Epoch 32 completed in 1348.6s (avg 22.858s/iter)
2025-07-29 06:55:22 Train INFO: [Train]: Final Loss=0.9780
2025-07-29 06:55:22 Train INFO: [Train]: Epoch 33 started (Total iterations: 59)
2025-07-29 07:00:11 Train INFO: [Train]: [033][00010/00058] (18.6%)  Loss=0.9980  cls_loss=0.6858  reg_loss=0.3122  lr_det=4.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1265s  iter_time=289.826s  fwd=2.060s/bwd=0.074s/opt=0.007s
2025-07-29 07:04:44 Train INFO: [Train]: [033][00020/00058] (35.6%)  Loss=0.9477  cls_loss=0.6517  reg_loss=0.2960  lr_det=4.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1018s  iter_time=272.609s
2025-07-29 07:08:09 Train INFO: [Train]: [033][00030/00058] (52.5%)  Loss=0.9731  cls_loss=0.6708  reg_loss=0.3023  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=693s  iter_time=205.178s
2025-07-29 07:12:21 Train INFO: [Train]: [033][00040/00058] (69.5%)  Loss=0.9724  cls_loss=0.6677  reg_loss=0.3047  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=251.672s
2025-07-29 07:15:30 Train INFO: [Train]: [033][00050/00058] (86.4%)  Loss=0.9786  cls_loss=0.6726  reg_loss=0.3059  lr_det=4.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=189s  iter_time=188.759s
2025-07-29 07:18:20 Train INFO: [Train]: [033][00058/00058] (100.0%)  Loss=0.9732  cls_loss=0.6685  reg_loss=0.3047  lr_det=4.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=170.449s
2025-07-29 07:18:21 Train INFO: [Train]: Epoch 33 completed in 1379.3s (avg 23.377s/iter)
2025-07-29 07:18:21 Train INFO: [Train]: Final Loss=0.9732
2025-07-29 07:18:21 Train INFO: Checkpoint saved at epoch 33
2025-07-29 07:18:21 Train INFO: [Train]: Epoch 34 started (Total iterations: 59)
2025-07-29 07:23:14 Train INFO: [Train]: [034][00010/00058] (18.6%)  Loss=1.0427  cls_loss=0.7174  reg_loss=0.3253  lr_det=4.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1277s  iter_time=292.652s  fwd=2.059s/bwd=0.063s/opt=0.006s
2025-07-29 07:27:23 Train INFO: [Train]: [034][00020/00058] (35.6%)  Loss=0.9903  cls_loss=0.6823  reg_loss=0.3079  lr_det=4.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=980s  iter_time=249.178s
2025-07-29 07:30:24 Train INFO: [Train]: [034][00030/00058] (52.5%)  Loss=0.9969  cls_loss=0.6868  reg_loss=0.3101  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=653s  iter_time=180.888s
2025-07-29 07:34:38 Train INFO: [Train]: [034][00040/00058] (69.5%)  Loss=0.9821  cls_loss=0.6773  reg_loss=0.3048  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=429s  iter_time=253.860s
2025-07-29 07:37:58 Train INFO: [Train]: [034][00050/00058] (86.4%)  Loss=0.9860  cls_loss=0.6793  reg_loss=0.3068  lr_det=4.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=185s  iter_time=199.692s
2025-07-29 07:40:32 Train INFO: [Train]: [034][00058/00058] (100.0%)  Loss=0.9855  cls_loss=0.6809  reg_loss=0.3045  lr_det=4.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=154.221s
2025-07-29 07:40:33 Train INFO: [Train]: Epoch 34 completed in 1331.2s (avg 22.563s/iter)
2025-07-29 07:40:33 Train INFO: [Train]: Final Loss=0.9855
2025-07-29 07:40:33 Train INFO: [Train]: Epoch 35 started (Total iterations: 59)
2025-07-29 07:45:29 Train INFO: [Train]: [035][00010/00058] (18.6%)  Loss=0.9902  cls_loss=0.6797  reg_loss=0.3105  lr_det=4.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1294s  iter_time=296.589s  fwd=2.064s/bwd=0.086s/opt=0.012s
2025-07-29 07:49:54 Train INFO: [Train]: [035][00020/00058] (35.6%)  Loss=0.9739  cls_loss=0.6705  reg_loss=0.3034  lr_det=4.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1016s  iter_time=264.866s
2025-07-29 07:53:19 Train INFO: [Train]: [035][00030/00058] (52.5%)  Loss=0.9713  cls_loss=0.6681  reg_loss=0.3032  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=692s  iter_time=204.652s
2025-07-29 07:57:32 Train INFO: [Train]: [035][00040/00058] (69.5%)  Loss=0.9732  cls_loss=0.6718  reg_loss=0.3014  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=447s  iter_time=252.865s
2025-07-29 08:00:43 Train INFO: [Train]: [035][00050/00058] (86.4%)  Loss=0.9682  cls_loss=0.6687  reg_loss=0.2995  lr_det=4.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=190s  iter_time=191.035s
2025-07-29 08:03:31 Train INFO: [Train]: [035][00058/00058] (100.0%)  Loss=0.9613  cls_loss=0.6643  reg_loss=0.2971  lr_det=4.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=168.279s
2025-07-29 08:03:32 Train INFO: [Train]: Epoch 35 completed in 1379.0s (avg 23.373s/iter)
2025-07-29 08:03:32 Train INFO: [Train]: Final Loss=0.9613
2025-07-29 08:03:32 Train INFO: Checkpoint saved at epoch 35
2025-07-29 08:03:32 Train INFO: [Train]: Epoch 36 started (Total iterations: 59)
2025-07-29 08:08:29 Train INFO: [Train]: [036][00010/00058] (18.6%)  Loss=1.0436  cls_loss=0.7243  reg_loss=0.3193  lr_det=4.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1295s  iter_time=296.752s  fwd=2.049s/bwd=0.068s/opt=0.011s
2025-07-29 08:12:47 Train INFO: [Train]: [036][00020/00058] (35.6%)  Loss=1.0069  cls_loss=0.6999  reg_loss=0.3070  lr_det=3.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1004s  iter_time=257.881s
2025-07-29 08:15:53 Train INFO: [Train]: [036][00030/00058] (52.5%)  Loss=0.9904  cls_loss=0.6870  reg_loss=0.3034  lr_det=3.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=669s  iter_time=186.040s
2025-07-29 08:20:06 Train INFO: [Train]: [036][00040/00058] (69.5%)  Loss=0.9959  cls_loss=0.6894  reg_loss=0.3064  lr_det=3.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=436s  iter_time=252.829s
2025-07-29 08:23:12 Train INFO: [Train]: [036][00050/00058] (86.4%)  Loss=0.9965  cls_loss=0.6891  reg_loss=0.3074  lr_det=3.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=185s  iter_time=185.690s
2025-07-29 08:25:58 Train INFO: [Train]: [036][00058/00058] (100.0%)  Loss=0.9864  cls_loss=0.6814  reg_loss=0.3050  lr_det=3.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=166.156s
2025-07-29 08:25:58 Train INFO: [Train]: Epoch 36 completed in 1346.1s (avg 22.815s/iter)
2025-07-29 08:25:58 Train INFO: [Train]: Final Loss=0.9864
2025-07-29 08:25:58 Train INFO: [Train]: Epoch 37 started (Total iterations: 59)
2025-07-29 08:30:59 Train INFO: [Train]: [037][00010/00058] (18.6%)  Loss=0.9939  cls_loss=0.6875  reg_loss=0.3063  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1312s  iter_time=300.723s  fwd=2.038s/bwd=0.079s/opt=0.007s
2025-07-29 08:35:13 Train INFO: [Train]: [037][00020/00058] (35.6%)  Loss=0.9766  cls_loss=0.6783  reg_loss=0.2983  lr_det=3.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1003s  iter_time=253.368s
2025-07-29 08:38:19 Train INFO: [Train]: [037][00030/00058] (52.5%)  Loss=0.9749  cls_loss=0.6758  reg_loss=0.2991  lr_det=3.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=669s  iter_time=186.813s
2025-07-29 08:42:27 Train INFO: [Train]: [037][00040/00058] (69.5%)  Loss=0.9739  cls_loss=0.6747  reg_loss=0.2992  lr_det=3.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=434s  iter_time=247.629s
2025-07-29 08:45:34 Train INFO: [Train]: [037][00050/00058] (86.4%)  Loss=0.9796  cls_loss=0.6775  reg_loss=0.3021  lr_det=3.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=184s  iter_time=186.917s
2025-07-29 08:48:16 Train INFO: [Train]: [037][00058/00058] (100.0%)  Loss=0.9739  cls_loss=0.6744  reg_loss=0.2995  lr_det=3.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=162.141s
2025-07-29 08:48:17 Train INFO: [Train]: Epoch 37 completed in 1338.3s (avg 22.683s/iter)
2025-07-29 08:48:17 Train INFO: [Train]: Final Loss=0.9739
2025-07-29 08:48:17 Train INFO: Checkpoint saved at epoch 37
2025-07-29 08:48:17 Train INFO: [Train]: Epoch 38 started (Total iterations: 59)
2025-07-29 08:53:22 Train INFO: [Train]: [038][00010/00058] (18.6%)  Loss=0.9809  cls_loss=0.6842  reg_loss=0.2967  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1331s  iter_time=305.026s  fwd=2.051s/bwd=0.080s/opt=0.011s
2025-07-29 08:57:46 Train INFO: [Train]: [038][00020/00058] (35.6%)  Loss=0.9707  cls_loss=0.6740  reg_loss=0.2967  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1029s  iter_time=263.648s
2025-07-29 09:00:50 Train INFO: [Train]: [038][00030/00058] (52.5%)  Loss=0.9740  cls_loss=0.6786  reg_loss=0.2955  lr_det=3.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=183.777s
2025-07-29 09:05:08 Train INFO: [Train]: [038][00040/00058] (69.5%)  Loss=0.9768  cls_loss=0.6786  reg_loss=0.2982  lr_det=3.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=444s  iter_time=257.928s
2025-07-29 09:08:13 Train INFO: [Train]: [038][00050/00058] (86.4%)  Loss=0.9751  cls_loss=0.6763  reg_loss=0.2988  lr_det=3.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=187s  iter_time=184.869s
2025-07-29 09:10:50 Train INFO: [Train]: [038][00058/00058] (100.0%)  Loss=0.9679  cls_loss=0.6712  reg_loss=0.2966  lr_det=3.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=157.601s
2025-07-29 09:10:51 Train INFO: [Train]: Epoch 38 completed in 1353.6s (avg 22.942s/iter)
2025-07-29 09:10:51 Train INFO: [Train]: Final Loss=0.9679
2025-07-29 09:10:51 Train INFO: [Train]: Epoch 39 started (Total iterations: 59)
2025-07-29 09:15:50 Train INFO: [Train]: [039][00010/00058] (18.6%)  Loss=0.9908  cls_loss=0.6871  reg_loss=0.3037  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1305s  iter_time=299.177s  fwd=2.030s/bwd=0.033s/opt=0.003s
2025-07-29 09:20:14 Train INFO: [Train]: [039][00020/00058] (35.6%)  Loss=0.9661  cls_loss=0.6708  reg_loss=0.2953  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1019s  iter_time=263.833s
2025-07-29 09:23:24 Train INFO: [Train]: [039][00030/00058] (52.5%)  Loss=0.9851  cls_loss=0.6833  reg_loss=0.3017  lr_det=3.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=680s  iter_time=190.292s
2025-07-29 09:27:58 Train INFO: [Train]: [039][00040/00058] (69.5%)  Loss=0.9773  cls_loss=0.6775  reg_loss=0.2997  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=451s  iter_time=274.232s
2025-07-29 09:31:01 Train INFO: [Train]: [039][00050/00058] (86.4%)  Loss=0.9740  cls_loss=0.6756  reg_loss=0.2984  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=190s  iter_time=182.305s
2025-07-29 09:33:34 Train INFO: [Train]: [039][00058/00058] (100.0%)  Loss=0.9758  cls_loss=0.6754  reg_loss=0.3004  lr_det=2.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=153.527s
2025-07-29 09:33:35 Train INFO: [Train]: Epoch 39 completed in 1364.1s (avg 23.121s/iter)
2025-07-29 09:33:35 Train INFO: [Train]: Final Loss=0.9758
2025-07-29 09:33:36 Train INFO: Checkpoint saved at epoch 39
2025-07-29 09:33:36 Train INFO: [Train]: Epoch 40 started (Total iterations: 59)
2025-07-29 09:38:49 Train INFO: [Train]: [040][00010/00058] (18.6%)  Loss=0.9788  cls_loss=0.6724  reg_loss=0.3064  lr_det=2.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1368s  iter_time=313.404s  fwd=2.066s/bwd=0.048s/opt=0.010s
2025-07-29 09:43:22 Train INFO: [Train]: [040][00020/00058] (35.6%)  Loss=0.9688  cls_loss=0.6726  reg_loss=0.2962  lr_det=2.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1062s  iter_time=273.341s
2025-07-29 09:46:37 Train INFO: [Train]: [040][00030/00058] (52.5%)  Loss=0.9748  cls_loss=0.6763  reg_loss=0.2985  lr_det=2.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=706s  iter_time=194.638s
2025-07-29 09:51:09 Train INFO: [Train]: [040][00040/00058] (69.5%)  Loss=0.9786  cls_loss=0.6782  reg_loss=0.3004  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=462s  iter_time=271.479s
2025-07-29 09:54:19 Train INFO: [Train]: [040][00050/00058] (86.4%)  Loss=0.9706  cls_loss=0.6717  reg_loss=0.2989  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=190.024s
2025-07-29 09:57:15 Train INFO: [Train]: [040][00058/00058] (100.0%)  Loss=0.9659  cls_loss=0.6683  reg_loss=0.2976  lr_det=2.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=176.154s
2025-07-29 09:57:16 Train INFO: [Train]: Epoch 40 completed in 1420.1s (avg 24.070s/iter)
2025-07-29 09:57:16 Train INFO: [Train]: Final Loss=0.9659
2025-07-29 09:57:16 Train INFO: [Train]: Epoch 41 started (Total iterations: 59)
2025-07-29 10:02:22 Train INFO: [Train]: [041][00010/00058] (18.6%)  Loss=1.0285  cls_loss=0.7161  reg_loss=0.3123  lr_det=2.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1337s  iter_time=306.465s  fwd=2.152s/bwd=0.082s/opt=0.009s
2025-07-29 10:06:48 Train INFO: [Train]: [041][00020/00058] (35.6%)  Loss=1.0033  cls_loss=0.6984  reg_loss=0.3049  lr_det=2.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1035s  iter_time=265.537s
2025-07-29 10:10:27 Train INFO: [Train]: [041][00030/00058] (52.5%)  Loss=1.0122  cls_loss=0.7040  reg_loss=0.3082  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=715s  iter_time=219.536s
2025-07-29 10:14:29 Train INFO: [Train]: [041][00040/00058] (69.5%)  Loss=0.9955  cls_loss=0.6909  reg_loss=0.3046  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=454s  iter_time=242.106s
2025-07-29 10:18:19 Train INFO: [Train]: [041][00050/00058] (86.4%)  Loss=0.9941  cls_loss=0.6903  reg_loss=0.3038  lr_det=2.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=198s  iter_time=230.033s
2025-07-29 10:21:12 Train INFO: [Train]: [041][00058/00058] (100.0%)  Loss=0.9937  cls_loss=0.6887  reg_loss=0.3050  lr_det=2.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=172.737s
2025-07-29 10:21:13 Train INFO: [Train]: Epoch 41 completed in 1437.5s (avg 24.364s/iter)
2025-07-29 10:21:13 Train INFO: [Train]: Final Loss=0.9937
2025-07-29 10:21:14 Train INFO: Checkpoint saved at epoch 41
2025-07-29 10:21:14 Train INFO: [Train]: Epoch 42 started (Total iterations: 59)
2025-07-29 10:26:16 Train INFO: [Train]: [042][00010/00058] (18.6%)  Loss=1.0154  cls_loss=0.7037  reg_loss=0.3117  lr_det=2.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1319s  iter_time=302.173s  fwd=2.078s/bwd=0.075s/opt=0.008s
2025-07-29 10:30:47 Train INFO: [Train]: [042][00020/00058] (35.6%)  Loss=0.9919  cls_loss=0.6868  reg_loss=0.3051  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1037s  iter_time=270.762s
2025-07-29 10:33:55 Train INFO: [Train]: [042][00030/00058] (52.5%)  Loss=0.9782  cls_loss=0.6784  reg_loss=0.2998  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=687s  iter_time=188.099s
2025-07-29 10:38:33 Train INFO: [Train]: [042][00040/00058] (69.5%)  Loss=0.9826  cls_loss=0.6801  reg_loss=0.3024  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=456s  iter_time=277.967s
2025-07-29 10:41:48 Train INFO: [Train]: [042][00050/00058] (86.4%)  Loss=0.9799  cls_loss=0.6774  reg_loss=0.3025  lr_det=2.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=194s  iter_time=195.240s
2025-07-29 10:44:42 Train INFO: [Train]: [042][00058/00058] (100.0%)  Loss=0.9635  cls_loss=0.6652  reg_loss=0.2982  lr_det=2.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=173.104s
2025-07-29 10:44:43 Train INFO: [Train]: Epoch 42 completed in 1408.5s (avg 23.873s/iter)
2025-07-29 10:44:43 Train INFO: [Train]: Final Loss=0.9635
2025-07-29 10:44:43 Train INFO: [Train]: Epoch 43 started (Total iterations: 59)
2025-07-29 10:49:57 Train INFO: [Train]: [043][00010/00058] (18.6%)  Loss=1.0505  cls_loss=0.7213  reg_loss=0.3291  lr_det=2.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1369s  iter_time=313.837s  fwd=2.157s/bwd=0.084s/opt=0.011s
2025-07-29 10:54:41 Train INFO: [Train]: [043][00020/00058] (35.6%)  Loss=1.0073  cls_loss=0.6955  reg_loss=0.3118  lr_det=2.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1083s  iter_time=284.819s
2025-07-29 10:57:55 Train INFO: [Train]: [043][00030/00058] (52.5%)  Loss=0.9976  cls_loss=0.6880  reg_loss=0.3096  lr_det=2.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=716s  iter_time=193.752s
2025-07-29 11:02:36 Train INFO: [Train]: [043][00040/00058] (69.5%)  Loss=0.9820  cls_loss=0.6771  reg_loss=0.3050  lr_det=2.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=471s  iter_time=280.473s
2025-07-29 11:06:14 Train INFO: [Train]: [043][00050/00058] (86.4%)  Loss=0.9893  cls_loss=0.6833  reg_loss=0.3060  lr_det=2.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=203s  iter_time=218.661s
2025-07-29 11:09:14 Train INFO: [Train]: [043][00058/00058] (100.0%)  Loss=0.9918  cls_loss=0.6847  reg_loss=0.3070  lr_det=2.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=179.987s
2025-07-29 11:09:15 Train INFO: [Train]: Epoch 43 completed in 1472.5s (avg 24.957s/iter)
2025-07-29 11:09:15 Train INFO: [Train]: Final Loss=0.9918
2025-07-29 11:09:16 Train INFO: Checkpoint saved at epoch 43
2025-07-29 11:09:16 Train INFO: [Train]: Epoch 44 started (Total iterations: 59)
2025-07-29 11:15:19 Train INFO: [Train]: [044][00010/00058] (18.6%)  Loss=1.0070  cls_loss=0.6990  reg_loss=0.3079  lr_det=1.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1583s  iter_time=362.845s  fwd=2.410s/bwd=0.075s/opt=0.010s
2025-07-29 11:20:41 Train INFO: [Train]: [044][00020/00058] (35.6%)  Loss=0.9696  cls_loss=0.6752  reg_loss=0.2944  lr_det=1.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1239s  iter_time=322.055s
2025-07-29 11:24:25 Train INFO: [Train]: [044][00030/00058] (52.5%)  Loss=0.9731  cls_loss=0.6785  reg_loss=0.2946  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=821s  iter_time=224.583s
2025-07-29 11:29:38 Train INFO: [Train]: [044][00040/00058] (69.5%)  Loss=0.9719  cls_loss=0.6762  reg_loss=0.2957  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=537s  iter_time=313.210s
2025-07-29 11:33:22 Train INFO: [Train]: [044][00050/00058] (86.4%)  Loss=0.9714  cls_loss=0.6737  reg_loss=0.2977  lr_det=1.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=227s  iter_time=223.550s
2025-07-29 11:36:19 Train INFO: [Train]: [044][00058/00058] (100.0%)  Loss=0.9681  cls_loss=0.6710  reg_loss=0.2972  lr_det=1.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=177.378s
2025-07-29 11:36:21 Train INFO: [Train]: Epoch 44 completed in 1624.9s (avg 27.541s/iter)
2025-07-29 11:36:21 Train INFO: [Train]: Final Loss=0.9681
2025-07-29 11:36:21 Train INFO: [Train]: Epoch 45 started (Total iterations: 59)
2025-07-29 11:42:21 Train INFO: [Train]: [045][00010/00058] (18.6%)  Loss=0.9784  cls_loss=0.6782  reg_loss=0.3003  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1571s  iter_time=360.079s  fwd=2.198s/bwd=0.061s/opt=0.011s
2025-07-29 11:47:33 Train INFO: [Train]: [045][00020/00058] (35.6%)  Loss=0.9764  cls_loss=0.6772  reg_loss=0.2992  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1216s  iter_time=311.770s
2025-07-29 11:51:09 Train INFO: [Train]: [045][00030/00058] (52.5%)  Loss=0.9820  cls_loss=0.6799  reg_loss=0.3020  lr_det=1.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=803s  iter_time=216.721s
2025-07-29 11:56:27 Train INFO: [Train]: [045][00040/00058] (69.5%)  Loss=0.9715  cls_loss=0.6735  reg_loss=0.2980  lr_det=1.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=529s  iter_time=317.324s
2025-07-29 12:00:02 Train INFO: [Train]: [045][00050/00058] (86.4%)  Loss=0.9631  cls_loss=0.6675  reg_loss=0.2956  lr_det=1.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=223s  iter_time=215.268s
2025-07-29 12:03:09 Train INFO: [Train]: [045][00058/00058] (100.0%)  Loss=0.9586  cls_loss=0.6652  reg_loss=0.2934  lr_det=1.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=186.983s
2025-07-29 12:03:10 Train INFO: [Train]: Epoch 45 completed in 1609.4s (avg 27.278s/iter)
2025-07-29 12:03:10 Train INFO: [Train]: Final Loss=0.9586
2025-07-29 12:03:11 Train INFO: Checkpoint saved at epoch 45
2025-07-29 12:03:11 Train INFO: [Train]: Epoch 46 started (Total iterations: 59)
2025-07-29 12:09:12 Train INFO: [Train]: [046][00010/00058] (18.6%)  Loss=1.0075  cls_loss=0.6954  reg_loss=0.3121  lr_det=1.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1577s  iter_time=361.314s  fwd=2.197s/bwd=0.053s/opt=0.006s
2025-07-29 12:14:34 Train INFO: [Train]: [046][00020/00058] (35.6%)  Loss=0.9977  cls_loss=0.6886  reg_loss=0.3092  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1235s  iter_time=321.376s
2025-07-29 12:18:27 Train INFO: [Train]: [046][00030/00058] (52.5%)  Loss=0.9959  cls_loss=0.6888  reg_loss=0.3071  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=828s  iter_time=233.796s
2025-07-29 12:23:48 Train INFO: [Train]: [046][00040/00058] (69.5%)  Loss=0.9779  cls_loss=0.6766  reg_loss=0.3013  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=543s  iter_time=320.257s
2025-07-29 12:27:39 Train INFO: [Train]: [046][00050/00058] (86.4%)  Loss=0.9864  cls_loss=0.6824  reg_loss=0.3040  lr_det=1.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=230s  iter_time=231.721s
2025-07-29 12:30:49 Train INFO: [Train]: [046][00058/00058] (100.0%)  Loss=0.9796  cls_loss=0.6776  reg_loss=0.3021  lr_det=1.3e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=189.270s
2025-07-29 12:30:50 Train INFO: [Train]: Epoch 46 completed in 1658.7s (avg 28.114s/iter)
2025-07-29 12:30:50 Train INFO: [Train]: Final Loss=0.9796
2025-07-29 12:30:50 Train INFO: [Train]: Epoch 47 started (Total iterations: 59)
2025-07-29 13:06:38 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-29 13:06:39 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-29 13:06:41 Train INFO: training subset: 942 videos
2025-07-29 13:06:41 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-29 13:06:41 Train INFO: Using single GPU training...
2025-07-29 13:06:41 Train INFO: Using Model EMA...
2025-07-29 13:06:41 Train INFO: Using Automatic Mixed Precision...
2025-07-29 13:06:41 Train INFO: GPU Memory: 24.0 GB
2025-07-29 13:06:41 Train INFO: Freeze the backbone...
2025-07-29 13:06:41 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_45.pth
2025-07-29 13:06:43 Train INFO: Resume epoch is 45
2025-07-29 13:06:43 Train INFO: Training Starts...

2025-07-29 13:06:43 Train INFO: [Train]: Epoch 46 started (Total iterations: 59)
2025-07-29 13:12:05 Train INFO: [Train]: [046][00010/00058] (18.6%)  Loss=1.0396  cls_loss=0.7161  reg_loss=0.3235  lr_det=1.5e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1407s  iter_time=322.364s  fwd=2.198s/bwd=0.073s/opt=0.019s
2025-07-29 13:16:53 Train INFO: [Train]: [046][00020/00058] (35.6%)  Loss=1.0201  cls_loss=0.7015  reg_loss=0.3187  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1103s  iter_time=287.457s
2025-07-29 13:20:18 Train INFO: [Train]: [046][00030/00058] (52.5%)  Loss=0.9919  cls_loss=0.6852  reg_loss=0.3067  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=736s  iter_time=205.193s
2025-07-29 13:25:06 Train INFO: [Train]: [046][00040/00058] (69.5%)  Loss=0.9784  cls_loss=0.6769  reg_loss=0.3016  lr_det=1.4e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=485s  iter_time=288.644s
2025-07-29 13:28:23 Train INFO: [Train]: [046][00050/00058] (86.4%)  Loss=0.9820  cls_loss=0.6791  reg_loss=0.3029  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=204s  iter_time=196.594s
2025-07-29 13:31:27 Train INFO: [Train]: [046][00058/00058] (100.0%)  Loss=0.9738  cls_loss=0.6738  reg_loss=0.3000  lr_det=1.3e-05  GPU=1355MB(alloc)/3790MB(reserved)/9268MB(max)  ETA=0s  iter_time=183.978s
2025-07-29 13:31:28 Train INFO: [Train]: Epoch 46 completed in 1485.4s (avg 25.176s/iter)
2025-07-29 13:31:28 Train INFO: [Train]: Final Loss=0.9738
2025-07-29 13:31:28 Train INFO: [Train]: Epoch 47 started (Total iterations: 59)
2025-07-29 13:37:13 Train INFO: [Train]: [047][00010/00058] (18.6%)  Loss=1.0254  cls_loss=0.7090  reg_loss=0.3164  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1506s  iter_time=345.229s  fwd=2.318s/bwd=0.043s/opt=0.018s
2025-07-29 13:42:20 Train INFO: [Train]: [047][00020/00058] (35.6%)  Loss=0.9991  cls_loss=0.6906  reg_loss=0.3084  lr_det=1.3e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1180s  iter_time=306.887s
2025-07-29 13:46:08 Train INFO: [Train]: [047][00030/00058] (52.5%)  Loss=0.9865  cls_loss=0.6840  reg_loss=0.3025  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=794s  iter_time=227.207s
2025-07-29 13:51:27 Train INFO: [Train]: [047][00040/00058] (69.5%)  Loss=0.9818  cls_loss=0.6813  reg_loss=0.3005  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=526s  iter_time=319.717s
2025-07-29 13:55:26 Train INFO: [Train]: [047][00050/00058] (86.4%)  Loss=0.9772  cls_loss=0.6785  reg_loss=0.2988  lr_det=1.2e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=226s  iter_time=238.529s
2025-07-29 13:58:47 Train INFO: [Train]: [047][00058/00058] (100.0%)  Loss=0.9692  cls_loss=0.6734  reg_loss=0.2958  lr_det=1.1e-05  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=201.366s
2025-07-29 13:58:48 Train INFO: [Train]: Epoch 47 completed in 1640.0s (avg 27.796s/iter)
2025-07-29 13:58:48 Train INFO: [Train]: Final Loss=0.9692
2025-07-29 13:58:49 Train INFO: Checkpoint saved at epoch 47
2025-07-29 13:58:49 Train INFO: [Train]: Epoch 48 started (Total iterations: 59)
2025-07-29 14:04:44 Train INFO: [Train]: [048][00010/00058] (18.6%)  Loss=1.0039  cls_loss=0.6916  reg_loss=0.3123  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1549s  iter_time=354.905s  fwd=2.198s/bwd=0.086s/opt=0.020s
2025-07-29 14:10:07 Train INFO: [Train]: [048][00020/00058] (35.6%)  Loss=0.9794  cls_loss=0.6797  reg_loss=0.2997  lr_det=1.1e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1228s  iter_time=323.609s
2025-07-29 14:13:54 Train INFO: [Train]: [048][00030/00058] (52.5%)  Loss=0.9837  cls_loss=0.6856  reg_loss=0.2981  lr_det=1.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=818s  iter_time=226.682s
2025-07-29 14:19:17 Train INFO: [Train]: [048][00040/00058] (69.5%)  Loss=0.9782  cls_loss=0.6797  reg_loss=0.2985  lr_det=1.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=539s  iter_time=322.958s
2025-07-29 14:23:01 Train INFO: [Train]: [048][00050/00058] (86.4%)  Loss=0.9701  cls_loss=0.6761  reg_loss=0.2940  lr_det=9.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=224.026s
2025-07-29 14:26:22 Train INFO: [Train]: [048][00058/00058] (100.0%)  Loss=0.9723  cls_loss=0.6765  reg_loss=0.2957  lr_det=9.6e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=201.012s
2025-07-29 14:26:23 Train INFO: [Train]: Epoch 48 completed in 1654.5s (avg 28.043s/iter)
2025-07-29 14:26:23 Train INFO: [Train]: Final Loss=0.9723
2025-07-29 14:26:23 Train INFO: [Train]: Epoch 49 started (Total iterations: 59)
2025-07-29 14:32:25 Train INFO: [Train]: [049][00010/00058] (18.6%)  Loss=0.9989  cls_loss=0.6858  reg_loss=0.3131  lr_det=9.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1580s  iter_time=362.054s  fwd=2.252s/bwd=0.047s/opt=0.019s
2025-07-29 14:37:49 Train INFO: [Train]: [049][00020/00058] (35.6%)  Loss=0.9825  cls_loss=0.6789  reg_loss=0.3036  lr_det=9.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1241s  iter_time=323.680s
2025-07-29 14:41:40 Train INFO: [Train]: [049][00030/00058] (52.5%)  Loss=0.9898  cls_loss=0.6846  reg_loss=0.3052  lr_det=8.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=828s  iter_time=230.425s
2025-07-29 14:46:57 Train INFO: [Train]: [049][00040/00058] (69.5%)  Loss=0.9906  cls_loss=0.6867  reg_loss=0.3040  lr_det=8.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=542s  iter_time=317.740s
2025-07-29 14:50:39 Train INFO: [Train]: [049][00050/00058] (86.4%)  Loss=0.9808  cls_loss=0.6797  reg_loss=0.3011  lr_det=8.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=221.527s
2025-07-29 14:53:58 Train INFO: [Train]: [049][00058/00058] (100.0%)  Loss=0.9721  cls_loss=0.6742  reg_loss=0.2979  lr_det=8.0e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=199.674s
2025-07-29 14:54:00 Train INFO: [Train]: Epoch 49 completed in 1656.5s (avg 28.076s/iter)
2025-07-29 14:54:00 Train INFO: [Train]: Final Loss=0.9721
2025-07-29 14:54:01 Train INFO: Checkpoint saved at epoch 49
2025-07-29 14:54:01 Train INFO: [Train]: Epoch 50 started (Total iterations: 59)
2025-07-29 14:59:55 Train INFO: [Train]: [050][00010/00058] (18.6%)  Loss=1.0277  cls_loss=0.7077  reg_loss=0.3200  lr_det=7.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1545s  iter_time=354.060s  fwd=2.246s/bwd=0.082s/opt=0.020s
2025-07-29 15:05:26 Train INFO: [Train]: [050][00020/00058] (35.6%)  Loss=1.0155  cls_loss=0.7006  reg_loss=0.3149  lr_det=7.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1240s  iter_time=331.022s
2025-07-29 15:09:09 Train INFO: [Train]: [050][00030/00058] (52.5%)  Loss=0.9854  cls_loss=0.6812  reg_loss=0.3042  lr_det=7.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=820s  iter_time=223.167s
2025-07-29 15:14:22 Train INFO: [Train]: [050][00040/00058] (69.5%)  Loss=0.9853  cls_loss=0.6827  reg_loss=0.3027  lr_det=6.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=536s  iter_time=313.017s
2025-07-29 15:18:23 Train INFO: [Train]: [050][00050/00058] (86.4%)  Loss=0.9657  cls_loss=0.6699  reg_loss=0.2958  lr_det=6.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=229s  iter_time=240.457s
2025-07-29 15:21:34 Train INFO: [Train]: [050][00058/00058] (100.0%)  Loss=0.9760  cls_loss=0.6763  reg_loss=0.2998  lr_det=6.5e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=191.741s
2025-07-29 15:21:36 Train INFO: [Train]: Epoch 50 completed in 1654.8s (avg 28.048s/iter)
2025-07-29 15:21:36 Train INFO: [Train]: Final Loss=0.9760
2025-07-29 15:21:36 Train INFO: [Train]: Epoch 51 started (Total iterations: 59)
2025-07-29 15:27:34 Train INFO: [Train]: [051][00010/00058] (18.6%)  Loss=0.9986  cls_loss=0.6930  reg_loss=0.3056  lr_det=6.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1562s  iter_time=357.988s  fwd=2.277s/bwd=0.086s/opt=0.022s
2025-07-29 15:33:09 Train INFO: [Train]: [051][00020/00058] (35.6%)  Loss=0.9762  cls_loss=0.6768  reg_loss=0.2994  lr_det=6.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1255s  iter_time=335.494s
2025-07-29 15:36:56 Train INFO: [Train]: [051][00030/00058] (52.5%)  Loss=0.9733  cls_loss=0.6758  reg_loss=0.2975  lr_det=5.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=831s  iter_time=226.981s
2025-07-29 15:42:26 Train INFO: [Train]: [051][00040/00058] (69.5%)  Loss=0.9720  cls_loss=0.6758  reg_loss=0.2962  lr_det=5.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=549s  iter_time=329.453s
2025-07-29 15:46:12 Train INFO: [Train]: [051][00050/00058] (86.4%)  Loss=0.9746  cls_loss=0.6755  reg_loss=0.2992  lr_det=5.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=232s  iter_time=226.777s
2025-07-29 15:49:42 Train INFO: [Train]: [051][00058/00058] (100.0%)  Loss=0.9638  cls_loss=0.6675  reg_loss=0.2963  lr_det=5.2e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=209.952s
2025-07-29 15:49:44 Train INFO: [Train]: Epoch 51 completed in 1687.9s (avg 28.608s/iter)
2025-07-29 15:49:44 Train INFO: [Train]: Final Loss=0.9638
2025-07-29 15:49:45 Train INFO: Checkpoint saved at epoch 51
2025-07-29 15:49:45 Train INFO: [Train]: Epoch 52 started (Total iterations: 59)
2025-07-29 15:55:47 Train INFO: [Train]: [052][00010/00058] (18.6%)  Loss=0.9946  cls_loss=0.6836  reg_loss=0.3110  lr_det=4.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1579s  iter_time=361.745s  fwd=2.246s/bwd=0.103s/opt=0.019s
2025-07-29 16:00:58 Train INFO: [Train]: [052][00020/00058] (35.6%)  Loss=0.9882  cls_loss=0.6768  reg_loss=0.3113  lr_det=4.7e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1217s  iter_time=311.050s
2025-07-29 16:04:46 Train INFO: [Train]: [052][00030/00058] (52.5%)  Loss=0.9770  cls_loss=0.6705  reg_loss=0.3065  lr_det=4.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=814s  iter_time=227.914s
2025-07-29 16:10:17 Train INFO: [Train]: [052][00040/00058] (69.5%)  Loss=0.9864  cls_loss=0.6764  reg_loss=0.3100  lr_det=4.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=541s  iter_time=331.318s
2025-07-29 16:14:00 Train INFO: [Train]: [052][00050/00058] (86.4%)  Loss=0.9787  cls_loss=0.6723  reg_loss=0.3064  lr_det=4.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=228s  iter_time=223.160s
2025-07-29 16:17:20 Train INFO: [Train]: [052][00058/00058] (100.0%)  Loss=0.9772  cls_loss=0.6725  reg_loss=0.3047  lr_det=4.0e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=200.275s
2025-07-29 16:17:22 Train INFO: [Train]: Epoch 52 completed in 1656.8s (avg 28.082s/iter)
2025-07-29 16:17:22 Train INFO: [Train]: Final Loss=0.9772
2025-07-29 16:17:22 Train INFO: [Train]: Epoch 53 started (Total iterations: 59)
2025-07-29 16:23:25 Train INFO: [Train]: [053][00010/00058] (18.6%)  Loss=0.9794  cls_loss=0.6721  reg_loss=0.3074  lr_det=3.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1585s  iter_time=363.236s  fwd=2.182s/bwd=0.075s/opt=0.018s
2025-07-29 16:28:44 Train INFO: [Train]: [053][00020/00058] (35.6%)  Loss=0.9761  cls_loss=0.6750  reg_loss=0.3011  lr_det=3.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1235s  iter_time=319.007s
2025-07-29 16:32:35 Train INFO: [Train]: [053][00030/00058] (52.5%)  Loss=0.9840  cls_loss=0.6801  reg_loss=0.3039  lr_det=3.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=825s  iter_time=230.641s
2025-07-29 16:37:48 Train INFO: [Train]: [053][00040/00058] (69.5%)  Loss=0.9819  cls_loss=0.6796  reg_loss=0.3023  lr_det=3.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=538s  iter_time=313.001s
2025-07-29 16:41:40 Train INFO: [Train]: [053][00050/00058] (86.4%)  Loss=0.9748  cls_loss=0.6756  reg_loss=0.2993  lr_det=3.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=229s  iter_time=232.480s
2025-07-29 16:45:01 Train INFO: [Train]: [053][00058/00058] (100.0%)  Loss=0.9672  cls_loss=0.6709  reg_loss=0.2963  lr_det=2.9e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=200.953s
2025-07-29 16:45:02 Train INFO: [Train]: Epoch 53 completed in 1660.6s (avg 28.145s/iter)
2025-07-29 16:45:02 Train INFO: [Train]: Final Loss=0.9672
2025-07-29 16:45:03 Train INFO: Checkpoint saved at epoch 53
2025-07-29 16:45:03 Train INFO: [Train]: Epoch 54 started (Total iterations: 59)
2025-07-29 16:50:51 Train INFO: [Train]: [054][00010/00058] (18.6%)  Loss=1.0236  cls_loss=0.7086  reg_loss=0.3150  lr_det=2.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1516s  iter_time=347.523s  fwd=2.235s/bwd=0.067s/opt=0.022s
2025-07-29 16:56:18 Train INFO: [Train]: [054][00020/00058] (35.6%)  Loss=0.9805  cls_loss=0.6771  reg_loss=0.3034  lr_det=2.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1221s  iter_time=327.091s
2025-07-29 16:59:57 Train INFO: [Train]: [054][00030/00058] (52.5%)  Loss=0.9791  cls_loss=0.6767  reg_loss=0.3024  lr_det=2.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=807s  iter_time=219.259s
2025-07-29 17:05:00 Train INFO: [Train]: [054][00040/00058] (69.5%)  Loss=0.9837  cls_loss=0.6820  reg_loss=0.3017  lr_det=2.3e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=525s  iter_time=302.779s
2025-07-29 17:08:43 Train INFO: [Train]: [054][00050/00058] (86.4%)  Loss=0.9702  cls_loss=0.6725  reg_loss=0.2976  lr_det=2.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=223s  iter_time=223.209s
2025-07-29 17:11:33 Train INFO: [Train]: [054][00058/00058] (100.0%)  Loss=0.9728  cls_loss=0.6739  reg_loss=0.2989  lr_det=2.0e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=169.738s
2025-07-29 17:11:34 Train INFO: [Train]: Epoch 54 completed in 1590.8s (avg 26.962s/iter)
2025-07-29 17:11:34 Train INFO: [Train]: Final Loss=0.9728
2025-07-29 17:11:34 Train INFO: [Train]: Epoch 55 started (Total iterations: 59)
2025-07-29 17:17:33 Train INFO: [Train]: [055][00010/00058] (18.6%)  Loss=1.0126  cls_loss=0.6985  reg_loss=0.3141  lr_det=1.9e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1567s  iter_time=359.134s  fwd=2.216s/bwd=0.068s/opt=0.021s
2025-07-29 17:22:50 Train INFO: [Train]: [055][00020/00058] (35.6%)  Loss=0.9752  cls_loss=0.6735  reg_loss=0.3017  lr_det=1.8e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1223s  iter_time=316.920s
2025-07-29 17:26:36 Train INFO: [Train]: [055][00030/00058] (52.5%)  Loss=0.9842  cls_loss=0.6795  reg_loss=0.3048  lr_det=1.6e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=815s  iter_time=225.811s
2025-07-29 17:31:51 Train INFO: [Train]: [055][00040/00058] (69.5%)  Loss=0.9707  cls_loss=0.6702  reg_loss=0.3005  lr_det=1.5e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=534s  iter_time=314.641s
2025-07-29 17:35:30 Train INFO: [Train]: [055][00050/00058] (86.4%)  Loss=0.9663  cls_loss=0.6673  reg_loss=0.2991  lr_det=1.4e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=225s  iter_time=219.580s
2025-07-29 17:38:34 Train INFO: [Train]: [055][00058/00058] (100.0%)  Loss=0.9562  cls_loss=0.6594  reg_loss=0.2968  lr_det=1.3e-06  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=184.160s
2025-07-29 17:38:35 Train INFO: [Train]: Epoch 55 completed in 1621.1s (avg 27.476s/iter)
2025-07-29 17:38:35 Train INFO: [Train]: Final Loss=0.9562
2025-07-29 17:38:36 Train INFO: Checkpoint saved at epoch 55
2025-07-29 17:38:36 Train INFO: [Train]: Epoch 56 started (Total iterations: 59)
2025-07-29 17:44:44 Train INFO: [Train]: [056][00010/00058] (18.6%)  Loss=1.0765  cls_loss=0.7435  reg_loss=0.3331  lr_det=1.2e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1607s  iter_time=368.248s  fwd=2.260s/bwd=0.104s/opt=0.020s
2025-07-29 17:50:16 Train INFO: [Train]: [056][00020/00058] (35.6%)  Loss=1.0233  cls_loss=0.7059  reg_loss=0.3174  lr_det=1.1e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1266s  iter_time=331.543s
2025-07-29 17:54:02 Train INFO: [Train]: [056][00030/00058] (52.5%)  Loss=1.0051  cls_loss=0.6937  reg_loss=0.3114  lr_det=1.0e-06  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=837s  iter_time=226.639s
2025-07-29 17:59:20 Train INFO: [Train]: [056][00040/00058] (69.5%)  Loss=1.0013  cls_loss=0.6905  reg_loss=0.3108  lr_det=9.1e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=546s  iter_time=317.446s
2025-07-29 18:03:05 Train INFO: [Train]: [056][00050/00058] (86.4%)  Loss=0.9975  cls_loss=0.6880  reg_loss=0.3094  lr_det=8.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=230s  iter_time=224.901s
2025-07-29 18:06:15 Train INFO: [Train]: [056][00058/00058] (100.0%)  Loss=0.9899  cls_loss=0.6834  reg_loss=0.3066  lr_det=7.5e-07  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=190.514s
2025-07-29 18:06:16 Train INFO: [Train]: Epoch 56 completed in 1660.2s (avg 28.138s/iter)
2025-07-29 18:06:16 Train INFO: [Train]: Final Loss=0.9899
2025-07-29 18:06:16 Train INFO: [Train]: Epoch 57 started (Total iterations: 59)
2025-07-29 18:12:08 Train INFO: [Train]: [057][00010/00058] (18.6%)  Loss=0.9584  cls_loss=0.6621  reg_loss=0.2963  lr_det=6.6e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1537s  iter_time=352.239s  fwd=2.186s/bwd=0.091s/opt=0.020s
2025-07-29 18:17:38 Train INFO: [Train]: [057][00020/00058] (35.6%)  Loss=0.9756  cls_loss=0.6770  reg_loss=0.2986  lr_det=5.9e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1233s  iter_time=329.395s
2025-07-29 18:21:38 Train INFO: [Train]: [057][00030/00058] (52.5%)  Loss=0.9833  cls_loss=0.6812  reg_loss=0.3021  lr_det=5.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=832s  iter_time=239.831s
2025-07-29 18:26:58 Train INFO: [Train]: [057][00040/00058] (69.5%)  Loss=0.9696  cls_loss=0.6720  reg_loss=0.2975  lr_det=4.5e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=545s  iter_time=320.916s
2025-07-29 18:30:40 Train INFO: [Train]: [057][00050/00058] (86.4%)  Loss=0.9755  cls_loss=0.6760  reg_loss=0.2995  lr_det=3.9e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=230s  iter_time=221.591s
2025-07-29 18:33:46 Train INFO: [Train]: [057][00058/00058] (100.0%)  Loss=0.9681  cls_loss=0.6697  reg_loss=0.2984  lr_det=3.4e-07  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=185.508s
2025-07-29 18:33:46 Train INFO: [Train]: Epoch 57 completed in 1650.3s (avg 27.971s/iter)
2025-07-29 18:33:46 Train INFO: [Train]: Final Loss=0.9681
2025-07-29 18:33:47 Train INFO: Checkpoint saved at epoch 57
2025-07-29 18:33:47 Train INFO: [Train]: Epoch 58 started (Total iterations: 59)
2025-07-29 18:40:11 Train INFO: [Train]: [058][00010/00058] (18.6%)  Loss=0.9709  cls_loss=0.6719  reg_loss=0.2991  lr_det=2.8e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1677s  iter_time=384.251s  fwd=2.254s/bwd=0.085s/opt=0.021s
2025-07-29 18:45:42 Train INFO: [Train]: [058][00020/00058] (35.6%)  Loss=0.9762  cls_loss=0.6754  reg_loss=0.3008  lr_det=2.3e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1293s  iter_time=330.564s
2025-07-29 18:49:34 Train INFO: [Train]: [058][00030/00058] (52.5%)  Loss=0.9614  cls_loss=0.6663  reg_loss=0.2952  lr_det=1.9e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=855s  iter_time=231.821s
2025-07-29 18:55:02 Train INFO: [Train]: [058][00040/00058] (69.5%)  Loss=0.9640  cls_loss=0.6679  reg_loss=0.2961  lr_det=1.5e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=560s  iter_time=328.016s
2025-07-29 18:58:45 Train INFO: [Train]: [058][00050/00058] (86.4%)  Loss=0.9674  cls_loss=0.6700  reg_loss=0.2974  lr_det=1.2e-07  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=235s  iter_time=223.633s
2025-07-29 19:02:11 Train INFO: [Train]: [058][00058/00058] (100.0%)  Loss=0.9648  cls_loss=0.6680  reg_loss=0.2968  lr_det=9.4e-08  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=205.304s
2025-07-29 19:02:12 Train INFO: [Train]: Epoch 58 completed in 1704.7s (avg 28.893s/iter)
2025-07-29 19:02:12 Train INFO: [Train]: Final Loss=0.9648
2025-07-29 19:02:12 Train INFO: [Train]: Epoch 59 started (Total iterations: 59)
2025-07-29 19:08:19 Train INFO: [Train]: [059][00010/00058] (18.6%)  Loss=1.0100  cls_loss=0.6986  reg_loss=0.3114  lr_det=6.6e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1603s  iter_time=367.337s  fwd=2.231s/bwd=0.050s/opt=0.013s
2025-07-29 19:13:52 Train INFO: [Train]: [059][00020/00058] (35.6%)  Loss=0.9824  cls_loss=0.6802  reg_loss=0.3023  lr_det=4.6e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1268s  iter_time=333.272s
2025-07-29 19:17:47 Train INFO: [Train]: [059][00030/00058] (52.5%)  Loss=0.9699  cls_loss=0.6710  reg_loss=0.2989  lr_det=3.0e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=845s  iter_time=234.796s
2025-07-29 19:23:04 Train INFO: [Train]: [059][00040/00058] (69.5%)  Loss=0.9907  cls_loss=0.6833  reg_loss=0.3074  lr_det=1.8e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=550s  iter_time=316.642s
2025-07-29 19:27:02 Train INFO: [Train]: [059][00050/00058] (86.4%)  Loss=0.9854  cls_loss=0.6805  reg_loss=0.3049  lr_det=1.2e-08  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=234s  iter_time=238.221s
2025-07-29 19:30:13 Train INFO: [Train]: [059][00058/00058] (100.0%)  Loss=0.9862  cls_loss=0.6818  reg_loss=0.3044  lr_det=1.0e-08  GPU=1355MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=0s  iter_time=191.478s
2025-07-29 19:30:14 Train INFO: [Train]: Epoch 59 completed in 1682.6s (avg 28.518s/iter)
2025-07-29 19:30:14 Train INFO: [Train]: Final Loss=0.9862
2025-07-29 19:30:15 Train INFO: Checkpoint saved at epoch 59
2025-07-29 19:30:15 Train INFO: Training Over...

2025-07-30 09:20:01 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 09:20:02 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 09:20:03 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 09:20:03 Test INFO: Using single GPU testing...
2025-07-30 09:20:03 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 09:20:05 Test INFO: Checkpoint is epoch 59.
2025-07-30 09:20:05 Test INFO: Using Model EMA...
2025-07-30 09:20:05 Test INFO: Using Automatic Mixed Precision...
2025-07-30 09:20:05 Test INFO: Testing Starts...

2025-07-30 10:35:20 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 10:35:21 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=16, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 10:35:22 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 10:35:22 Test INFO: Using single GPU testing...
2025-07-30 10:35:22 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 10:35:22 Test INFO: Checkpoint is epoch 59.
2025-07-30 10:35:22 Test INFO: Using Model EMA...
2025-07-30 10:35:22 Test INFO: Using Automatic Mixed Precision...
2025-07-30 10:35:22 Test INFO: Testing Starts...

2025-07-30 12:01:01 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 12:01:01 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=8, num_workers=4),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 12:01:02 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 12:01:03 Test INFO: Using single GPU testing...
2025-07-30 12:01:03 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 12:01:04 Test INFO: Checkpoint is epoch 59.
2025-07-30 12:01:04 Test INFO: Using Model EMA...
2025-07-30 12:01:04 Test INFO: Using Automatic Mixed Precision...
2025-07-30 12:01:04 Test INFO: Testing Starts...

2025-07-30 15:54:01 Evaluation INFO: Starting evaluation...
2025-07-30 15:54:48 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 15:54:48 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 15:54:48 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 15:54:48 Evaluation INFO: Number of predictions: 264000
2025-07-30 15:54:48 Evaluation INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7, 0.9]
2025-07-30 15:54:48 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.10 is 0.60%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.30 is 0.45%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.50 is 0.21%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 15:54:48 Evaluation INFO: mAP at tIoU 0.90 is 0.00%
2025-07-30 15:54:48 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 15:54:48 Evaluation INFO: Evaluation completed!
2025-07-30 15:57:33 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 15:57:33 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 15:57:34 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 15:57:35 Test INFO: Using single GPU testing...
2025-07-30 15:57:35 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 15:57:35 Test INFO: Checkpoint is epoch 59.
2025-07-30 15:57:35 Test INFO: Using Model EMA...
2025-07-30 15:57:35 Test INFO: Using Automatic Mixed Precision...
2025-07-30 15:57:35 Test INFO: Testing Starts...

2025-07-30 16:01:17 Test INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 16:01:18 Test INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 16:01:18 Test INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 16:01:19 Test INFO: Using single GPU testing...
2025-07-30 16:01:19 Test INFO: Loading checkpoint from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_59.pth
2025-07-30 16:01:19 Test INFO: Checkpoint is epoch 59.
2025-07-30 16:01:19 Test INFO: Using Model EMA...
2025-07-30 16:01:19 Test INFO: Using Automatic Mixed Precision...
2025-07-30 16:01:19 Test INFO: Testing Starts...

2025-07-30 16:59:02 Test INFO: Evaluation starts...
2025-07-30 16:59:57 Test INFO: Evaluating PKU-MMD dataset.
2025-07-30 16:59:57 Test INFO: Loaded annotations from test subset.
2025-07-30 16:59:57 Test INFO: Number of ground truth instances: 2704
2025-07-30 16:59:57 Test INFO: Number of predictions: 264000
2025-07-30 16:59:57 Test INFO: Fixed threshold for tiou score: [0.1, 0.3, 0.5, 0.7, 0.9]
2025-07-30 16:59:57 Test INFO: Average-mAP: 0.29 (%)
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.10 is 0.68%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.30 is 0.50%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 16:59:57 Test INFO: mAP at tIoU 0.90 is 0.00%
2025-07-30 16:59:58 Test INFO: Testing Over...

2025-07-30 16:59:58 Test INFO: PKU-MMD evaluation completed successfully!
2025-07-30 16:59:58 Test INFO: Results saved in: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\gpu1_id0/
2025-07-30 17:16:45 Evaluation INFO: Starting evaluation...
2025-07-30 17:17:26 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 17:17:26 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 17:17:26 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 17:17:26 Evaluation INFO: Number of predictions: 264000
2025-07-30 17:17:26 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-30 17:17:26 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.20 is 0.55%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.40 is 0.33%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.60 is 0.10%
2025-07-30 17:17:26 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 17:17:26 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 17:17:26 Evaluation INFO: Evaluation completed!
2025-07-30 17:27:56 Evaluation INFO: Starting evaluation...
2025-07-30 17:28:38 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 17:28:38 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 17:28:38 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 17:28:38 Evaluation INFO: Number of predictions: 264000
2025-07-30 17:28:38 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-30 17:28:38 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.20 is 0.55%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.40 is 0.33%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.60 is 0.10%
2025-07-30 17:28:38 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 17:28:38 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 17:28:38 Evaluation INFO: Evaluation completed!
2025-07-30 18:01:20 Evaluation INFO: Starting evaluation...
2025-07-30 18:01:48 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-30 18:01:48 Evaluation INFO: Loaded annotations from test subset.
2025-07-30 18:01:48 Evaluation INFO: Number of ground truth instances: 2704
2025-07-30 18:01:48 Evaluation INFO: Number of predictions: 264000
2025-07-30 18:01:48 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-30 18:01:48 Evaluation INFO: Average-mAP: 0.26 (%)
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.20 is 0.55%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.40 is 0.33%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.50 is 0.25%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.60 is 0.10%
2025-07-30 18:01:48 Evaluation INFO: mAP at tIoU 0.70 is 0.04%
2025-07-30 18:01:48 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-30 18:01:48 Evaluation INFO: Evaluation completed!
2025-07-30 18:26:52 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 18:26:53 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=768,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=12,
        patch_size=16,
        pretrained=
        'pretrained/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=768,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 18:26:54 Train INFO: training subset: 942 videos
2025-07-30 18:26:54 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 18:42:15 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-30 18:42:16 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-30 18:42:16 Train INFO: training subset: 942 videos
2025-07-30 18:42:16 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-30 18:42:17 Train INFO: Using single GPU training...
2025-07-30 18:42:17 Train INFO: Using Model EMA...
2025-07-30 18:42:17 Train INFO: Using Automatic Mixed Precision...
2025-07-30 18:42:17 Train INFO: GPU Memory: 24.0 GB
2025-07-30 18:42:17 Train INFO: Freeze the backbone...
2025-07-30 18:42:17 Train INFO: Training Starts...

2025-07-30 18:42:17 Train INFO: [Train]: Epoch 0 started (Total iterations: 59)
2025-07-30 18:47:18 Train INFO: [Train]: [000][00010/00058] (18.6%)  Loss=1.6274  cls_loss=0.8717  reg_loss=0.7558  lr_det=3.4e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1314s  iter_time=301.139s  fwd=2.078s/bwd=0.043s/opt=0.010s
2025-07-30 18:51:43 Train INFO: [Train]: [000][00020/00058] (35.6%)  Loss=1.7138  cls_loss=0.9560  reg_loss=0.7577  lr_det=6.8e-06  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1024s  iter_time=264.874s
2025-07-30 18:54:57 Train INFO: [Train]: [000][00030/00058] (52.5%)  Loss=1.6507  cls_loss=0.9818  reg_loss=0.6690  lr_det=1.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=686s  iter_time=193.641s
2025-07-30 18:59:41 Train INFO: [Train]: [000][00040/00058] (69.5%)  Loss=1.5867  cls_loss=0.9983  reg_loss=0.5884  lr_det=1.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=458s  iter_time=284.173s
2025-07-30 19:03:03 Train INFO: [Train]: [000][00050/00058] (86.4%)  Loss=1.5577  cls_loss=1.0157  reg_loss=0.5420  lr_det=1.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=202.349s
2025-07-30 19:06:01 Train INFO: [Train]: [000][00058/00058] (100.0%)  Loss=1.5217  cls_loss=1.0101  reg_loss=0.5116  lr_det=2.0e-05  GPU=1354MB(alloc)/4212MB(reserved)/9063MB(max)  ETA=0s  iter_time=178.374s
2025-07-30 19:06:02 Train INFO: [Train]: Epoch 0 completed in 1425.4s (avg 24.159s/iter)
2025-07-30 19:06:02 Train INFO: [Train]: Final Loss=1.5217
2025-07-30 19:06:02 Train INFO: [Train]: Epoch 1 started (Total iterations: 59)
2025-07-30 19:11:04 Train INFO: [Train]: [001][00010/00058] (18.6%)  Loss=1.3965  cls_loss=1.0490  reg_loss=0.3476  lr_det=2.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1316s  iter_time=301.625s  fwd=2.081s/bwd=0.087s/opt=0.006s
2025-07-30 19:15:42 Train INFO: [Train]: [001][00020/00058] (35.6%)  Loss=1.3406  cls_loss=1.0077  reg_loss=0.3329  lr_det=2.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1049s  iter_time=278.341s
2025-07-30 19:18:59 Train INFO: [Train]: [001][00030/00058] (52.5%)  Loss=1.3256  cls_loss=1.0015  reg_loss=0.3241  lr_det=3.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=701s  iter_time=196.425s
2025-07-30 19:23:41 Train INFO: [Train]: [001][00040/00058] (69.5%)  Loss=1.3104  cls_loss=0.9908  reg_loss=0.3195  lr_det=3.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=465s  iter_time=282.419s
2025-07-30 19:26:47 Train INFO: [Train]: [001][00050/00058] (86.4%)  Loss=1.3025  cls_loss=0.9857  reg_loss=0.3168  lr_det=3.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=186.372s
2025-07-30 19:29:33 Train INFO: [Train]: [001][00058/00058] (100.0%)  Loss=1.2930  cls_loss=0.9794  reg_loss=0.3136  lr_det=4.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=165.431s
2025-07-30 19:29:34 Train INFO: [Train]: Epoch 1 completed in 1411.4s (avg 23.921s/iter)
2025-07-30 19:29:34 Train INFO: [Train]: Final Loss=1.2930
2025-07-30 19:29:34 Train INFO: Checkpoint saved at epoch 1
2025-07-30 19:29:34 Train INFO: [Train]: Epoch 2 started (Total iterations: 59)
2025-07-30 19:34:57 Train INFO: [Train]: [002][00010/00058] (18.6%)  Loss=1.3112  cls_loss=0.9866  reg_loss=0.3246  lr_det=4.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1409s  iter_time=322.846s  fwd=2.129s/bwd=0.069s/opt=0.008s
2025-07-30 19:39:35 Train INFO: [Train]: [002][00020/00058] (35.6%)  Loss=1.2774  cls_loss=0.9668  reg_loss=0.3106  lr_det=4.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1088s  iter_time=278.219s
2025-07-30 19:42:40 Train INFO: [Train]: [002][00030/00058] (52.5%)  Loss=1.2866  cls_loss=0.9745  reg_loss=0.3120  lr_det=5.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=709s  iter_time=184.186s
2025-07-30 19:47:15 Train INFO: [Train]: [002][00040/00058] (69.5%)  Loss=1.2815  cls_loss=0.9692  reg_loss=0.3123  lr_det=5.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=466s  iter_time=275.762s
2025-07-30 19:50:18 Train INFO: [Train]: [002][00050/00058] (86.4%)  Loss=1.2699  cls_loss=0.9621  reg_loss=0.3078  lr_det=5.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=195s  iter_time=182.936s
2025-07-30 19:53:05 Train INFO: [Train]: [002][00058/00058] (100.0%)  Loss=1.2709  cls_loss=0.9610  reg_loss=0.3099  lr_det=6.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=166.536s
2025-07-30 19:53:06 Train INFO: [Train]: Epoch 2 completed in 1411.3s (avg 23.920s/iter)
2025-07-30 19:53:06 Train INFO: [Train]: Final Loss=1.2709
2025-07-30 19:53:06 Train INFO: [Train]: Epoch 3 started (Total iterations: 59)
2025-07-30 19:58:16 Train INFO: [Train]: [003][00010/00058] (18.6%)  Loss=1.2838  cls_loss=0.9620  reg_loss=0.3218  lr_det=6.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1353s  iter_time=310.021s  fwd=2.146s/bwd=0.087s/opt=0.009s
2025-07-30 20:02:33 Train INFO: [Train]: [003][00020/00058] (35.6%)  Loss=1.2328  cls_loss=0.9183  reg_loss=0.3144  lr_det=6.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1027s  iter_time=257.359s
2025-07-30 20:05:51 Train INFO: [Train]: [003][00030/00058] (52.5%)  Loss=1.2137  cls_loss=0.8978  reg_loss=0.3159  lr_det=7.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=692s  iter_time=198.393s
2025-07-30 20:10:29 Train INFO: [Train]: [003][00040/00058] (69.5%)  Loss=1.1924  cls_loss=0.8772  reg_loss=0.3151  lr_det=7.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=458s  iter_time=277.558s
2025-07-30 20:13:27 Train INFO: [Train]: [003][00050/00058] (86.4%)  Loss=1.1612  cls_loss=0.8491  reg_loss=0.3121  lr_det=7.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=192s  iter_time=177.765s
2025-07-30 20:16:02 Train INFO: [Train]: [003][00058/00058] (100.0%)  Loss=1.1369  cls_loss=0.8278  reg_loss=0.3090  lr_det=8.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=155.223s
2025-07-30 20:16:03 Train INFO: [Train]: Epoch 3 completed in 1377.1s (avg 23.341s/iter)
2025-07-30 20:16:03 Train INFO: [Train]: Final Loss=1.1369
2025-07-30 20:16:03 Train INFO: Checkpoint saved at epoch 3
2025-07-30 20:16:03 Train INFO: [Train]: Epoch 4 started (Total iterations: 59)
2025-07-30 20:21:16 Train INFO: [Train]: [004][00010/00058] (18.6%)  Loss=1.0912  cls_loss=0.7638  reg_loss=0.3273  lr_det=8.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1366s  iter_time=312.947s  fwd=2.084s/bwd=0.084s/opt=0.013s
2025-07-30 20:25:44 Train INFO: [Train]: [004][00020/00058] (35.6%)  Loss=1.0739  cls_loss=0.7509  reg_loss=0.3229  lr_det=8.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1051s  iter_time=268.024s
2025-07-30 20:28:48 Train INFO: [Train]: [004][00030/00058] (52.5%)  Loss=1.0445  cls_loss=0.7308  reg_loss=0.3137  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=691s  iter_time=183.816s
2025-07-30 20:33:17 Train INFO: [Train]: [004][00040/00058] (69.5%)  Loss=1.0428  cls_loss=0.7291  reg_loss=0.3137  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=454s  iter_time=268.419s
2025-07-30 20:36:35 Train INFO: [Train]: [004][00050/00058] (86.4%)  Loss=1.0228  cls_loss=0.7163  reg_loss=0.3065  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=193s  iter_time=198.578s
2025-07-30 20:39:27 Train INFO: [Train]: [004][00058/00058] (100.0%)  Loss=1.0330  cls_loss=0.7223  reg_loss=0.3107  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=171.456s
2025-07-30 20:39:27 Train INFO: [Train]: Epoch 4 completed in 1404.0s (avg 23.796s/iter)
2025-07-30 20:39:27 Train INFO: [Train]: Final Loss=1.0330
2025-07-30 20:39:27 Train INFO: [Train]: Epoch 5 started (Total iterations: 59)
2025-07-30 20:44:25 Train INFO: [Train]: [005][00010/00058] (18.6%)  Loss=1.0508  cls_loss=0.7352  reg_loss=0.3156  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1299s  iter_time=297.713s  fwd=2.079s/bwd=0.052s/opt=0.012s
2025-07-30 20:49:10 Train INFO: [Train]: [005][00020/00058] (35.6%)  Loss=1.0220  cls_loss=0.7134  reg_loss=0.3086  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1055s  iter_time=285.400s
2025-07-30 20:52:30 Train INFO: [Train]: [005][00030/00058] (52.5%)  Loss=1.0189  cls_loss=0.7129  reg_loss=0.3060  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=707s  iter_time=199.917s
2025-07-30 20:57:25 Train INFO: [Train]: [005][00040/00058] (69.5%)  Loss=1.0181  cls_loss=0.7129  reg_loss=0.3053  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=473s  iter_time=294.649s
2025-07-30 21:00:36 Train INFO: [Train]: [005][00050/00058] (86.4%)  Loss=1.0194  cls_loss=0.7117  reg_loss=0.3078  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=199s  iter_time=190.495s
2025-07-30 21:03:23 Train INFO: [Train]: [005][00058/00058] (100.0%)  Loss=1.0090  cls_loss=0.7040  reg_loss=0.3050  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=167.114s
2025-07-30 21:03:23 Train INFO: [Train]: Epoch 5 completed in 1436.1s (avg 24.340s/iter)
2025-07-30 21:03:23 Train INFO: [Train]: Final Loss=1.0090
2025-07-30 21:03:24 Train INFO: Checkpoint saved at epoch 5
2025-07-30 21:03:24 Train INFO: [Train]: Epoch 6 started (Total iterations: 59)
2025-07-30 21:08:41 Train INFO: [Train]: [006][00010/00058] (18.6%)  Loss=1.0308  cls_loss=0.7144  reg_loss=0.3164  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1385s  iter_time=317.458s  fwd=2.296s/bwd=0.068s/opt=0.009s
2025-07-30 21:13:32 Train INFO: [Train]: [006][00020/00058] (35.6%)  Loss=1.0192  cls_loss=0.7052  reg_loss=0.3140  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1100s  iter_time=290.413s
2025-07-30 21:16:53 Train INFO: [Train]: [006][00030/00058] (52.5%)  Loss=1.0070  cls_loss=0.6973  reg_loss=0.3098  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=730s  iter_time=200.746s
2025-07-30 21:21:38 Train INFO: [Train]: [006][00040/00058] (69.5%)  Loss=1.0213  cls_loss=0.7069  reg_loss=0.3145  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=480s  iter_time=285.116s
2025-07-30 21:24:54 Train INFO: [Train]: [006][00050/00058] (86.4%)  Loss=1.0124  cls_loss=0.7013  reg_loss=0.3111  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=202s  iter_time=196.328s
2025-07-30 21:27:46 Train INFO: [Train]: [006][00058/00058] (100.0%)  Loss=1.0103  cls_loss=0.7009  reg_loss=0.3094  lr_det=1.0e-04  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=171.580s
2025-07-30 21:27:47 Train INFO: [Train]: Epoch 6 completed in 1462.5s (avg 24.788s/iter)
2025-07-30 21:27:47 Train INFO: [Train]: Final Loss=1.0103
2025-07-30 21:27:47 Train INFO: [Train]: Epoch 7 started (Total iterations: 59)
2025-07-30 21:33:40 Train INFO: [Train]: [007][00010/00058] (18.6%)  Loss=1.0113  cls_loss=0.7000  reg_loss=0.3113  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1543s  iter_time=353.628s  fwd=2.172s/bwd=0.054s/opt=0.010s
2025-07-30 21:38:25 Train INFO: [Train]: [007][00020/00058] (35.6%)  Loss=1.0072  cls_loss=0.6998  reg_loss=0.3074  lr_det=1.0e-04  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1156s  iter_time=285.289s
2025-07-30 21:41:59 Train INFO: [Train]: [007][00030/00058] (52.5%)  Loss=1.0137  cls_loss=0.7029  reg_loss=0.3108  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=770s  iter_time=213.994s
2025-07-30 21:46:53 Train INFO: [Train]: [007][00040/00058] (69.5%)  Loss=1.0121  cls_loss=0.7028  reg_loss=0.3092  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=503s  iter_time=293.282s
2025-07-30 21:50:20 Train INFO: [Train]: [007][00050/00058] (86.4%)  Loss=1.0056  cls_loss=0.6992  reg_loss=0.3064  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=212s  iter_time=207.055s
2025-07-30 21:53:08 Train INFO: [Train]: [007][00058/00058] (100.0%)  Loss=0.9980  cls_loss=0.6942  reg_loss=0.3038  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=168.461s
2025-07-30 21:53:09 Train INFO: [Train]: Epoch 7 completed in 1522.5s (avg 25.805s/iter)
2025-07-30 21:53:09 Train INFO: [Train]: Final Loss=0.9980
2025-07-30 21:53:10 Train INFO: Checkpoint saved at epoch 7
2025-07-30 21:53:10 Train INFO: [Train]: Epoch 8 started (Total iterations: 59)
2025-07-30 21:59:07 Train INFO: [Train]: [008][00010/00058] (18.6%)  Loss=1.0546  cls_loss=0.7308  reg_loss=0.3238  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1562s  iter_time=357.863s  fwd=2.080s/bwd=0.067s/opt=0.006s
2025-07-30 22:04:08 Train INFO: [Train]: [008][00020/00058] (35.6%)  Loss=1.0087  cls_loss=0.6985  reg_loss=0.3101  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1191s  iter_time=300.461s
2025-07-30 22:07:45 Train INFO: [Train]: [008][00030/00058] (52.5%)  Loss=1.0087  cls_loss=0.6994  reg_loss=0.3093  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=790s  iter_time=216.744s
2025-07-30 22:13:08 Train INFO: [Train]: [008][00040/00058] (69.5%)  Loss=1.0154  cls_loss=0.7068  reg_loss=0.3086  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=526s  iter_time=323.297s
2025-07-30 22:16:42 Train INFO: [Train]: [008][00050/00058] (86.4%)  Loss=1.0001  cls_loss=0.6958  reg_loss=0.3043  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=221s  iter_time=213.595s
2025-07-30 22:19:40 Train INFO: [Train]: [008][00058/00058] (100.0%)  Loss=1.0030  cls_loss=0.6970  reg_loss=0.3060  lr_det=9.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=178.832s
2025-07-30 22:19:41 Train INFO: [Train]: Epoch 8 completed in 1591.7s (avg 26.978s/iter)
2025-07-30 22:19:41 Train INFO: [Train]: Final Loss=1.0030
2025-07-30 22:19:41 Train INFO: [Train]: Epoch 9 started (Total iterations: 59)
2025-07-30 22:25:23 Train INFO: [Train]: [009][00010/00058] (18.6%)  Loss=1.0409  cls_loss=0.7177  reg_loss=0.3231  lr_det=9.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1489s  iter_time=341.292s  fwd=2.067s/bwd=0.062s/opt=0.006s
2025-07-30 22:30:30 Train INFO: [Train]: [009][00020/00058] (35.6%)  Loss=1.0026  cls_loss=0.6933  reg_loss=0.3093  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1174s  iter_time=307.683s
2025-07-30 22:34:16 Train INFO: [Train]: [009][00030/00058] (52.5%)  Loss=1.0101  cls_loss=0.6978  reg_loss=0.3123  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=790s  iter_time=226.169s
2025-07-30 22:39:28 Train INFO: [Train]: [009][00040/00058] (69.5%)  Loss=0.9967  cls_loss=0.6890  reg_loss=0.3077  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=521s  iter_time=311.581s
2025-07-30 22:43:01 Train INFO: [Train]: [009][00050/00058] (86.4%)  Loss=0.9927  cls_loss=0.6864  reg_loss=0.3063  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=220s  iter_time=213.408s
2025-07-30 22:45:58 Train INFO: [Train]: [009][00058/00058] (100.0%)  Loss=0.9819  cls_loss=0.6783  reg_loss=0.3035  lr_det=9.8e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=176.610s
2025-07-30 22:45:59 Train INFO: [Train]: Epoch 9 completed in 1577.5s (avg 26.738s/iter)
2025-07-30 22:45:59 Train INFO: [Train]: Final Loss=0.9819
2025-07-30 22:45:59 Train INFO: Checkpoint saved at epoch 9
2025-07-30 22:45:59 Train INFO: [Train]: Epoch 10 started (Total iterations: 59)
2025-07-30 22:51:35 Train INFO: [Train]: [010][00010/00058] (18.6%)  Loss=1.1041  cls_loss=0.7629  reg_loss=0.3412  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1464s  iter_time=335.533s  fwd=2.045s/bwd=0.050s/opt=0.004s
2025-07-30 22:56:44 Train INFO: [Train]: [010][00020/00058] (35.6%)  Loss=1.0484  cls_loss=0.7256  reg_loss=0.3228  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1167s  iter_time=309.171s
2025-07-30 23:00:14 Train INFO: [Train]: [010][00030/00058] (52.5%)  Loss=1.0285  cls_loss=0.7126  reg_loss=0.3159  lr_det=9.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=772s  iter_time=210.013s
2025-07-30 23:05:27 Train INFO: [Train]: [010][00040/00058] (69.5%)  Loss=1.0246  cls_loss=0.7089  reg_loss=0.3157  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=513s  iter_time=313.092s
2025-07-30 23:08:56 Train INFO: [Train]: [010][00050/00058] (86.4%)  Loss=1.0228  cls_loss=0.7081  reg_loss=0.3147  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=216s  iter_time=208.671s
2025-07-30 23:12:13 Train INFO: [Train]: [010][00058/00058] (100.0%)  Loss=1.0151  cls_loss=0.7029  reg_loss=0.3122  lr_det=9.7e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=197.413s
2025-07-30 23:12:14 Train INFO: [Train]: Epoch 10 completed in 1574.7s (avg 26.689s/iter)
2025-07-30 23:12:14 Train INFO: [Train]: Final Loss=1.0151
2025-07-30 23:12:14 Train INFO: [Train]: Epoch 11 started (Total iterations: 59)
2025-07-30 23:18:03 Train INFO: [Train]: [011][00010/00058] (18.6%)  Loss=0.9883  cls_loss=0.6844  reg_loss=0.3039  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1524s  iter_time=349.305s  fwd=2.085s/bwd=0.074s/opt=0.010s
2025-07-30 23:23:09 Train INFO: [Train]: [011][00020/00058] (35.6%)  Loss=1.0040  cls_loss=0.6973  reg_loss=0.3067  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1184s  iter_time=305.221s
2025-07-30 23:26:37 Train INFO: [Train]: [011][00030/00058] (52.5%)  Loss=1.0114  cls_loss=0.7008  reg_loss=0.3107  lr_det=9.7e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=779s  iter_time=208.426s
2025-07-30 23:31:50 Train INFO: [Train]: [011][00040/00058] (69.5%)  Loss=0.9945  cls_loss=0.6890  reg_loss=0.3054  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=516s  iter_time=312.836s
2025-07-30 23:35:35 Train INFO: [Train]: [011][00050/00058] (86.4%)  Loss=0.9993  cls_loss=0.6916  reg_loss=0.3077  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=220s  iter_time=225.248s
2025-07-30 23:38:31 Train INFO: [Train]: [011][00058/00058] (100.0%)  Loss=0.9911  cls_loss=0.6852  reg_loss=0.3059  lr_det=9.6e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=175.734s
2025-07-30 23:38:32 Train INFO: [Train]: Epoch 11 completed in 1577.9s (avg 26.744s/iter)
2025-07-30 23:38:32 Train INFO: [Train]: Final Loss=0.9911
2025-07-30 23:38:33 Train INFO: Checkpoint saved at epoch 11
2025-07-30 23:38:33 Train INFO: [Train]: Epoch 12 started (Total iterations: 59)
2025-07-30 23:43:57 Train INFO: [Train]: [012][00010/00058] (18.6%)  Loss=0.9987  cls_loss=0.6905  reg_loss=0.3082  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1416s  iter_time=324.539s  fwd=2.063s/bwd=0.084s/opt=0.006s
2025-07-30 23:49:04 Train INFO: [Train]: [012][00020/00058] (35.6%)  Loss=1.0048  cls_loss=0.6947  reg_loss=0.3102  lr_det=9.6e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1141s  iter_time=306.277s
2025-07-30 23:52:32 Train INFO: [Train]: [012][00030/00058] (52.5%)  Loss=0.9872  cls_loss=0.6831  reg_loss=0.3041  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=758s  iter_time=208.699s
2025-07-30 23:57:51 Train INFO: [Train]: [012][00040/00058] (69.5%)  Loss=0.9887  cls_loss=0.6849  reg_loss=0.3039  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=508s  iter_time=318.238s
2025-07-31 00:01:23 Train INFO: [Train]: [012][00050/00058] (86.4%)  Loss=0.9912  cls_loss=0.6863  reg_loss=0.3049  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=215s  iter_time=211.959s
2025-07-31 00:04:26 Train INFO: [Train]: [012][00058/00058] (100.0%)  Loss=0.9875  cls_loss=0.6836  reg_loss=0.3040  lr_det=9.5e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=183.141s
2025-07-31 00:04:27 Train INFO: [Train]: Epoch 12 completed in 1553.9s (avg 26.337s/iter)
2025-07-31 00:04:27 Train INFO: [Train]: Final Loss=0.9875
2025-07-31 00:04:27 Train INFO: [Train]: Epoch 13 started (Total iterations: 59)
2025-07-31 00:10:27 Train INFO: [Train]: [013][00010/00058] (18.6%)  Loss=1.0352  cls_loss=0.7142  reg_loss=0.3209  lr_det=9.5e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1571s  iter_time=360.025s  fwd=2.164s/bwd=0.105s/opt=0.013s
2025-07-31 00:15:44 Train INFO: [Train]: [013][00020/00058] (35.6%)  Loss=1.0050  cls_loss=0.6944  reg_loss=0.3106  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1226s  iter_time=317.584s
2025-07-31 00:19:20 Train INFO: [Train]: [013][00030/00058] (52.5%)  Loss=0.9925  cls_loss=0.6870  reg_loss=0.3054  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=807s  iter_time=216.167s
2025-07-31 00:24:25 Train INFO: [Train]: [013][00040/00058] (69.5%)  Loss=1.0150  cls_loss=0.7012  reg_loss=0.3138  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=526s  iter_time=304.126s
2025-07-31 00:27:50 Train INFO: [Train]: [013][00050/00058] (86.4%)  Loss=1.0086  cls_loss=0.6977  reg_loss=0.3110  lr_det=9.4e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=220s  iter_time=204.921s
2025-07-31 00:30:49 Train INFO: [Train]: [013][00058/00058] (100.0%)  Loss=1.0087  cls_loss=0.6984  reg_loss=0.3104  lr_det=9.4e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=179.475s
2025-07-31 00:30:50 Train INFO: [Train]: Epoch 13 completed in 1583.1s (avg 26.832s/iter)
2025-07-31 00:30:50 Train INFO: [Train]: Final Loss=1.0087
2025-07-31 00:30:50 Train INFO: Checkpoint saved at epoch 13
2025-07-31 00:30:50 Train INFO: [Train]: Epoch 14 started (Total iterations: 59)
2025-07-31 00:36:36 Train INFO: [Train]: [014][00010/00058] (18.6%)  Loss=0.9697  cls_loss=0.6771  reg_loss=0.2926  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1510s  iter_time=345.995s  fwd=2.070s/bwd=0.058s/opt=0.007s
2025-07-31 00:41:45 Train INFO: [Train]: [014][00020/00058] (35.6%)  Loss=0.9594  cls_loss=0.6696  reg_loss=0.2898  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1185s  iter_time=308.800s
2025-07-31 00:45:21 Train INFO: [Train]: [014][00030/00058] (52.5%)  Loss=0.9862  cls_loss=0.6863  reg_loss=0.2999  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=786s  iter_time=215.691s
2025-07-31 00:50:13 Train INFO: [Train]: [014][00040/00058] (69.5%)  Loss=0.9845  cls_loss=0.6858  reg_loss=0.2987  lr_det=9.3e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=510s  iter_time=291.765s
2025-07-31 00:54:10 Train INFO: [Train]: [014][00050/00058] (86.4%)  Loss=0.9832  cls_loss=0.6846  reg_loss=0.2986  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=219s  iter_time=237.021s
2025-07-31 00:57:05 Train INFO: [Train]: [014][00058/00058] (100.0%)  Loss=0.9822  cls_loss=0.6835  reg_loss=0.2987  lr_det=9.2e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=175.444s
2025-07-31 00:57:06 Train INFO: [Train]: Epoch 14 completed in 1575.5s (avg 26.704s/iter)
2025-07-31 00:57:06 Train INFO: [Train]: Final Loss=0.9822
2025-07-31 00:57:06 Train INFO: [Train]: Epoch 15 started (Total iterations: 59)
2025-07-31 01:02:48 Train INFO: [Train]: [015][00010/00058] (18.6%)  Loss=1.0217  cls_loss=0.7070  reg_loss=0.3146  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1492s  iter_time=341.897s  fwd=2.078s/bwd=0.097s/opt=0.013s
2025-07-31 01:07:42 Train INFO: [Train]: [015][00020/00058] (35.6%)  Loss=1.0287  cls_loss=0.7106  reg_loss=0.3182  lr_det=9.2e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1150s  iter_time=293.866s
2025-07-31 01:11:11 Train INFO: [Train]: [015][00030/00058] (52.5%)  Loss=1.0192  cls_loss=0.7057  reg_loss=0.3135  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=763s  iter_time=208.924s
2025-07-31 01:16:30 Train INFO: [Train]: [015][00040/00058] (69.5%)  Loss=1.0227  cls_loss=0.7080  reg_loss=0.3147  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=511s  iter_time=318.861s
2025-07-31 01:19:57 Train INFO: [Train]: [015][00050/00058] (86.4%)  Loss=1.0062  cls_loss=0.6962  reg_loss=0.3100  lr_det=9.1e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=215s  iter_time=207.773s
2025-07-31 01:22:58 Train INFO: [Train]: [015][00058/00058] (100.0%)  Loss=0.9931  cls_loss=0.6869  reg_loss=0.3063  lr_det=9.0e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=180.666s
2025-07-31 01:22:59 Train INFO: [Train]: Epoch 15 completed in 1552.8s (avg 26.318s/iter)
2025-07-31 01:22:59 Train INFO: [Train]: Final Loss=0.9931
2025-07-31 01:22:59 Train INFO: Checkpoint saved at epoch 15
2025-07-31 01:22:59 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-31 01:28:44 Train INFO: [Train]: [016][00010/00058] (18.6%)  Loss=1.0781  cls_loss=0.7413  reg_loss=0.3368  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1503s  iter_time=344.461s  fwd=2.035s/bwd=0.056s/opt=0.004s
2025-07-31 01:33:50 Train INFO: [Train]: [016][00020/00058] (35.6%)  Loss=1.0178  cls_loss=0.7007  reg_loss=0.3171  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1177s  iter_time=305.935s
2025-07-31 01:37:26 Train INFO: [Train]: [016][00030/00058] (52.5%)  Loss=1.0023  cls_loss=0.6928  reg_loss=0.3095  lr_det=9.0e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=783s  iter_time=216.500s
2025-07-31 01:42:29 Train INFO: [Train]: [016][00040/00058] (69.5%)  Loss=0.9999  cls_loss=0.6906  reg_loss=0.3093  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=514s  iter_time=302.774s
2025-07-31 01:46:10 Train INFO: [Train]: [016][00050/00058] (86.4%)  Loss=1.0085  cls_loss=0.6963  reg_loss=0.3123  lr_det=8.9e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=218s  iter_time=220.561s
2025-07-31 01:49:17 Train INFO: [Train]: [016][00058/00058] (100.0%)  Loss=1.0009  cls_loss=0.6916  reg_loss=0.3093  lr_det=8.9e-05  GPU=1354MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=0s  iter_time=187.664s
2025-07-31 01:49:18 Train INFO: [Train]: Epoch 16 completed in 1578.7s (avg 26.757s/iter)
2025-07-31 01:49:18 Train INFO: [Train]: Final Loss=1.0009
2025-07-31 01:49:18 Train INFO: [Train]: Epoch 17 started (Total iterations: 59)
2025-07-31 01:55:09 Train INFO: [Train]: [017][00010/00058] (18.6%)  Loss=1.0386  cls_loss=0.7179  reg_loss=0.3207  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1530s  iter_time=350.549s  fwd=2.041s/bwd=0.054s/opt=0.008s
2025-07-31 02:00:18 Train INFO: [Train]: [017][00020/00058] (35.6%)  Loss=0.9770  cls_loss=0.6780  reg_loss=0.2990  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=1194s  iter_time=309.457s
2025-07-31 02:04:13 Train INFO: [Train]: [017][00030/00058] (52.5%)  Loss=0.9774  cls_loss=0.6780  reg_loss=0.2994  lr_det=8.8e-05  GPU=1429MB(alloc)/11446MB(reserved)/9063MB(max)  ETA=808s  iter_time=234.760s
2025-07-31 09:18:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:18:12 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:18:13 Train INFO: training subset: 942 videos
2025-07-31 09:18:13 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:18:13 Train INFO: Using single GPU training...
2025-07-31 09:18:13 Train INFO: Using Model EMA...
2025-07-31 09:18:14 Train INFO: Using Automatic Mixed Precision...
2025-07-31 09:18:14 Train INFO: GPU Memory: 24.0 GB
2025-07-31 09:18:14 Train INFO: Freeze the backbone...
2025-07-31 09:18:14 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_15.pth
2025-07-31 09:18:15 Train INFO: Resume epoch is 15
2025-07-31 09:18:15 Train INFO: Training Starts...

2025-07-31 09:18:15 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-31 09:27:35 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:27:35 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(pipeline=[
        dict(format='avi', type='PrepareVideoInfo'),
        dict(num_threads=4, type='mmaction.DecordInit'),
        dict(
            method='sliding_window',
            num_clips=1,
            scale_factor=1,
            type='LoadFrames'),
        dict(type='mmaction.DecordDecode'),
        dict(scale=(
            -1,
            160,
        ), type='mmaction.Resize'),
        dict(crop_size=160, type='mmaction.CenterCrop'),
        dict(input_format='NCTHW', type='mmaction.FormatShape'),
        dict(keys=[
            'imgs',
        ], type='ConvertToTensor'),
        dict(inputs='imgs', keys=[
            'masks',
        ], type='Collect'),
    ]),
    train=dict(pipeline=[
        dict(format='avi', type='PrepareVideoInfo'),
        dict(num_threads=6, type='mmaction.DecordInit'),
        dict(
            crop_ratio=[
                0.9,
                1.0,
            ],
            method='random_trunc',
            num_clips=1,
            scale_factor=1,
            trunc_len=512,
            trunc_thresh=0.5,
            type='LoadFrames'),
        dict(type='mmaction.DecordDecode'),
        dict(scale=(
            -1,
            182,
        ), type='mmaction.Resize'),
        dict(type='mmaction.RandomResizedCrop'),
        dict(keep_ratio=False, scale=(
            160,
            160,
        ), type='mmaction.Resize'),
        dict(flip_ratio=0.5, type='mmaction.Flip'),
        dict(transforms='default', type='mmaction.ImgAug'),
        dict(type='mmaction.ColorJitter'),
        dict(input_format='NCTHW', type='mmaction.FormatShape'),
        dict(
            keys=[
                'imgs',
                'gt_segments',
                'gt_labels',
            ],
            type='ConvertToTensor'),
        dict(
            inputs='imgs',
            keys=[
                'masks',
                'gt_segments',
                'gt_labels',
            ],
            type='Collect'),
    ]))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51))
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:28:20 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:28:20 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        type='PkuSlidingDataset'),
    train=dict(
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51))
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:30:11 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:30:11 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51))
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:30:12 Train INFO: training subset: 942 videos
2025-07-31 09:30:12 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:30:49 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:30:49 Train INFO: Config: 
base_ = [
    '../../_base_/datasets/pku-mmd/pku-mmd.py',
    '../../_base_/models/actionformer.py',
]
chunk_num = 32
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=None),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    projection=dict(
        attn_cfg=dict(n_mha_win_size=-1), in_channels=384, max_seq_len=512),
    rpn_head=dict(num_classes=51),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:30:50 Train INFO: training subset: 942 videos
2025-07-31 09:30:50 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:32:05 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:32:05 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=6),
    train=dict(batch_size=16, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:32:06 Train INFO: training subset: 942 videos
2025-07-31 09:32:06 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:32:06 Train INFO: Using single GPU training...
2025-07-31 09:32:06 Train INFO: Using Model EMA...
2025-07-31 09:32:06 Train INFO: Using Automatic Mixed Precision...
2025-07-31 09:32:06 Train INFO: GPU Memory: 24.0 GB
2025-07-31 09:32:06 Train INFO: Freeze the backbone...
2025-07-31 09:32:06 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_15.pth
2025-07-31 09:32:07 Train INFO: Resume epoch is 15
2025-07-31 09:32:07 Train INFO: Training Starts...

2025-07-31 09:32:07 Train INFO: [Train]: Epoch 16 started (Total iterations: 59)
2025-07-31 09:37:50 Train INFO: [Train]: [016][00010/00058] (18.6%)  Loss=1.0778  cls_loss=0.7410  reg_loss=0.3368  lr_det=9.0e-05  GPU=1428MB(alloc)/11026MB(reserved)/9268MB(max)  ETA=1497s  iter_time=343.147s  fwd=2.065s/bwd=0.056s/opt=0.014s
2025-07-31 09:41:09 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 09:41:09 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/annotations_test.json'
annotation_train = 'data/PKU-MMD/annotations_train.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/annotations_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/annotations_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=1,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/annotations_test.json',
    subset='test',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(load_from_raw_predictions=False, save_raw_prediction=False)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    4,
                ),
                (
                    4,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    10000,
                ),
            ],
            strides=[
                1,
                2,
                4,
                8,
                16,
                32,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=2000,
        multiclass=True,
        sigma=0.7,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=60, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=2, num_workers=2),
    train=dict(batch_size=8, num_workers=2))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=60,
    logging_interval=10,
    val_eval_interval=1000,
    val_loss_interval=-1,
    val_start_epoch=100)

2025-07-31 09:41:10 Train INFO: training subset: 942 videos
2025-07-31 09:41:10 Train INFO: testing subset: 132 videos, truncated as 2370 windows.
2025-07-31 09:41:11 Train INFO: Using single GPU training...
2025-07-31 09:41:11 Train INFO: Using Model EMA...
2025-07-31 09:41:11 Train INFO: Using Automatic Mixed Precision...
2025-07-31 09:41:11 Train INFO: GPU Memory: 24.0 GB
2025-07-31 09:41:11 Train INFO: Freeze the backbone...
2025-07-31 09:41:11 Train INFO: Resume training from: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0/checkpoint/epoch_15.pth
2025-07-31 09:41:13 Train INFO: Resume epoch is 15
2025-07-31 09:41:13 Train INFO: Training Starts...

2025-07-31 09:41:13 Train INFO: [Train]: Epoch 16 started (Total iterations: 118)
2025-07-31 09:45:06 Train INFO: [Train]: [016][00010/00117] (9.3%)  Loss=0.6885  cls_loss=0.4831  reg_loss=0.2054  lr_det=9.0e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2267s  iter_time=233.038s  fwd=1.204s/bwd=0.078s/opt=0.021s
2025-07-31 09:48:25 Train INFO: [Train]: [016][00020/00117] (17.8%)  Loss=0.7528  cls_loss=0.5217  reg_loss=0.2311  lr_det=9.0e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1998s  iter_time=199.558s
2025-07-31 09:51:47 Train INFO: [Train]: [016][00030/00117] (26.3%)  Loss=0.8102  cls_loss=0.5634  reg_loss=0.2468  lr_det=9.0e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1781s  iter_time=202.132s
2025-07-31 09:55:03 Train INFO: [Train]: [016][00040/00117] (34.7%)  Loss=0.8565  cls_loss=0.5973  reg_loss=0.2592  lr_det=8.9e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1560s  iter_time=195.912s
2025-07-31 09:58:26 Train INFO: [Train]: [016][00050/00117] (43.2%)  Loss=0.8888  cls_loss=0.6191  reg_loss=0.2698  lr_det=8.9e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1357s  iter_time=202.519s
2025-07-31 10:01:15 Train INFO: [Train]: [016][00060/00117] (51.7%)  Loss=0.9127  cls_loss=0.6334  reg_loss=0.2793  lr_det=8.9e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1124s  iter_time=169.682s
2025-07-31 10:04:04 Train INFO: [Train]: [016][00070/00117] (60.2%)  Loss=0.9248  cls_loss=0.6425  reg_loss=0.2823  lr_det=8.8e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=908s  iter_time=168.292s
2025-07-31 10:06:44 Train INFO: [Train]: [016][00080/00117] (68.6%)  Loss=0.9364  cls_loss=0.6494  reg_loss=0.2870  lr_det=8.8e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=699s  iter_time=160.126s
2025-07-31 10:09:29 Train INFO: [Train]: [016][00090/00117] (77.1%)  Loss=0.9431  cls_loss=0.6542  reg_loss=0.2890  lr_det=8.8e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=503s  iter_time=165.469s
2025-07-31 10:12:18 Train INFO: [Train]: [016][00100/00117] (85.6%)  Loss=0.9410  cls_loss=0.6523  reg_loss=0.2887  lr_det=8.7e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=314s  iter_time=168.504s
2025-07-31 10:15:01 Train INFO: [Train]: [016][00110/00117] (94.1%)  Loss=0.9487  cls_loss=0.6561  reg_loss=0.2926  lr_det=8.7e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=128s  iter_time=163.098s
2025-07-31 10:16:33 Train INFO: [Train]: [016][00117/00117] (100.0%)  Loss=0.9511  cls_loss=0.6581  reg_loss=0.2930  lr_det=8.7e-05  GPU=1054MB(alloc)/2286MB(reserved)/4968MB(max)  ETA=0s  iter_time=91.872s
2025-07-31 10:16:34 Train INFO: [Train]: Epoch 16 completed in 2121.2s (avg 17.976s/iter)
2025-07-31 10:16:34 Train INFO: [Train]: Final Loss=0.9511
2025-07-31 10:16:34 Train INFO: [Train]: Epoch 17 started (Total iterations: 118)
2025-07-31 10:20:07 Train INFO: [Train]: [017][00010/00117] (9.3%)  Loss=1.0008  cls_loss=0.6920  reg_loss=0.3088  lr_det=8.7e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2073s  iter_time=213.067s  fwd=1.103s/bwd=0.069s/opt=0.011s
2025-07-31 10:22:53 Train INFO: [Train]: [017][00020/00117] (17.8%)  Loss=1.0102  cls_loss=0.6956  reg_loss=0.3146  lr_det=8.6e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1753s  iter_time=166.479s
2025-07-31 10:25:41 Train INFO: [Train]: [017][00030/00117] (26.3%)  Loss=1.0007  cls_loss=0.6927  reg_loss=0.3080  lr_det=8.6e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1536s  iter_time=167.625s
2025-07-31 10:28:23 Train INFO: [Train]: [017][00040/00117] (34.7%)  Loss=0.9989  cls_loss=0.6938  reg_loss=0.3052  lr_det=8.6e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1333s  iter_time=162.528s
2025-07-31 10:31:00 Train INFO: [Train]: [017][00050/00117] (43.2%)  Loss=0.9955  cls_loss=0.6922  reg_loss=0.3034  lr_det=8.5e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=1138s  iter_time=156.884s
2025-07-31 10:34:05 Train INFO: [Train]: [017][00060/00117] (51.7%)  Loss=1.0043  cls_loss=0.6983  reg_loss=0.3060  lr_det=8.5e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=982s  iter_time=184.818s
2025-07-31 10:36:51 Train INFO: [Train]: [017][00070/00117] (60.2%)  Loss=0.9949  cls_loss=0.6914  reg_loss=0.3035  lr_det=8.4e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=806s  iter_time=165.535s
2025-07-31 10:39:40 Train INFO: [Train]: [017][00080/00117] (68.6%)  Loss=0.9985  cls_loss=0.6939  reg_loss=0.3046  lr_det=8.4e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=633s  iter_time=169.217s
2025-07-31 10:42:31 Train INFO: [Train]: [017][00090/00117] (77.1%)  Loss=1.0033  cls_loss=0.6964  reg_loss=0.3069  lr_det=8.4e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=462s  iter_time=170.817s
2025-07-31 10:45:23 Train INFO: [Train]: [017][00100/00117] (85.6%)  Loss=0.9966  cls_loss=0.6911  reg_loss=0.3055  lr_det=8.3e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=291s  iter_time=172.695s
2025-07-31 10:48:35 Train INFO: [Train]: [017][00110/00117] (94.1%)  Loss=0.9908  cls_loss=0.6869  reg_loss=0.3039  lr_det=8.3e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=121s  iter_time=191.813s
2025-07-31 10:50:28 Train INFO: [Train]: [017][00117/00117] (100.0%)  Loss=0.9843  cls_loss=0.6828  reg_loss=0.3016  lr_det=8.3e-05  GPU=1054MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=0s  iter_time=112.520s
2025-07-31 10:50:29 Train INFO: [Train]: Epoch 17 completed in 2035.4s (avg 17.249s/iter)
2025-07-31 10:50:29 Train INFO: [Train]: Final Loss=0.9843
2025-07-31 10:50:30 Train INFO: Checkpoint saved at epoch 17
2025-07-31 10:50:30 Train INFO: [Train]: Epoch 18 started (Total iterations: 118)
2025-07-31 10:54:37 Train INFO: [Train]: [018][00010/00117] (9.3%)  Loss=1.0581  cls_loss=0.7261  reg_loss=0.3320  lr_det=8.2e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2399s  iter_time=246.588s  fwd=1.234s/bwd=0.079s/opt=0.019s
2025-07-31 10:57:51 Train INFO: [Train]: [018][00020/00117] (17.8%)  Loss=1.0485  cls_loss=0.7233  reg_loss=0.3252  lr_det=8.2e-05  GPU=1132MB(alloc)/5940MB(reserved)/4968MB(max)  ETA=2035s  iter_time=193.963s
2025-07-31 11:00:49 Train INFO: [Train]: [018][00030/00117] (26.3%)  Loss=1.0493  cls_loss=0.7236  reg_loss=0.3257  lr_det=8.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1737s  iter_time=178.266s
2025-07-31 11:03:42 Train INFO: [Train]: [018][00040/00117] (34.7%)  Loss=1.0248  cls_loss=0.7068  reg_loss=0.3180  lr_det=8.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1486s  iter_time=172.518s
2025-07-31 11:06:33 Train INFO: [Train]: [018][00050/00117] (43.2%)  Loss=1.0186  cls_loss=0.7038  reg_loss=0.3147  lr_det=8.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1265s  iter_time=171.379s
2025-07-31 11:09:17 Train INFO: [Train]: [018][00060/00117] (51.7%)  Loss=1.0142  cls_loss=0.7010  reg_loss=0.3132  lr_det=8.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1053s  iter_time=164.453s
2025-07-31 11:12:01 Train INFO: [Train]: [018][00070/00117] (60.2%)  Loss=1.0100  cls_loss=0.6985  reg_loss=0.3115  lr_det=8.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=855s  iter_time=163.822s
2025-07-31 11:14:51 Train INFO: [Train]: [018][00080/00117] (68.6%)  Loss=1.0083  cls_loss=0.6959  reg_loss=0.3124  lr_det=8.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=667s  iter_time=169.300s
2025-07-31 11:17:42 Train INFO: [Train]: [018][00090/00117] (77.1%)  Loss=1.0059  cls_loss=0.6937  reg_loss=0.3122  lr_det=7.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=484s  iter_time=171.570s
2025-07-31 11:20:28 Train INFO: [Train]: [018][00100/00117] (85.6%)  Loss=0.9998  cls_loss=0.6905  reg_loss=0.3094  lr_det=7.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=303s  iter_time=165.339s
2025-07-31 11:23:12 Train INFO: [Train]: [018][00110/00117] (94.1%)  Loss=1.0024  cls_loss=0.6928  reg_loss=0.3096  lr_det=7.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=124s  iter_time=164.093s
2025-07-31 11:24:51 Train INFO: [Train]: [018][00117/00117] (100.0%)  Loss=0.9931  cls_loss=0.6863  reg_loss=0.3068  lr_det=7.8e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=99.161s
2025-07-31 11:24:52 Train INFO: [Train]: Epoch 18 completed in 2061.4s (avg 17.470s/iter)
2025-07-31 11:24:52 Train INFO: [Train]: Final Loss=0.9931
2025-07-31 11:24:52 Train INFO: [Train]: Epoch 19 started (Total iterations: 118)
2025-07-31 11:28:23 Train INFO: [Train]: [019][00010/00117] (9.3%)  Loss=1.0938  cls_loss=0.7549  reg_loss=0.3389  lr_det=7.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2060s  iter_time=211.770s  fwd=1.137s/bwd=0.040s/opt=0.017s
2025-07-31 11:31:07 Train INFO: [Train]: [019][00020/00117] (17.8%)  Loss=1.0176  cls_loss=0.7055  reg_loss=0.3122  lr_det=7.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1735s  iter_time=163.779s
2025-07-31 11:33:54 Train INFO: [Train]: [019][00030/00117] (26.3%)  Loss=1.0328  cls_loss=0.7129  reg_loss=0.3198  lr_det=7.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1521s  iter_time=166.370s
2025-07-31 11:36:34 Train INFO: [Train]: [019][00040/00117] (34.7%)  Loss=1.0296  cls_loss=0.7101  reg_loss=0.3194  lr_det=7.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1318s  iter_time=159.905s
2025-07-31 11:39:17 Train INFO: [Train]: [019][00050/00117] (43.2%)  Loss=1.0115  cls_loss=0.6978  reg_loss=0.3137  lr_det=7.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1136s  iter_time=163.019s
2025-07-31 11:42:01 Train INFO: [Train]: [019][00060/00117] (51.7%)  Loss=1.0100  cls_loss=0.6977  reg_loss=0.3123  lr_det=7.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=962s  iter_time=164.664s
2025-07-31 11:44:45 Train INFO: [Train]: [019][00070/00117] (60.2%)  Loss=1.0047  cls_loss=0.6944  reg_loss=0.3103  lr_det=7.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=790s  iter_time=163.580s
2025-07-31 11:47:27 Train INFO: [Train]: [019][00080/00117] (68.6%)  Loss=1.0050  cls_loss=0.6934  reg_loss=0.3116  lr_det=7.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=619s  iter_time=162.484s
2025-07-31 11:50:08 Train INFO: [Train]: [019][00090/00117] (77.1%)  Loss=1.0071  cls_loss=0.6952  reg_loss=0.3119  lr_det=7.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=450s  iter_time=160.944s
2025-07-31 11:52:51 Train INFO: [Train]: [019][00100/00117] (85.6%)  Loss=0.9994  cls_loss=0.6899  reg_loss=0.3095  lr_det=7.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=283s  iter_time=162.500s
2025-07-31 11:55:35 Train INFO: [Train]: [019][00110/00117] (94.1%)  Loss=1.0025  cls_loss=0.6922  reg_loss=0.3103  lr_det=7.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=116s  iter_time=164.027s
2025-07-31 11:57:14 Train INFO: [Train]: [019][00117/00117] (100.0%)  Loss=0.9988  cls_loss=0.6893  reg_loss=0.3095  lr_det=7.3e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=99.302s
2025-07-31 11:57:15 Train INFO: [Train]: Epoch 19 completed in 1943.1s (avg 16.467s/iter)
2025-07-31 11:57:15 Train INFO: [Train]: Final Loss=0.9988
2025-07-31 11:57:15 Train INFO: Checkpoint saved at epoch 19
2025-07-31 11:57:15 Train INFO: [Train]: Epoch 20 started (Total iterations: 118)
2025-07-31 12:00:45 Train INFO: [Train]: [020][00010/00117] (9.3%)  Loss=1.0085  cls_loss=0.6929  reg_loss=0.3156  lr_det=7.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2043s  iter_time=210.004s  fwd=1.221s/bwd=0.064s/opt=0.017s
2025-07-31 12:03:26 Train INFO: [Train]: [020][00020/00117] (17.8%)  Loss=0.9927  cls_loss=0.6822  reg_loss=0.3105  lr_det=7.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1710s  iter_time=160.282s
2025-07-31 12:06:12 Train INFO: [Train]: [020][00030/00117] (26.3%)  Loss=0.9718  cls_loss=0.6695  reg_loss=0.3023  lr_det=7.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1506s  iter_time=166.318s
2025-07-31 12:08:57 Train INFO: [Train]: [020][00040/00117] (34.7%)  Loss=0.9861  cls_loss=0.6822  reg_loss=0.3039  lr_det=7.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1317s  iter_time=164.578s
2025-07-31 12:11:39 Train INFO: [Train]: [020][00050/00117] (43.2%)  Loss=0.9768  cls_loss=0.6761  reg_loss=0.3007  lr_det=7.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1134s  iter_time=162.130s
2025-07-31 12:14:16 Train INFO: [Train]: [020][00060/00117] (51.7%)  Loss=0.9861  cls_loss=0.6820  reg_loss=0.3041  lr_det=7.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=954s  iter_time=157.396s
2025-07-31 12:16:54 Train INFO: [Train]: [020][00070/00117] (60.2%)  Loss=0.9884  cls_loss=0.6843  reg_loss=0.3041  lr_det=7.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=780s  iter_time=157.443s
2025-07-31 12:19:36 Train INFO: [Train]: [020][00080/00117] (68.6%)  Loss=0.9829  cls_loss=0.6802  reg_loss=0.3027  lr_det=7.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=612s  iter_time=162.494s
2025-07-31 12:22:25 Train INFO: [Train]: [020][00090/00117] (77.1%)  Loss=0.9853  cls_loss=0.6811  reg_loss=0.3041  lr_det=6.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=448s  iter_time=168.680s
2025-07-31 12:25:12 Train INFO: [Train]: [020][00100/00117] (85.6%)  Loss=0.9837  cls_loss=0.6811  reg_loss=0.3026  lr_det=6.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=282s  iter_time=166.886s
2025-07-31 12:27:55 Train INFO: [Train]: [020][00110/00117] (94.1%)  Loss=0.9779  cls_loss=0.6772  reg_loss=0.3007  lr_det=6.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=116s  iter_time=163.188s
2025-07-31 12:29:26 Train INFO: [Train]: [020][00117/00117] (100.0%)  Loss=0.9819  cls_loss=0.6801  reg_loss=0.3018  lr_det=6.8e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=90.889s
2025-07-31 12:29:27 Train INFO: [Train]: Epoch 20 completed in 1931.1s (avg 16.365s/iter)
2025-07-31 12:29:27 Train INFO: [Train]: Final Loss=0.9819
2025-07-31 12:29:27 Train INFO: [Train]: Epoch 21 started (Total iterations: 118)
2025-07-31 12:32:50 Train INFO: [Train]: [021][00010/00117] (9.3%)  Loss=1.0273  cls_loss=0.7185  reg_loss=0.3088  lr_det=6.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1981s  iter_time=203.605s  fwd=1.139s/bwd=0.061s/opt=0.021s
2025-07-31 12:35:32 Train INFO: [Train]: [021][00020/00117] (17.8%)  Loss=1.0114  cls_loss=0.7062  reg_loss=0.3053  lr_det=6.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1687s  iter_time=161.579s
2025-07-31 12:38:18 Train INFO: [Train]: [021][00030/00117] (26.3%)  Loss=0.9923  cls_loss=0.6889  reg_loss=0.3034  lr_det=6.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1492s  iter_time=166.577s
2025-07-31 12:41:27 Train INFO: [Train]: [021][00040/00117] (34.7%)  Loss=0.9887  cls_loss=0.6891  reg_loss=0.2996  lr_det=6.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1354s  iter_time=189.138s
2025-07-31 12:44:49 Train INFO: [Train]: [021][00050/00117] (43.2%)  Loss=0.9866  cls_loss=0.6883  reg_loss=0.2983  lr_det=6.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1212s  iter_time=201.356s
2025-07-31 12:47:58 Train INFO: [Train]: [021][00060/00117] (51.7%)  Loss=1.0001  cls_loss=0.6961  reg_loss=0.3040  lr_det=6.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1039s  iter_time=189.200s
2025-07-31 12:50:53 Train INFO: [Train]: [021][00070/00117] (60.2%)  Loss=0.9929  cls_loss=0.6891  reg_loss=0.3038  lr_det=6.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=852s  iter_time=175.463s
2025-07-31 12:54:09 Train INFO: [Train]: [021][00080/00117] (68.6%)  Loss=0.9878  cls_loss=0.6861  reg_loss=0.3017  lr_det=6.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=677s  iter_time=195.315s
2025-07-31 12:57:29 Train INFO: [Train]: [021][00090/00117] (77.1%)  Loss=0.9961  cls_loss=0.6913  reg_loss=0.3048  lr_det=6.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=499s  iter_time=200.330s
2025-07-31 13:00:38 Train INFO: [Train]: [021][00100/00117] (85.6%)  Loss=0.9868  cls_loss=0.6845  reg_loss=0.3022  lr_det=6.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=315s  iter_time=189.357s
2025-07-31 13:03:52 Train INFO: [Train]: [021][00110/00117] (94.1%)  Loss=0.9864  cls_loss=0.6845  reg_loss=0.3019  lr_det=6.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=130s  iter_time=193.508s
2025-07-31 13:05:51 Train INFO: [Train]: [021][00117/00117] (100.0%)  Loss=0.9834  cls_loss=0.6832  reg_loss=0.3002  lr_det=6.3e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=118.559s
2025-07-31 13:05:52 Train INFO: [Train]: Epoch 21 completed in 2185.4s (avg 18.520s/iter)
2025-07-31 13:05:52 Train INFO: [Train]: Final Loss=0.9834
2025-07-31 13:05:53 Train INFO: Checkpoint saved at epoch 21
2025-07-31 13:05:53 Train INFO: [Train]: Epoch 22 started (Total iterations: 118)
2025-07-31 13:10:04 Train INFO: [Train]: [022][00010/00117] (9.3%)  Loss=1.0299  cls_loss=0.7169  reg_loss=0.3130  lr_det=6.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2441s  iter_time=250.989s  fwd=1.172s/bwd=0.078s/opt=0.020s
2025-07-31 13:13:25 Train INFO: [Train]: [022][00020/00117] (17.8%)  Loss=1.0183  cls_loss=0.7056  reg_loss=0.3127  lr_det=6.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2086s  iter_time=200.715s
2025-07-31 13:16:40 Train INFO: [Train]: [022][00030/00117] (26.3%)  Loss=1.0293  cls_loss=0.7149  reg_loss=0.3144  lr_det=6.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1815s  iter_time=195.133s
2025-07-31 13:19:53 Train INFO: [Train]: [022][00040/00117] (34.7%)  Loss=1.0051  cls_loss=0.6972  reg_loss=0.3080  lr_det=6.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1577s  iter_time=193.081s
2025-07-31 13:23:07 Train INFO: [Train]: [022][00050/00117] (43.2%)  Loss=1.0000  cls_loss=0.6941  reg_loss=0.3058  lr_det=6.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1358s  iter_time=193.783s
2025-07-31 13:25:51 Train INFO: [Train]: [022][00060/00117] (51.7%)  Loss=1.0078  cls_loss=0.6992  reg_loss=0.3086  lr_det=6.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1119s  iter_time=164.031s
2025-07-31 13:28:40 Train INFO: [Train]: [022][00070/00117] (60.2%)  Loss=1.0001  cls_loss=0.6940  reg_loss=0.3061  lr_det=5.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=905s  iter_time=168.984s
2025-07-31 13:31:25 Train INFO: [Train]: [022][00080/00117] (68.6%)  Loss=0.9921  cls_loss=0.6889  reg_loss=0.3032  lr_det=5.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=700s  iter_time=164.750s
2025-07-31 13:34:14 Train INFO: [Train]: [022][00090/00117] (77.1%)  Loss=0.9920  cls_loss=0.6891  reg_loss=0.3029  lr_det=5.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=505s  iter_time=169.378s
2025-07-31 13:36:58 Train INFO: [Train]: [022][00100/00117] (85.6%)  Loss=0.9891  cls_loss=0.6873  reg_loss=0.3018  lr_det=5.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=314s  iter_time=163.925s
2025-07-31 13:39:42 Train INFO: [Train]: [022][00110/00117] (94.1%)  Loss=0.9901  cls_loss=0.6867  reg_loss=0.3034  lr_det=5.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=128s  iter_time=163.965s
2025-07-31 13:41:25 Train INFO: [Train]: [022][00117/00117] (100.0%)  Loss=0.9870  cls_loss=0.6843  reg_loss=0.3027  lr_det=5.7e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=103.499s
2025-07-31 13:41:27 Train INFO: [Train]: Epoch 22 completed in 2133.5s (avg 18.080s/iter)
2025-07-31 13:41:27 Train INFO: [Train]: Final Loss=0.9870
2025-07-31 13:41:27 Train INFO: [Train]: Epoch 23 started (Total iterations: 118)
2025-07-31 13:45:05 Train INFO: [Train]: [023][00010/00117] (9.3%)  Loss=0.9826  cls_loss=0.6748  reg_loss=0.3078  lr_det=5.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2121s  iter_time=218.082s  fwd=1.195s/bwd=0.048s/opt=0.012s
2025-07-31 13:47:55 Train INFO: [Train]: [023][00020/00117] (17.8%)  Loss=0.9786  cls_loss=0.6714  reg_loss=0.3072  lr_det=5.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1795s  iter_time=170.586s
2025-07-31 13:50:43 Train INFO: [Train]: [023][00030/00117] (26.3%)  Loss=0.9797  cls_loss=0.6749  reg_loss=0.3048  lr_det=5.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1563s  iter_time=168.117s
2025-07-31 13:53:35 Train INFO: [Train]: [023][00040/00117] (34.7%)  Loss=0.9873  cls_loss=0.6789  reg_loss=0.3084  lr_det=5.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1368s  iter_time=171.410s
2025-07-31 13:56:29 Train INFO: [Train]: [023][00050/00117] (43.2%)  Loss=0.9781  cls_loss=0.6743  reg_loss=0.3038  lr_det=5.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1186s  iter_time=174.370s
2025-07-31 13:59:12 Train INFO: [Train]: [023][00060/00117] (51.7%)  Loss=0.9913  cls_loss=0.6840  reg_loss=0.3073  lr_det=5.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=996s  iter_time=162.860s
2025-07-31 14:01:57 Train INFO: [Train]: [023][00070/00117] (60.2%)  Loss=0.9782  cls_loss=0.6760  reg_loss=0.3022  lr_det=5.4e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=815s  iter_time=165.357s
2025-07-31 14:04:49 Train INFO: [Train]: [023][00080/00117] (68.6%)  Loss=0.9789  cls_loss=0.6764  reg_loss=0.3026  lr_det=5.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=641s  iter_time=172.085s
2025-07-31 14:07:31 Train INFO: [Train]: [023][00090/00117] (77.1%)  Loss=0.9814  cls_loss=0.6769  reg_loss=0.3045  lr_det=5.3e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=464s  iter_time=161.570s
2025-07-31 14:10:11 Train INFO: [Train]: [023][00100/00117] (85.6%)  Loss=0.9766  cls_loss=0.6745  reg_loss=0.3021  lr_det=5.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=290s  iter_time=160.138s
2025-07-31 14:13:01 Train INFO: [Train]: [023][00110/00117] (94.1%)  Loss=0.9815  cls_loss=0.6774  reg_loss=0.3041  lr_det=5.2e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=119s  iter_time=169.765s
2025-07-31 14:14:41 Train INFO: [Train]: [023][00117/00117] (100.0%)  Loss=0.9798  cls_loss=0.6758  reg_loss=0.3041  lr_det=5.1e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=100.520s
2025-07-31 14:14:42 Train INFO: [Train]: Epoch 23 completed in 1995.9s (avg 16.914s/iter)
2025-07-31 14:14:42 Train INFO: [Train]: Final Loss=0.9798
2025-07-31 14:14:43 Train INFO: Checkpoint saved at epoch 23
2025-07-31 14:14:43 Train INFO: [Train]: Epoch 24 started (Total iterations: 118)
2025-07-31 14:18:27 Train INFO: [Train]: [024][00010/00117] (9.3%)  Loss=1.0056  cls_loss=0.7009  reg_loss=0.3048  lr_det=5.1e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2180s  iter_time=224.116s  fwd=1.122s/bwd=0.074s/opt=0.014s
2025-07-31 14:21:15 Train INFO: [Train]: [024][00020/00117] (17.8%)  Loss=0.9916  cls_loss=0.6885  reg_loss=0.3031  lr_det=5.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1812s  iter_time=168.156s
2025-07-31 14:23:47 Evaluation INFO: Starting evaluation...
2025-07-31 14:24:04 Train INFO: [Train]: [024][00030/00117] (26.3%)  Loss=1.0027  cls_loss=0.6976  reg_loss=0.3050  lr_det=5.0e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1576s  iter_time=169.177s
2025-07-31 14:24:49 Evaluation INFO: Evaluating PKU-MMD dataset.
2025-07-31 14:24:49 Evaluation INFO: Loaded annotations from test subset.
2025-07-31 14:24:49 Evaluation INFO: Number of ground truth instances: 2704
2025-07-31 14:24:49 Evaluation INFO: Number of predictions: 264000
2025-07-31 14:24:49 Evaluation INFO: Fixed threshold for tiou score: [0.2, 0.4, 0.5, 0.6, 0.7]
2025-07-31 14:24:49 Evaluation INFO: Average-mAP: 0.01 (%)
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.20 is 0.05%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.40 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.50 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.60 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: mAP at tIoU 0.70 is 0.00%
2025-07-31 14:24:49 Evaluation INFO: Evaluation results saved to: work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter/gpu1_id0\evaluation_results.json
2025-07-31 14:24:49 Evaluation INFO: Evaluation completed!
2025-07-31 14:26:59 Train INFO: [Train]: [024][00040/00117] (34.7%)  Loss=1.0011  cls_loss=0.6941  reg_loss=0.3071  lr_det=4.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1382s  iter_time=174.298s
2025-07-31 14:29:45 Train INFO: [Train]: [024][00050/00117] (43.2%)  Loss=0.9923  cls_loss=0.6892  reg_loss=0.3031  lr_det=4.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1185s  iter_time=166.149s
2025-07-31 14:32:34 Train INFO: [Train]: [024][00060/00117] (51.7%)  Loss=0.9917  cls_loss=0.6890  reg_loss=0.3026  lr_det=4.9e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1001s  iter_time=169.330s
2025-07-31 14:35:16 Train INFO: [Train]: [024][00070/00117] (60.2%)  Loss=0.9756  cls_loss=0.6782  reg_loss=0.2974  lr_det=4.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=816s  iter_time=162.011s
2025-07-31 14:38:02 Train INFO: [Train]: [024][00080/00117] (68.6%)  Loss=0.9839  cls_loss=0.6849  reg_loss=0.2989  lr_det=4.8e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=639s  iter_time=165.268s
2025-07-31 14:40:48 Train INFO: [Train]: [024][00090/00117] (77.1%)  Loss=0.9882  cls_loss=0.6872  reg_loss=0.3010  lr_det=4.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=464s  iter_time=165.983s
2025-07-31 14:43:39 Train INFO: [Train]: [024][00100/00117] (85.6%)  Loss=0.9853  cls_loss=0.6851  reg_loss=0.3002  lr_det=4.7e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=292s  iter_time=171.268s
2025-07-31 14:46:21 Train INFO: [Train]: [024][00110/00117] (94.1%)  Loss=0.9843  cls_loss=0.6839  reg_loss=0.3003  lr_det=4.6e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=120s  iter_time=162.173s
2025-07-31 14:48:01 Train INFO: [Train]: [024][00117/00117] (100.0%)  Loss=0.9786  cls_loss=0.6793  reg_loss=0.2992  lr_det=4.6e-05  GPU=1054MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=0s  iter_time=99.943s
2025-07-31 14:48:02 Train INFO: [Train]: Epoch 24 completed in 1998.8s (avg 16.939s/iter)
2025-07-31 14:48:02 Train INFO: [Train]: Final Loss=0.9786
2025-07-31 14:48:02 Train INFO: [Train]: Epoch 25 started (Total iterations: 118)
2025-07-31 14:51:35 Train INFO: [Train]: [025][00010/00117] (9.3%)  Loss=0.9992  cls_loss=0.7001  reg_loss=0.2991  lr_det=4.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=2071s  iter_time=212.934s  fwd=1.108s/bwd=0.042s/opt=0.014s
2025-07-31 14:54:26 Train INFO: [Train]: [025][00020/00117] (17.8%)  Loss=1.0118  cls_loss=0.7051  reg_loss=0.3067  lr_det=4.5e-05  GPU=1132MB(alloc)/5942MB(reserved)/4968MB(max)  ETA=1774s  iter_time=171.087s
2025-07-31 15:20:51 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 15:20:51 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-07-31 15:20:52 Train INFO: training subset: 831 videos
2025-07-31 15:24:01 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 15:24:02 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-07-31 15:24:03 Train INFO: training subset: 831 videos
2025-07-31 15:25:04 Train INFO: Using torch version: 2.5.1+cu121, CUDA version: 12.1
2025-07-31 15:25:04 Train INFO: Config: 
annotation_test = 'data/PKU-MMD/pku_test.json'
annotation_train = 'data/PKU-MMD/pku_train.json'
annotation_val = 'data/PKU-MMD/pku_val.json'
block_list = None
chunk_num = 32
class_map = 'data/PKU-MMD/class_map.txt'
dataset = dict(
    test=dict(
        ann_file='data/PKU-MMD/pku_test.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(keys=[
                'imgs',
            ], type='ConvertToTensor'),
            dict(inputs='imgs', keys=[
                'masks',
            ], type='Collect'),
        ],
        sample_stride=1,
        subset_name='testing',
        test_mode=True,
        type='PkuSlidingDataset',
        window_overlap_ratio=0.5,
        window_size=512),
    train=dict(
        ann_file='data/PKU-MMD/pku_train.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=6, type='mmaction.DecordInit'),
            dict(
                crop_ratio=[
                    0.9,
                    1.0,
                ],
                method='random_trunc',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                trunc_thresh=0.5,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                182,
            ), type='mmaction.Resize'),
            dict(type='mmaction.RandomResizedCrop'),
            dict(keep_ratio=False, scale=(
                160,
                160,
            ), type='mmaction.Resize'),
            dict(flip_ratio=0.5, type='mmaction.Flip'),
            dict(transforms='default', type='mmaction.ImgAug'),
            dict(type='mmaction.ColorJitter'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='training',
        type='PkuPaddingDataset'),
    val=dict(
        ann_file='data/PKU-MMD/pku_val.json',
        class_map='data/PKU-MMD/class_map.txt',
        data_path='F:/dataset/pku-mmd/rgb',
        feature_stride=4,
        filter_gt=False,
        pipeline=[
            dict(format='avi', type='PrepareVideoInfo'),
            dict(num_threads=4, type='mmaction.DecordInit'),
            dict(
                method='sliding_window',
                num_clips=1,
                scale_factor=1,
                trunc_len=512,
                type='LoadFrames'),
            dict(type='mmaction.DecordDecode'),
            dict(scale=(
                -1,
                160,
            ), type='mmaction.Resize'),
            dict(crop_size=160, type='mmaction.CenterCrop'),
            dict(input_format='NCTHW', type='mmaction.FormatShape'),
            dict(
                keys=[
                    'imgs',
                    'gt_segments',
                    'gt_labels',
                ],
                type='ConvertToTensor'),
            dict(
                inputs='imgs',
                keys=[
                    'masks',
                    'gt_segments',
                    'gt_labels',
                ],
                type='Collect'),
        ],
        sample_stride=1,
        subset_name='validation',
        type='PkuPaddingDataset'))
evaluation = dict(
    ground_truth_filename='data/PKU-MMD/pku_val.json',
    subset='validation',
    tiou_thresholds=[
        0.1,
        0.3,
        0.5,
        0.7,
        0.9,
    ],
    type='mAP_PKU_MMD')
inference = dict(
    load_from_raw_predictions=False,
    save_raw_prediction=False,
    score_thresh=0.3)
model = dict(
    backbone=dict(
        adapter_index=[
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
        ],
        adapter_mlp_ratio=0.25,
        custom=dict(
            freeze_backbone=True,
            norm_eval=False,
            post_processing_pipeline=[
                dict(
                    keys=[
                        'feats',
                    ],
                    ops='b n c t -> b c t',
                    reduction='mean',
                    type='Reduce'),
            ],
            pretrain=
            'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth'
        ),
        depth=12,
        drop_path_rate=0.1,
        embed_dims=384,
        img_size=160,
        mlp_ratio=4.0,
        num_frames=16,
        num_heads=6,
        patch_size=16,
        pretrained=
        'pretrained/vit-small-p16_videomae-k400-pre_16x4x1_kinetics-400_my.pth',
        qkv_bias=True,
        return_feat_map=True,
        total_frames=512,
        tubelet_size=2,
        type='VisionTransformerAdapter',
        use_mean_pooling=True,
        with_cp=True),
    neck=dict(
        in_channels=512, num_levels=6, out_channels=512, type='FPNIdentity'),
    projection=dict(
        arch=(
            2,
            2,
            5,
        ),
        attn_cfg=dict(n_head=4, n_mha_win_size=-1),
        conv_cfg=dict(kernel_size=3, proj_pdrop=0.0),
        in_channels=384,
        max_seq_len=512,
        norm_cfg=dict(type='LN'),
        out_channels=512,
        path_pdrop=0.1,
        type='Conv1DTransformerProj',
        use_abs_pe=False),
    rpn_head=dict(
        center_sample='radius',
        center_sample_radius=1.5,
        cls_prior_prob=0.01,
        feat_channels=512,
        in_channels=512,
        label_smoothing=0.0,
        loss=dict(
            cls_loss=dict(type='FocalLoss'), reg_loss=dict(type='DIOULoss')),
        loss_normalizer=100,
        loss_normalizer_momentum=0.9,
        num_classes=51,
        num_convs=2,
        prior_generator=dict(
            regression_range=[
                (
                    0,
                    8,
                ),
                (
                    8,
                    16,
                ),
                (
                    16,
                    32,
                ),
                (
                    32,
                    64,
                ),
                (
                    64,
                    128,
                ),
                (
                    128,
                    10000,
                ),
            ],
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='PointGenerator'),
        type='ActionFormerHead'),
    type='ActionFormer')
optimizer = dict(
    backbone=dict(
        custom=[
            dict(lr=0.0002, name='adapter', weight_decay=0.05),
        ],
        exclude=[
            'backbone',
        ],
        lr=0,
        weight_decay=0),
    lr=0.0001,
    paramwise=True,
    type='AdamW',
    weight_decay=0.05)
post_processing = dict(
    nms=dict(
        max_seg_num=500,
        multiclass=False,
        sigma=0.3,
        use_soft_nms=True,
        voting_thresh=0.7),
    save_dict=True)
scale_factor = 1
scheduler = dict(
    max_epoch=120, type='LinearWarmupCosineAnnealingLR', warmup_epoch=5)
solver = dict(
    amp=True,
    clip_grad_norm=1,
    ema=True,
    fp16_compress=True,
    static_graph=True,
    test=dict(batch_size=4, num_workers=4),
    train=dict(batch_size=16, num_workers=4),
    val=dict(batch_size=8, num_workers=4))
video_dir = 'F:/dataset/pku-mmd/rgb'
window_size = 512
work_dir = 'work_dirs/e2e_pku_mmd_videomae_s_768x1_160_adapter\\gpu1_id0/'
workflow = dict(
    checkpoint_interval=2,
    end_epoch=120,
    logging_interval=10,
    val_eval_interval=5,
    val_loss_interval=5,
    val_start_epoch=10)

2025-07-31 15:25:05 Train INFO: training subset: 831 videos
2025-07-31 15:25:05 Train INFO: validation subset: 111 videos
2025-07-31 15:25:05 Train INFO: testing subset: 132 videos, truncated as 549 windows.
2025-07-31 15:25:06 Train INFO: Using single GPU training...
2025-07-31 15:25:06 Train INFO: Using Model EMA...
2025-07-31 15:25:06 Train INFO: Using Automatic Mixed Precision...
2025-07-31 15:25:06 Train INFO: GPU Memory: 24.0 GB
2025-07-31 15:25:06 Train INFO: Freeze the backbone...
2025-07-31 15:25:06 Train INFO: Training Starts...

2025-07-31 15:25:06 Train INFO: [Train]: Epoch 0 started (Total iterations: 52)
2025-07-31 15:33:13 Train INFO: [Train]: [000][00010/00051] (21.2%)  Loss=3.3739  cls_loss=1.8422  reg_loss=1.5317  lr_det=3.9e-06  GPU=1430MB(alloc)/11454MB(reserved)/9063MB(max)  ETA=1816s  iter_time=487.157s  fwd=2.145s/bwd=0.107s/opt=0.012s
2025-07-31 15:40:53 Train INFO: [Train]: [000][00020/00051] (40.4%)  Loss=2.6993  cls_loss=1.5757  reg_loss=1.1236  lr_det=7.7e-06  GPU=1430MB(alloc)/11454MB(reserved)/9063MB(max)  ETA=1398s  iter_time=460.094s
2025-07-31 15:46:10 Train INFO: [Train]: [000][00030/00051] (59.6%)  Loss=2.3302  cls_loss=1.4309  reg_loss=0.8994  lr_det=1.2e-05  GPU=1430MB(alloc)/11454MB(reserved)/9063MB(max)  ETA=856s  iter_time=317.077s
2025-07-31 15:54:37 Train INFO: [Train]: [000][00040/00051] (78.8%)  Loss=2.1058  cls_loss=1.3305  reg_loss=0.7753  lr_det=1.5e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=475s  iter_time=506.442s
2025-07-31 16:00:05 Train INFO: [Train]: [000][00050/00051] (98.1%)  Loss=1.9501  cls_loss=1.2512  reg_loss=0.6990  lr_det=1.9e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=41s  iter_time=328.329s
2025-07-31 16:00:10 Train INFO: [Train]: [000][00051/00051] (100.0%)  Loss=1.9338  cls_loss=1.2419  reg_loss=0.6919  lr_det=2.0e-05  GPU=1395MB(alloc)/4210MB(reserved)/9063MB(max)  ETA=0s  iter_time=4.863s
2025-07-31 16:00:11 Train INFO: [Train]: Epoch 0 completed in 2105.2s (avg 40.485s/iter)
2025-07-31 16:00:11 Train INFO: [Train]: Final Loss=1.9338
2025-07-31 16:00:11 Train INFO: [Train]: Epoch 1 started (Total iterations: 52)
2025-07-31 16:09:19 Train INFO: [Train]: [001][00010/00051] (21.2%)  Loss=1.2270  cls_loss=0.8527  reg_loss=0.3742  lr_det=2.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=2041s  iter_time=547.668s  fwd=2.183s/bwd=0.067s/opt=0.008s
2025-07-31 16:17:32 Train INFO: [Train]: [001][00020/00051] (40.4%)  Loss=1.2124  cls_loss=0.8401  reg_loss=0.3723  lr_det=2.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1537s  iter_time=493.328s
2025-07-31 16:23:50 Train INFO: [Train]: [001][00030/00051] (59.6%)  Loss=1.2096  cls_loss=0.8361  reg_loss=0.3735  lr_det=3.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=961s  iter_time=377.393s
2025-07-31 16:32:34 Train INFO: [Train]: [001][00040/00051] (78.8%)  Loss=1.2032  cls_loss=0.8300  reg_loss=0.3732  lr_det=3.6e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=521s  iter_time=524.847s
2025-07-31 16:38:50 Train INFO: [Train]: [001][00050/00051] (98.1%)  Loss=1.1999  cls_loss=0.8267  reg_loss=0.3732  lr_det=3.9e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=45s  iter_time=375.768s
2025-07-31 16:38:53 Train INFO: [Train]: [001][00051/00051] (100.0%)  Loss=1.1972  cls_loss=0.8247  reg_loss=0.3725  lr_det=4.0e-05  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=2.778s
2025-07-31 16:38:54 Train INFO: [Train]: Epoch 1 completed in 2323.3s (avg 44.679s/iter)
2025-07-31 16:38:54 Train INFO: [Train]: Final Loss=1.1972
2025-07-31 16:38:56 Train INFO: Checkpoint saved at epoch 1
2025-07-31 16:38:56 Train INFO: [Train]: Epoch 2 started (Total iterations: 52)
2025-07-31 16:48:25 Train INFO: [Train]: [002][00010/00051] (21.2%)  Loss=1.1956  cls_loss=0.8170  reg_loss=0.3786  lr_det=4.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=2122s  iter_time=569.369s  fwd=2.140s/bwd=0.087s/opt=0.009s
2025-07-31 16:57:04 Train INFO: [Train]: [002][00020/00051] (40.4%)  Loss=1.1807  cls_loss=0.8088  reg_loss=0.3719  lr_det=4.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1607s  iter_time=519.252s
2025-07-31 17:03:12 Train INFO: [Train]: [002][00030/00051] (59.6%)  Loss=1.1848  cls_loss=0.8134  reg_loss=0.3715  lr_det=5.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=986s  iter_time=367.402s
2025-07-31 17:12:01 Train INFO: [Train]: [002][00040/00051] (78.8%)  Loss=1.1849  cls_loss=0.8137  reg_loss=0.3712  lr_det=5.6e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=533s  iter_time=529.755s
2025-07-31 17:18:24 Train INFO: [Train]: [002][00050/00051] (98.1%)  Loss=1.1827  cls_loss=0.8122  reg_loss=0.3705  lr_det=5.9e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=46s  iter_time=382.576s
2025-07-31 17:18:26 Train INFO: [Train]: [002][00051/00051] (100.0%)  Loss=1.1800  cls_loss=0.8103  reg_loss=0.3697  lr_det=6.0e-05  GPU=1395MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=0s  iter_time=2.218s
2025-07-31 17:18:27 Train INFO: [Train]: Epoch 2 completed in 2371.8s (avg 45.611s/iter)
2025-07-31 17:18:27 Train INFO: [Train]: Final Loss=1.1800
2025-07-31 17:18:27 Train INFO: [Train]: Epoch 3 started (Total iterations: 52)
2025-07-31 17:27:10 Train INFO: [Train]: [003][00010/00051] (21.2%)  Loss=1.1777  cls_loss=0.8053  reg_loss=0.3724  lr_det=6.4e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1947s  iter_time=522.247s  fwd=3.045s/bwd=0.081s/opt=0.034s
2025-07-31 17:35:12 Train INFO: [Train]: [003][00020/00051] (40.4%)  Loss=1.1466  cls_loss=0.7823  reg_loss=0.3643  lr_det=6.8e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=1483s  iter_time=482.293s
2025-07-31 17:40:51 Train INFO: [Train]: [003][00030/00051] (59.6%)  Loss=1.1486  cls_loss=0.7805  reg_loss=0.3681  lr_det=7.2e-05  GPU=1430MB(alloc)/11456MB(reserved)/9063MB(max)  ETA=910s  iter_time=338.970s
